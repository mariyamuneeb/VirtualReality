{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - CSC678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "__a__. Implement 20 layer ResNet model (ResNet20) with the specification given on Page 7 of the paper for CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 Dataset :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of Classes 10 <br>\n",
    "- 50,000 training images <br>\n",
    "- 10,000 testing images <br>\n",
    "- image size  = 32 x 32 x 3 <br>\n",
    "- 6000 images per class <br>\n",
    "_do data set analysis exploratory data analysis _ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape  X=(50000, 32, 32, 3), y=(50000, 1)\n",
      "Test Data shape  X=(10000, 32, 32, 3), y=(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training Data shape  X=%s, y=%s'% (x_train.shape, y_train.shape))\n",
    "print('Test Data shape  X=%s, y=%s'% (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAO5ElEQVR4nO3df6zddX3H8efL1t9OW+VKWFtXFhsnLlHIDXQjWTZqSkFj+UOSmk0b0qX/1A0XEwf+Q6aSaLKIM5lkjXSrzg0JamgcERvALPtDpAhDoZLeoaN37WxdC7oZddX3/jifugPcH+fC7TnI5/lIbs73+/5+vuf7/uQ2r/O93/M9p6kqJEl9eMGkG5AkjY+hL0kdMfQlqSOGviR1xNCXpI6snHQDCznrrLNq/fr1k25Dkn6l3HfffT+oqqm5tj2nQ3/9+vUcOHBg0m1I0q+UJP8+3zYv70hSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjBT6Sb6X5FtJHkhyoNVenWR/kkPtcXWrJ8knk8wkeTDJBUPPs72NP5Rk+5mZkiRpPks50/+DqnpLVU239WuAO6tqA3BnWwe4DNjQfnYCN8LgRQK4DrgIuBC47vQLhSRpPJ7N5Z2twN62vBe4Yqj+mRr4OrAqyTnApcD+qjpRVSeB/cCWZ3F8SdISjfqJ3AK+mqSAv6mq3cDZVXUUoKqOJnltG7sGODy072yrzVd/kiQ7GfyFwOte97olTOXp1l/zT89q/8V876Nv6/LYCx3fY3tsj31mj/1sjRr6F1fVkRbs+5N8Z4GxmaNWC9SfXBi8oOwGmJ6e9r/1kqRlNNLlnao60h6PAV9icE3+++2yDe3xWBs+C6wb2n0tcGSBuiRpTBYN/SQvT/Jrp5eBzcC3gX3A6TtwtgO3teV9wHvaXTwbgSfaZaA7gM1JVrc3cDe3miRpTEa5vHM28KUkp8f/Q1V9Jcm9wC1JdgCPAVe28bcDlwMzwI+BqwCq6kSSDwP3tnEfqqoTyzYTSdKiFg39qnoUePMc9f8CNs1RL2DXPM+1B9iz9DYlScvBT+RKUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjJy6CdZkeT+JF9u6+cmuSfJoSSfT/KiVn9xW59p29cPPce1rf5IkkuXezKSpIUt5Uz/auDg0PrHgBuqagNwEtjR6juAk1X1euCGNo4k5wHbgDcBW4BPJVnx7NqXJC3FSKGfZC3wNuDTbT3AJcCtbche4Iq2vLWt07ZvauO3AjdX1U+r6rvADHDhckxCkjSaUc/0PwF8APhFW38N8HhVnWrrs8CatrwGOAzQtj/Rxv+yPsc+v5RkZ5IDSQ4cP358CVORJC1m0dBP8nbgWFXdN1yeY2gtsm2hff6/ULW7qqaranpqamqx9iRJS7ByhDEXA+9IcjnwEuCVDM78VyVZ2c7m1wJH2vhZYB0wm2Ql8CrgxFD9tOF9JEljsOiZflVdW1Vrq2o9gzdi76qqPwTuBt7Zhm0HbmvL+9o6bftdVVWtvq3d3XMusAH4xrLNRJK0qFHO9Ofz58DNST4C3A/c1Oo3AZ9NMsPgDH8bQFU9lOQW4GHgFLCrqn7+LI4vSVqiJYV+VX0N+FpbfpQ57r6pqp8AV86z//XA9UttUpK0PPxEriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakji4Z+kpck+UaSf03yUJK/aPVzk9yT5FCSzyd5Uau/uK3PtO3rh57r2lZ/JMmlZ2pSkqS5jXKm/1Pgkqp6M/AWYEuSjcDHgBuqagNwEtjRxu8ATlbV64Eb2jiSnAdsA94EbAE+lWTFck5GkrSwRUO/Bv67rb6w/RRwCXBrq+8FrmjLW9s6bfumJGn1m6vqp1X1XWAGuHBZZiFJGslI1/STrEjyAHAM2A/8G/B4VZ1qQ2aBNW15DXAYoG1/AnjNcH2OfSRJYzBS6FfVz6vqLcBaBmfnb5xrWHvMPNvmqz9Jkp1JDiQ5cPz48VHakySNaEl371TV48DXgI3AqiQr26a1wJG2PAusA2jbXwWcGK7Psc/wMXZX1XRVTU9NTS2lPUnSIka5e2cqyaq2/FLgrcBB4G7gnW3YduC2tryvrdO231VV1erb2t095wIbgG8s10QkSYtbufgQzgH2tjttXgDcUlVfTvIwcHOSjwD3Aze18TcBn00yw+AMfxtAVT2U5BbgYeAUsKuqfr6805EkLWTR0K+qB4Hz56g/yhx331TVT4Ar53mu64Hrl96mJGk5+IlcSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyKKhn2RdkruTHEzyUJKrW/3VSfYnOdQeV7d6knwyyUySB5NcMPRc29v4Q0m2n7lpSZLmMsqZ/ing/VX1RmAjsCvJecA1wJ1VtQG4s60DXAZsaD87gRth8CIBXAdcBFwIXHf6hUKSNB6Lhn5VHa2qb7blHwEHgTXAVmBvG7YXuKItbwU+UwNfB1YlOQe4FNhfVSeq6iSwH9iyrLORJC1oSdf0k6wHzgfuAc6uqqMweGEAXtuGrQEOD+0222rz1Z96jJ1JDiQ5cPz48aW0J0laxMihn+QVwBeA91XVDxcaOketFqg/uVC1u6qmq2p6ampq1PYkSSMYKfSTvJBB4H+uqr7Yyt9vl21oj8dafRZYN7T7WuDIAnVJ0piMcvdOgJuAg1X18aFN+4DTd+BsB24bqr+n3cWzEXiiXf65A9icZHV7A3dzq0mSxmTlCGMuBt4NfCvJA632QeCjwC1JdgCPAVe2bbcDlwMzwI+BqwCq6kSSDwP3tnEfqqoTyzILSdJIFg39qvoX5r4eD7BpjvEF7JrnufYAe5bSoCRp+fiJXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHFg39JHuSHEvy7aHaq5PsT3KoPa5u9ST5ZJKZJA8muWBon+1t/KEk28/MdCRJCxnlTP/vgC1PqV0D3FlVG4A72zrAZcCG9rMTuBEGLxLAdcBFwIXAdadfKCRJ47No6FfVPwMnnlLeCuxty3uBK4bqn6mBrwOrkpwDXArsr6oTVXUS2M/TX0gkSWfYM72mf3ZVHQVoj69t9TXA4aFxs602X/1pkuxMciDJgePHjz/D9iRJc1nuN3IzR60WqD+9WLW7qqaranpqampZm5Ok3j3T0P9+u2xDezzW6rPAuqFxa4EjC9QlSWP0TEN/H3D6DpztwG1D9fe0u3g2Ak+0yz93AJuTrG5v4G5uNUnSGK1cbECSfwR+HzgrySyDu3A+CtySZAfwGHBlG347cDkwA/wYuAqgqk4k+TBwbxv3oap66pvDkqQzbNHQr6p3zbNp0xxjC9g1z/PsAfYsqTtJ0rLyE7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRsYd+ki1JHkkyk+SacR9fkno21tBPsgL4a+Ay4DzgXUnOG2cPktSzcZ/pXwjMVNWjVfUz4GZg65h7kKRuparGd7DkncCWqvrjtv5u4KKqeu/QmJ3Azrb6BuCRsTUIZwE/GOPxniucd1+c9/Pfb1TV1FwbVo65kcxRe9KrTlXtBnaPp50nS3KgqqYncexJct59cd59G/flnVlg3dD6WuDImHuQpG6NO/TvBTYkOTfJi4BtwL4x9yBJ3Rrr5Z2qOpXkvcAdwApgT1U9NM4eFjGRy0rPAc67L867Y2N9I1eSNFl+IleSOmLoS1JHDH36/WqIJOuS3J3kYJKHklw96Z7GKcmKJPcn+fKkexmXJKuS3JrkO+33/juT7mkckvxZ+zf+7ST/mOQlk+5pUroP/c6/GuIU8P6qeiOwEdjV0dwBrgYOTrqJMfsr4CtV9VvAm+lg/knWAH8KTFfVbzO4iWTbZLuanO5Dn46/GqKqjlbVN9vyjxgEwJrJdjUeSdYCbwM+PelexiXJK4HfA24CqKqfVdXjk+1qbFYCL02yEngZHX8+yNAfhNzhofVZOgm+YUnWA+cD90y2k7H5BPAB4BeTbmSMfhM4Dvxtu6z16SQvn3RTZ1pV/Qfwl8BjwFHgiar66mS7mhxDf4Svhni+S/IK4AvA+6rqh5Pu50xL8nbgWFXdN+lexmwlcAFwY1WdD/wP8Lx/DyvJagZ/vZ8L/Drw8iR/NNmuJsfQ7/yrIZK8kEHgf66qvjjpfsbkYuAdSb7H4HLeJUn+frItjcUsMFtVp/+au5XBi8Dz3VuB71bV8ar6X+CLwO9OuKeJMfQ7/mqIJGFwffdgVX180v2MS1VdW1Vrq2o9g9/3XVX1vD/zq6r/BA4neUMrbQIenmBL4/IYsDHJy9q/+U108Ab2fMb9LZvPOb8CXw1xJl0MvBv4VpIHWu2DVXX7BHvSmfUnwOfaCc6jwFUT7ueMq6p7ktwKfJPBHWv30/FXMvg1DJLUES/vSFJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkf8DXFkFW9DyG5AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels,counts = np.unique(y_train,return_counts=True)\n",
    "plt.bar(labels,counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(x,num_of_classes):\n",
    "    categorical  = np.zeros((x.shape[0],num_of_classes))\n",
    "    count=0\n",
    "    for sample in x:\n",
    "        categorical[count][sample]=1\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-1cf0a35360ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m330\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI8AAABjCAYAAACi5VNqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAE5klEQVR4nO3dT4gWdRzH8fcnzQIPCekhSjBJWjx00IfwFEEE6kEPddCLGcYiJZ2DDoGX6BRIkWwkZQeTPG1QBFHgSfNZKNOiWINoSXCt8BJYwrfDDLZujzuzX2d8Zh8/L3jgmWf+PN8f+2GeZ2b2+Y4iArOMu4ZdgC1dDo+lOTyW5vBYmsNjaQ6PpVWGR9IRSZcknbvJfEk6JGla0llJm5ov07qozp7nfWDrAvO3ARvKxzjwzq2XZUtBZXgi4iTwxwKL7ASORuEUsErSA00VaN3VxHeeB4Ff50zPlK/ZiFvewDY04LWB1zwkjVN8tLFy5crNY2NjDby93aqpqanLEbFmses1EZ4ZYO2c6YeA3wYtGBETwARAr9eLfr/fwNvbrZL0S2a9Jj62JoE95VHXFuBKRFxsYLvWcZV7HknHgCeB1ZJmgNeAuwEi4jDwKbAdmAb+Ap5vq1jrlsrwRMTuivkBvNRYRbZk+AyzpTk8lubwWJrDY2kOj6U5PJbm8Fiaw2NpDo+lOTyW5vBYmsNjaQ6PpTk8lubwWJrDY2kOj6U5PJbm8Fiaw2NpDo+lOTyW5vBYWq3wSNoq6ceyB88rA+bvlTQr6Zvy8ULzpVrX1PnF6DLgbeBpit+ln5E0GRHfz1v0eEQcaKFG66g6e57HgemI+Dki/gY+oujJY3e4OuGp23/nmbKt3AlJawfMtxFTJzx1+u98AqyLiMeAL4APBm5IGpfUl9SfnZ1dXKXWOXXCU9l/JyJ+j4ir5eS7wOZBG4qIiYjoRURvzZpF9xKyjqkTnjPABkkPS1oB7KLoyXPdvB6EO4AfmivRuqpOi5Vrkg4AnwPLgCMRcV7SQaAfEZPAy5J2ANcoml/ubbFm6wgN65ZJbivXHZKmIqK32PV8htnSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCytqf4890g6Xs4/LWld04Va91SGZ05/nm3ARmC3pI3zFtsH/BkRjwBvAm80Xah1T1P9eXbyX2eME8BTkgZ117AR0lR/nuvLRMQ14ApwfxMFWndVNjqgXn+eOssgaRwYLyevSjpX4/27bDVwedhFNODRzEp1wlPZn2fOMjOSlgP3UXTLuEFETAATAJL6mR/Xd8kojAGKcWTWa6Q/Tzn9XPn8WeDLGFb7DbttmurP8x7woaRpij3OrjaLtm4YWn8eSePlx9iSNQpjgPw4hhYeW/p8ecLSWg/PKFzaGIXbJ0g6IunSzU6PqHCoHONZSZsqNxoRrT0ovmBfANYDK4BvgY3zlnkROFw+30VxG4JW62phDHuBt4Zda8U4ngA2AeduMn878BnFObstwOmqbba95xmFSxsjcfuEiDjJgHNvc+wEjkbhFLBqXovk/2k7PKNwaeNOuX1C3XFe13Z4Gru0MUSN3T6h4xb9d2g7PIu5tMFClzaGqLHbJ3Rcnb/VDdoOzyhc2rhTbp8wCewpj7q2AFci4uKCa9yGb/nbgZ8ojlheLV87COwon98LfAxMA18D64d9ZJIYw+vAeYojsa+AsWHXPGAMx4CLwD8Ue5l9wH5gfzlfFP/0dwH4DuhVbdNnmC3NZ5gtzeGxNIfH0hweS3N4LM3hsTSHx9IcHkv7FyqzOWDK9CHBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(330+1+i)\n",
    "    plt.imshow(x_train[i],ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageDataGenerator is a class in Tensorflow Keras which helps in \n",
    "1. Generating mini-batches for training <br>\n",
    "2. Augmenting  and preprocessing images before they are loaded to a network for training <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Training data into training and validation according to the section 4.2 K. He et.al 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape  X=(45000, 32, 32, 3), y=(45000, 10)\n",
      "Validation Data shape  X=(5000, 32, 32, 3), y=(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "validation_size = 5000\n",
    "validation_indices = np.random.choice(x_train.shape[0],validation_size,replace = False)\n",
    "training_indices = np.array([i for i in range(x_train.shape[0]) if i not in validation_indices])\n",
    "x_validation = x_train[validation_indices]\n",
    "y_validation = y_train[validation_indices]\n",
    "y_validation = to_categorical(y_validation, 10)\n",
    "x_train_1 = x_train[training_indices]\n",
    "y_train_1 = y_train[training_indices]\n",
    "y_train_1 = to_categorical(y_train_1, 10)\n",
    "print('Training Data shape  X=%s, y=%s'% (x_train_1.shape, y_train_1.shape))\n",
    "print('Validation Data shape  X=%s, y=%s'% (x_validation.shape, y_validation.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is performed on the training data, according to the section 4.2 K. He et.al 2015,  <br>\n",
    "but not on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_size = 45e3\n",
    "validation_size = 5e3\n",
    "learning_rate = 0.1\n",
    "iterations = 64e3\n",
    "epochs = int(np.floor(iterations/(train_size/batch_size)))\n",
    "steps_per_epoch = int(train_size/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = ImageDataGenerator( \n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True).flow(x_train_1,y_train_1,batch_size=batch_size,shuffle=True)\n",
    "validation_data_generator = ImageDataGenerator().flow(x_validation, y_validation, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to the network are of the size 32 x 32 <br>\n",
    "\n",
    "According Ke. He et. al in Section 4.2 there are total of 6n + 2 weighted layers <br>\n",
    "- First layer is a 3 x 3 convolution with stride of 1\n",
    "- Stack of 6n layers with 3 x 3 convolution layers of feature map sizes 32,16,8 with 2n layers of each feature map size.<br>\n",
    "The number of filters in each 2n layer stack is 16,32,64\n",
    "- Last layer is a fully connected layer for classification\n",
    "\n",
    "***\n",
    "For ResNet-20 the value of n = 3 <br>\n",
    "- There are 3 stacks of 6 (2*3) layers with each stack containing different feature map(32,16,8) size and number of filters\n",
    "- There are two type of shortcut connections <br>\n",
    "\n",
    "    1.Skip connections within each stack where there is no change in dimensions <br>\n",
    "     In this case Identity mapping is used. <br><br>\n",
    "    2.Skip connections between stacks with change in dimensions <br>\n",
    "    Projections shortcut is used to match dimensions using 1 x 1 convolutions\n",
    "***\n",
    "A residual building block function _residual_block_ is defined which contain convolution and batch normalization. This function can be called according to the ResNet size that we intend to analyze. <br>\n",
    "Below is the function definition of the residual building block function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, number_of_filters=16, filter_size=3,strides=1,\n",
    "                   activation='relu',batch_normalization=True):\n",
    "    x = inputs    \n",
    "    x = layers.Conv2D(number_of_filters, filter_size, strides, padding='same',kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    if batch_normalization:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if activation is not None:        \n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet20():  \n",
    "    n = 3 # number of stacks\n",
    "    layers_per_stack = 6\n",
    "    number_of_filters = 16\n",
    "    filter_size = 3\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = residual_block(inputs)\n",
    "    \n",
    "    for stack in range(n):\n",
    "    \n",
    "        for layer in range(layers_per_stack):\n",
    "            strides = 1\n",
    "            if stack > 0 and layer==0:\n",
    "                strides = 2\n",
    "            y = residual_block(x,number_of_filters,filter_size,strides)\n",
    "            y = residual_block(y,number_of_filters,filter_size,activation=None)\n",
    "         \n",
    "            if  stack > 0 and layer==0:\n",
    "            # for reducing the dimension of the output of the skip-connection a 1x1 convolution is used \n",
    "            # with strides=2\n",
    "                x = layers.Conv2D(filters=number_of_filters,kernel_size=1,strides=strides,activation=None,kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "            x = layers.add([x,y])\n",
    "        \n",
    "            # It must be noted that activation is applied to the second layer after the skip \n",
    "            # -connection is added\n",
    "            x = layers.Activation('relu')(x)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        number_of_filters*=2\n",
    "\n",
    "    x = layers.AveragePooling2D(pool_size=8)(x)\n",
    "    y = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(10,activation='softmax',kernel_initializer='he_normal')(y)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet20()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Training using different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(epoch):\n",
    "    learning_rate = 0.1\n",
    "    learning_rate_scale_factor = 0.1\n",
    "    if epoch > np.floor(3.2e4/(train_size/batch_size)):\n",
    "        learning_rate*=learning_rate_scale_factor\n",
    "    if epoch > np.floor(4.8e4/(train_size/batch_size)):\n",
    "        learning_rate*=learning_rate_scale_factor\n",
    "        \n",
    "    return learning_rate\n",
    "def tensorboard_callback():\n",
    "    logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [lr_scheduler,tensorboard_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_schedule(0),decay=decay,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/182\n",
      "  3/351 [..............................] - ETA: 32:33 - loss: 0.2916 - accuracy: 0.8620"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-39c1cb8dbf31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#                   callbacks=callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                    )\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    607\u001b[0m   ]\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mariyahmuneeb\\pycharmprojects\\virtualreality\\vr_venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1188\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(training_data_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_data_generator,\n",
    "#                   callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.  RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.9\n",
    "rms_prop = tf.keras.optimizers.RMSprop(learning_rate=learning_rate_schedule(0),rho=rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=rms_prop, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_schedule(0),decay=decay,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(training_data_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_data_generator,\n",
    "#                   callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule(0), beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(training_data_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_data_generator,\n",
    "#                   callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d__ . ResNet-18 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/classification_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renet18 = load_model('pretrained_weights/resnet18_imagenet_1000_no_top.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
