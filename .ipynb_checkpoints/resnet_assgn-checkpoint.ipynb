{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - CSC678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "__a__. Implement 20 layer ResNet model (ResNet20) with the specification given on Page 7 of the paper for CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10 Dataset :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of Classes 10 <br>\n",
    "- 50,000 training images <br>\n",
    "- 10,000 testing images <br>\n",
    "- image size  = 32 x 32 x 3 <br>\n",
    "- 6000 images per class <br>\n",
    "_do data set analysis exploratory data analysis _ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape  X=(50000, 32, 32, 3), y=(50000, 1)\n",
      "Test Data shape  X=(10000, 32, 32, 3), y=(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training Data shape  X=%s, y=%s'% (x_train.shape, y_train.shape))\n",
    "print('Test Data shape  X=%s, y=%s'% (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels,counts = np.unique(y_train,return_counts=True)\n",
    "plt.bar(labels,counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAD7CAYAAAAFI30bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9WZQd13UluG/Em+ecJySQAAiA4EyKoihSM21LrpItD2W3XavdVau9Wj/2qvbq+rDb1Wt1f/Ra7S9XVVd1DWrZXnaVa9muli3LklyyLFkjKZGEOIIAARBTIufp5ZuniNsfe99HZZoYUgSTiVxxfl6+9yLiRd4TEXfffc7Zx1hrEVlkkUUW2a2b926fQGSRRRbZnWbRgzOyyCKLbIcWPTgjiyyyyHZo0YMzssgii2yHFj04I4ssssh2aNGDM7LIIotsh/a2HpzGmE8YY143xlwwxvzW7TqpyN5di/y6fy3y7e0x86PmcRpjfADnAPw4gGsAngPwy9ba127f6UW22xb5df9a5NvbZ7G3se9jAC5Yay8CgDHmTwB8CsB1nRCPx20ylUIQBAAAD3xo+4bfJ2IEwHG9xnwfOrZeBZC1fa/H47hHv++212QQ2pDfh3xvPLPlfMIw2LJf/3PtZ/RD7tXTdr7nbTmvUL9nsfX4tv85bXZhddVaO4K9bTv2a75QtEOjY+i0GgCAXqcFALCW4xFPpAAAiSRf/XgCAODJH61mDQDQaTe5n66P7eNuNO7ZXB4AkNTxbNADADSbDZ3RVv+3mjxuoO36fpFjej1uF4buc76PxWJ69XXUYMt+ITfDZrlyJ/gV2KFvh4eH7czMzO6d3TYLNcC9Hv3W94P85Ln7sH9f89Vuefej26lTp67r17fz4JwCMPtD768BeN/2jYwxnwbwaQBIJpN46JH3oFxeBwAkPQ7MYIL/6sGhDABgZDALABgu5QAACT/Ok02meVCfp72+UQYAdHrcf6BUBAB4QRcA0G63AQCtFm/kVJo3WqAboKEbtlgq8LiWn3faHf4M+LvuwZrP8XyyWZ5fPM7jNbW9dQ92L7blOD09QH79//yPV7aPzx60Hft1cGQU/+J3/x2unT0FAFi5dAYAEAQch7GDdwMADh49CQAYGD8IAEil+f25008DAK5ceBkA0K3SL772LwzQr7EUr4/HnvwQAOCu4zxua5PX0+lXXwAAhCHHvdOl3187/QoAoFJeBQC0O7wuuh36dX2ND9xag9v3An4/MjLI8x2k3wNb5fe8vNBq8rr7y7/4mzvBr8At+PaH/Xrw4EE8//zz/QfYO262fw4AgGadfllbp98GBwcAAIEm5nSG14OfSHJ33X+hHplb4dDOzff96/r17Tw43+qB/vfW/dbazwD4DADE4nF7+rXTKK9qIFI60BD/GA6IJEx6FABQD3lD1AIhAUOk0mjxxmg0dQMEdOyqoGsqxu0dkvD1IEsmk9q/zu91g5nWEADA00h39cBNx3heNT0A14VYMhk+OI3HB6vRgx2aARst3lm9Ll/9WPIthmrP2o79On34qK1srGOoxAeNHRnja4wT0sTBIwCAIOR4eCFviLDB8WxtrHH7Jm+IqWH6/+D0XQCA6bsOAQAmpw4AAEZHefx4nOPaK/EGmj4wzvc9+qvVItIsb/BBvLrK6ymWcBceHT4wxOOkstx+s7IBAEimeN2ElucZlx8rm5qw23dcufJNffvDfn300Uct8Cay221rNzYBAOvXLgIAZs/w/WaF9++TH3sKAFAQIHIhm/5K5R08t7dz7GsApn/o/QEA82/vdCLbAxb5df9a5NvbZG8HcT4H4Jgx5jCAOQC/BOAf32gHD0A6ZgABsENCmjNjXIqNammUdojOQfY2kUirSyRo9XkiraW7luo25PfFQSKQXpefJ+LcTtRZH9q3Bfm7PR4vo89jWW6f0vue4QznifvquaWA5u9clr9X09Kiq7Wco16qlc0bDctesx37FdYC3S46bf7fjQYR38zxKQBArc7xc0vnwWEtveOct48dOw4AeOLxRwEAU2NElsUi6aVujI7LpOQft6QT99WsE1G2hfAzafpjoETkevTIPQCAM2de146OyqG/igUuAUW9YrOyxH8L/D8c97mxwf+j2Wj3/+07zHbuW7zJCb/T5n7HM3xdnL0EAHj5mW8BALrisOM5+qup+6owyOeGW6K7Jfs7edY/8oPTWtszxvw6gK+AdMLvW2tP37Yzi+xdsciv+9ci394+ezuIE9baLwP48q1ub4xFyvSQz/Nnj09x5hhKk2uKh0QktXXO9EHImaMpLswTIigoaBQTIixvkrRXEBSDeSKOqriQjjjNprhHF/3OKcjT7ZDb8hSMiIsLDRRkiglatoWoEi4qHPK82jVyYhAXmxRX2hOpvllv38rw7BnbqV9tGKLXasIoyyGZIGLfFJc9NE4EefBecpaj05MAgLiDeELo3R79f3aBnGfj4go/93g9vP7KSwCA954kgvzQY+915wsAqAiBXL3C1WdCwbtEglzr8AgR8NXZ8/xcwaZas679eb6xOP1dKPB7F60Xxd3nzpPJxK0N0B6ynfoWeHPl906bBce1q5XA/CxjM4UMr6dMiTGQ5Q3e72sLcwCAsWkGG12Qoh9V9965844qhyKLLLLIdmhvC3Hu+MeMwUAyhrQQXVFc4kiBUelAeZWiIuErb8tFq9uKyvbz68Q5Bi7/z+d2y8uMegZdHqna4AzWCIhccmmlH7WVx6mZznErvvIDm3UioEy8oN/j9y1F9ZtdQpBQc1y5xu3LDZ5nzUWNu/t7frJhiHajjpyim4VBcpOPPPgQAGD6yDEAQFWc5OsXmRFTkV9qZfprrUykubBIBF8QxwmPiP2Lf/o5AED8FzmeH37/B/g+zvEeH5/UCRE5loVMfvAC05xiisJn8/RnTyuETo2/r8unn4YU6Hpx6TAeiEDd9VdS+ltkb8+2c5sr67wOLl++CgBo630+payaWgUAcPYlpp+NzxwFAJTGp9wBf/jlHUHM+/uOjiyyyCJ7B2x3EadvMFJKIR8nkkyl+Or5nBrSipJ3xZX1o2RWCc2KngcdIozQirMUMrAxzkjVDjmrIODxG8rz7Om1Wud+c+vcLq5E/EKNv9ddJMJobhIRHRwWNzdKrs7kyaW1lX9Yq/E4m1UiztVNIuDLs9wu8Hd1mHfdjGeQTMbR9clBNdPkoC9VOA4vfudZAMD6GqPfc/OMWsfFHbvxb/fzL/k6McJxW14U1yVOsVom4jh3iVHXiYlhHifO7Semmc85qderi0S4r7/C19EJItnLV+lndFU51NEKRlF8l1WRjHFF1Gzx80JBK5A7Kz93D5tDiBzfuWvXAACXrvJ19gLzOIfzvK4ODDM2sXCV18Urzz8HAHj0IyUAQKaglcA7SM1GiDOyyCKLbIe2q1AoHvMxOZJFIUGuK5chgjBCjm7mMeIu24pmepo6hvKcSbJZcmmVTSKGohBAVVHzK3P8vNYm4kyoYmwqI240LkS4Rm6rbRXVF8dSLBA5PXEP8worC6pRbuj7YSKQdoPHq9U4/yTj/Hx6nPu7CpelCpHo5Zeu3nyQ7kDzvBgymTEsl+nXC7NEdq+dfpXfCwkGykpoVonQfSHNZpsIslzla1V5mZevsXQzm+Z4njh6gj8oZPrdb38DAHDo8GEAwPETzAcdGuJ14ip/igUiQ6/HFUC97bI1yJ02y+RCg8CV5tKPtQo/L4gTTWqF1Om4fFVXG7/fzZVcbodw14F01r24P7S/05z4e3hNmg/KUnF50FWVwF5bYsXXkl6DgPm5B0Z5nLPPcUUzOj4BADj+3sd0XPrfU8mzceF2/bw+7j9vdmIR4owsssgi26HtOsc5mE8j1iHSSwqJZJKMVrabyufTzFMqMc/TRd06AZ/zXVWgZCS6Mb9C5PDGFSKKlSr3V1Abh5Qn+jMfZJT3wAT3+/9OkTt55sIigDdr12Mef69aZh5ho8bj5/OqSQ9UE5/i+4SQSMbwfU8JfweVr5hfJ3L5+j5FnL4fQ2lwGBdmzwEAFi6Te8zEOW6bdUbJa5VlAIBRfmtZYh5l1ajHkhy/4TEiirRWGFMzDwIApjXOl156hr9r6K+uSsJWVsk5338/xUTuOsYa+WlxmrnHHwYAvHxW0doWVy7tuDhOEFm62vTFReWDuiyQgVH9x8oLlurS/re3rsHZrgbW36wf1XbqYBzPPtLsI0/36ox/HZQiU0ZIv1LXOKsi6NVZXkdpccwxceKnn/4mAGBoiiu9gQP0v+m5laxTTxLC1X3u/QglRhHijCyyyCLboe0u4ozFMDo4hOY6EYZnxBEq77HZke6eVGsaXafbSWt2ObOUBjgTdZSHd/EakcF6RVykouu+EvMKKX4+GiPyS60TCR0rMOq6MMjtlsqcydqqtX7hHBGUp0qRblb5n0XOaE4+rlgkYs6rprklDsx2yNnNjGRvZXjuWGu363jjjWdx9o0LAID5hTcAAIG4zHyR//+JYzMAgPtO3gcAWFghkriywu1Gxjmuh46Ss8wPEeEtqUbcrhLJXr1CxLiivE8VEuHHjxNp1ms8rtKCYTtCJN8jUj12giuPsSlGYb/3LGuhF5for67yc1tN7rehfNB0jts7nc96o35L43Pn21vjK7MNqfVr2kOnh0oHdMVJJxKKafR33Kqf2VerGmCWxAc+9BEAwCsvngUAXL7EKHqgrJsLPleKqRmu7ILXWRH2yje/CwB4309xpZHOSBbQcZruVT/b24aozS2E4yPEGVlkkUW2Q9tlxBnHwPAIBnLM1/SkZ1mW/mFX0VQvcHmcUnAXF5rLkZPqgq9nLhIR1tuc+VNSz0kluH1aqkUDPhHEqQvMH+x1+H27SMQ5MsDjGXFcrma6oRr2uqLpHVW+GCFfNzHFVRNrVSsbV2VJT7qeNrjzZHR2YvVaBd/71lcRG2PU++jJ+wEAaeVFnryHlUMnjjMPNmipptjT+MLViEsh3iey6/boz3qV0dSiViSu4ufqMq+bVI41y07l6MjRGR5fuKBZZvT77Pdf5OdNntd9H/8EAOD+B8iFNZ8n4nzjwmUAQEZIpVga0n/K67Ki69WpK+17s9sgWv9zx2HaLV/3xBGfv0AE2JQWwN0nuSJISszB21bREyq7JdRj6YknPwgAuHqJ/v3sf/gsj6+VwNUVxUoyvE6OaeX4+refBwCMiOO8+0lG2RviWuPSwEjo99el++kErh2ivZFFiDOyyCKLbIe2yyUtBvDiMMp3dJZUdDqDrE6Kz3OnPN0V8kymGWVdXSTn1FjlzH9EUvKS7URKSPPEUdauevqiJ6V2hxhiPmeafIK/OzTAmtejx6i2cukqKxLOnuOMl4g5HUYi415PeWLiVOMJHt+1GnCVT/1eSfvUup0elmdX8fCD/xAAkEySWxqU1MDEJJH8uvIlZy8QQXZC5VcaaQbEVLljpSbVc/mfrhcRv88VyYGtqWLLk//Cvm6k49r4kkvx92cmqeGbUqWaB/rx/vvIqZZKRLpfaP4NAGBxgdfJ1Kg4NMPryFUoVSoV/d6ZG47PnW5uXB012Y+WK3ukf3kLwc3OkYP+qy9/EcCbqlVPrDKG8NEPfwzAmx0Z3PFdNqWr8Mvlmb/7yU99EgBw4XWuMP/2r7/K44qLPjtHrnPASEe3xRP63n+jH2NDXDl4Y/RvvczziYsEX6iwQmmzys9dq50b2f6+oyOLLLLI3gHbVcQZWotmqwvTdflvnDHqdc7cHakI9Tz1+mkQoVT0OjXN07U9vj80zBnu6CSRXqPF91PHmfeXsJw5NjYZ5U47rmqNUGhalQZlKZQfuZtcXGEgo1dyMhsrVR1HM5UQjmc5Y3Y1c7meVoFmQicHuFsK2u+WeV4Mmdwg4vo3y8pOSA5yhm8oK8FN5OkBdakMNUAtlw2ht11yh66Zm6d8zVBZDLkhIsCEJXL108r3TYgjM9zfBPKTtALiWa4M0jm+9trSdZwj9z2UJVL+1D/4OADg+ZcuAwBq4tRabeb1tpW/WcqXbmF09oOJ8xO03JBGw+YGx99Ic2BxhX5/5nlW8pw6Tf3Uyroq9BQbuPd+ZlWMjnDl4Ms/lSr9VpZa1swBcuKTB5hd8U//p/8eADA7x6yN779E1at2nX4/f43IMzPO92uvsnKt8ec8/aNPPsLzr2nF2uBzp23UQ0odJpzi/40sQpyRRRZZZDu0XUWcFhaBCfrciENi6RS5iZyU2+eV33fpGmf4mKBMYon5mq0lfn5slEjzqY8QKb4xxxkwP0XkMDzEqPnyChFFqSQEEqriR1Hw5RVymLEUZ56V8gIAYG6BHFg8zvMqFVRbrbawNra1r7Pr0+6iha4P+D4PqiORSGLi4OH+/9tqcSZfqvDySpSILLo95fGJ427WOL5dy/2c2lDP52tGGgSjQ/SLXed10RGiN4qOOlUt16XUVf4Eys7wpMbl9Fpr9ar2F3eu867oOklnqMf5ofc/AAB4/Q3mD776GhFNTZ0FnML8/jULoN2/rl3YfFNK+d9++jsAgCvz5AhXK/TThsbXE8JPtXnfLa+5/b4NAJiZIefsuM453e9d5d02GzxercpXUcs4+V5Gy1+8wLbPnSpvsGtSzXK9ww4U6Z9Lz/8AAOAnFTuZpH83e0S4/TbClufr2orfyCLEGVlkkUW2Q9tVxOn7HkqlHHoxIoKaFNOtKoRcVOvK1SV9T0SSTvH5vnCJM8qYlKCnpthvuzTJqGi8KpJRUfoDDzJ/K7VIRJnucUYLwN+tS+F9IkOE2lE0z2Sl+5dVrXmJyLW6RsSxvESOp6va9Jbyv1zRa1YK8p2mEGtiaxbBfjNrAGv8fsVNo0rEkRQSrFYURW9xnBpSHVJrH+SzRAgjA0QChUEilJGSupPGmE3RTPL464fol3bAlQG6rieQ60rJAwdSXzJCnKVBcqFhoO11vsUifyehsHFZCMd26b+HTtL/pTzP84tfZLR2ZWn11gboDrVmq4HTZ15CTHqkDgluiIMs19TjSb1/iqOMIQxqPIeGeV+tvEE/nXmVCPGrf8uoeLHA7Vynh3ZH+dLKgvlvX1EWg+Cd4zozUid78KG7AQAvfIfdSxuKy59b08pBHPdAj5z6he+d4nmP8P5c1/UR7/B9z12/t6B6FSHOyCKLLLId2u5G1YMequU1xDoOcei5LZIh5qtGXTPZQJ4zRkn6m80NIs7RSc5sUw98GADw6jXOhOcu8PWJCSKXcpnvx44yyu6BM0lH0dGSao4ry0SQadWYT6hPczkgwog/oD7O4j6/++UvAACuzfI4fh9Rqg+8OM2uy0ftOr3RfWrWAr0OYlKXErWE6SLH4+4jjD7nxGX78ntdnFhLlRvpLMfpxDGO//QhRlW9OFcWrjfR9ASzIU5cYhS3oDzewQGnzM4ViQuOqiCln9/baynrQd/HHTcLIuKhYa44akIe9TJXGlMjRFA/81M/AQD4/Jf+9lZG5461er2Gp599Gk1xutkU78dPfvJTAICeskpOvcJa8mJe94m61U5Kj7a7RG56s67eX+eJEAfEOWalZZAb4PimsrwviyU6zuntFgr0SzpHP37kY+/jcVd5/bz6KtXOgi6vu6tll3fL+zO2SL9XN1SBlhc3niYHPzfL+7tSubkGQYQ4I4ssssh2aLveDMc3QCDuz+niecrnDKSOsiGAVqkoet1WDxrNTO/96EcBAAdOPA4A+PM/+H0AwLi4SV815nMXme81foTyOakh9g7KWuVxrUvXL+RM2ZHi/KryyUoj5E6HxmcAAM0aZz5PIklBgjOai6p3lafm+osbqcO4CqP9avlsBh9+/3tw5B4i+/k5cl5Til4eP8aKrPERclS+ap+rVZffp7xLb2u/e6dN4KtPe1yItlkn0n/kPiLRmeMzAICuuqC6GvWedF2t8gx9hWW7LVWquHxblx2REumq922tFGKqOAukIzsiRPqBD7Kv+5997qu3MEp3nrXbHVy8fBGb0gQ4dpjZK+k0/TM/z/vnyiVWCuXUtbbvT/WcaqozgEtsvusoo+JHR8hd57VSWF7WSlM15xPT/J1qhcdznRxSIZ8TBe3/45/g82BdK9Klazyv1TZ3yGxqpep6RYnLnsrz+syOkcOeu3wZANBR3viNLEKckUUWWWQ7tF2FQgasdw00k7u8v5jrASIFeKOZZXCIXMZ4hjPWI4+yp8zJJ4g0N5aJXJPqJXNElQahDjA+Ss7EcVqNsuuWqd4mTdVCgwjijTnmo73yKtVVnnic2w+Nk1OtVDmTKa0TwzOqkXb5mh0hTCHkTam3tKuZWxmeO9YymTTe88DduPdhIs7mfUSY2aIU1bWdVX6rJwQ3mOVMrzTO/izuav1dlBO6XtqqWT96F7UE0qrgatY3dRxdztJ5tcbpQqo7qnH5toreqgIoCJXfG3MrIJ5JdY1I58ol9lB68gNUkG90iUgyDqHuUwuDAPXNTTRaHKdkhiuAfvbL7GUAQEl+DpSlYpQ9sbBIfdaFeWYfGI+f/+LP/xyPX2O2xde/8w0e72WuVIaK5KgXz6sScJL+3uwyWo4478PBIXKo959gJVLnZ+j33/+9/wQAaKrr7HyZzwmI+25LtaumjgGTOv+Eek0Nj5KTv3r5+mMTIc7IIosssh3a7lYOWSDsBWiKe0iIk3R5Yr5HpHbXODnHVJrP9ZlDrDB48APkMiZOsKLjxWf+AABwcJrbj99LHcjECBFPLEMOpNHijNNU/uDSPBHExhIRZiBOJp3njDqsPLHZ+RcAAGMTVFnqNcTNNjlzGvXSCazUe4Rw0uqdkxiXGlNyfyMTz/OQzmaRkx5qVt1Eofw8F902DnE65KeshtD1NXcqPELwPWHVfs2/ovG5Erkpp6IThK5kyCmKq2LI7Sjp70DXWb/7ovI+jSpjkjpOXL2tsk43VFHhlYtEPAdOcGWz6tVuPjh3sIU2RKfdREN6txcuEUH+xec/BwD4zjfZ48f18lmqcDxWrvD+UiunvpZDYpz343e/xcqhtiqQXjsvXd0lrjDKK9y+NMT7cUXR8Momz2NA+b2dgPt94xusDEoXuDIcGCaXvtolomy0uf+cEKjV/ZjR8XzV2JfUHdXVzv/g2RevOzYR4owsssgi26HtLsdpDOJ+DBuKWgdSM0pnlN+nxLpRcZuzC+QIjz5Cpe4D939CRyLC7KqnTVHdEEeOs5dMPUZEcvoF6mm2pUBdUd7gqvQC/YCII6X+21OHiSwfOM7oe88n9xWXInk8oSirZH4aV8jJhIqi9zQN1ZSPmhni/mOTTkF8f5rv+8gXB2HFXTbE8VrV/Lb1vi79zI6yD9rqs95zPZ3EZbrsBFfB0VDtc0/cZ35Q0dgi/VLKMw8vpZ42QegU+hU1V9ZGXiuKtWWpHSm7I1RWhYHyPwOed0GVQocOkktrqseQVbS+mN/fvaT8mI/iYBESLUOlxuj0ay8SiS1dYg8oT4+RTMxpQHAcXa8nT9kzB7RyG1S+50aDSP7IDDsHXAm4giuvEykGSfp3SdxpoxHoeyJ/o/usZbRfg1k0nrIwQl/nIdUsV1kU6HrLartckefjepS5Xkk3sghxRhZZZJHt0G6KOI0x0wD+CMA4GCD9jLX2XxtjBgH8KYAZAJcB/KK1duNGx7JhiHazhUySP2vUJzvuKd9OqknpHD//6f/upwEAT/zkUwCAwjBn/qWLVNz2tV9ZUb6Vy6xImK9yxvjG5z8PAMgpWtZqE2GMjxGxFIQYLl0jJ9PR8QYnZwAAx+9/D09cFUTrZXKiTvdzoymVHglJttTLpuYUslWLf3IPyjbeTr+WyxV8/gt/jSBO7mpjQ1oDm+SwXIWOQ55LS/w+EPk5qPzOgWEi86Q4prp0HM+dp78r0i6YPsz8TV8VIYU89zt8mNHXA9OM1h8+IoQjTisvDYNQUVQIsXR13flK7/C1/diMkGxBuqtCIgIyGBws3GhY3hW7nX71fR+5wSJiuk86a0Tcq+d4v0zneB8ZIcxqk9d7S/eRSRPhJ5WfvbLEKPqp71Onc0wK72sb9POmshxq4kabq05hn/6IaeDTcddNltfTiirKAsljZWJpnZcq91JO/0gHtlzZ1NWvvaJ804Eh3ajhzWMSt4I4ewD+ubX2JIDHAfyaMeYeAL8F4GvW2mMAvqb3kd05Fvl1f1rk112wmyJOa+0CgAX9XTXGnAEwBeBTAD6izf4QwDcA/OYNjwWL0Hb6Da+NuIaeZgDXbzmV5Ez+0HuI+JJCFq+9yCj3xjy5jLZUVKpSop698BoAoGZVaRLw+5yiuwXV2o4McKZcWGINck/cWqNKRDOrSgjgNI8nxehUjOfXSxIhrfV4nmnNrBnVvqalK1mVwrSrYNlLdjv9WqnW8NW/exqlA+SqbMBxfOHpvwMAHFJ+7fAQkeGclLp7ug4yUorvSK1mSSuApx57PwDgoQfuBQA05G9PFUCXrlIn89x5Xg+vvMrro1RktsbP/6OfBQA8eS/zfxNKGD0wwSyNjhBnX09VK4Wui8rHFG0v0b9pIZjQJ9LZi5pXt/V+NUCY8GCVlZAQBxiXmtnBgrIbhPSqQoy+asq9hDQmltRFUt1Gq2u8n1alp1pWt9CZR5gts7hCjrO8wf1yOd63LXHMXemgthQtbyorw2VRpPS71vC+DoQ0fXWf9XrK7xVnvqx8a9fcMpa4zX3VjTEzAB4G8H0AY3KSc9bodfb5tDHmeWPM83W1IIhsb9nb9Wunc3Ph18h2396uXxu15lttEhl2EFU3xuQAfA7Ab1hrK8bcWm6itfYzAD4DANOjeQuECJU/F1MJjutj3FH0c0xRrq98gV3yBseI/EYdUpCaTjxOZJfLqgZVM19WCHVc+oDNKqmctJTF11bIvXVV6ZOXak9HHNr5F1g5tHCWeWLtni4g6To6LiV7QFHVrKKHSSKilBDmAHjck/ce1mj84MaD9S7Y7fDrzOFj9hd++X9AcpS1zI0qEeX5V8hlTYzTb65raVpdJzshx/X4fdxvYIL3cmOY/v/kT/4YgDeRfF2I01FQPeWBtnr8fHmZK48rl9gpIJPh7yxeI4K5fJp9vj1lRVxcZP7eYz/xKADg0Ax1Ph3n6Un3FXGtkNzKQV05E67EbQ/a7ftshZ0AACAASURBVPDr2NSoLZeraDd4fWc7vO5HxjlOa1c4fhcuE/mvdDmug1IX83Rf1UPlO0u1qNfgRNtqKxtFK82VRd6X9RoRqO3y80ySz4mOOFQjxfieKpQS0jZwXVBbbdejShViet4k4/RnQvnGuYzUlvTa1e+56/RGdkuI0xgTB53wx9ZatT7CkjFmQt9PAFi+lWNFtncs8uv+tMiv77zdSlTdAPg9AGestb/7Q199AcA/AfA7ev3Lm/6aNQhDg4Q4x5T6aLvSEKu8yVC6mKur6vGywtd0l5xhKAHPwQEiytKkatKVfzc3z+1dhYinGmZXo+5LuT2bkj6jTsN3f2gGDDpEtp4gTqXBmbOTJFLKT/L36mlyJFXlD7bqnI+GClSBGR7de3mct9OvxgDJhIdzZ9lVsLKp8XecoaKfNeVxOvSTUoVVV2o0myvcfukqOc6//spfAwA2pCi/KZ3WvFRuilKMzyrqfe0akeboMKPpqQIR7Le/xOOsn2dXxEDX14VFRvevKU/02Eki32Iho+OTC0+rRruY5fnGFaXNZJI3G5pdt9t6v4YGaMYB1+beELGpqSQWFC1f0H1TUw041ugnP648XHGJVvdRs+d6jgm5CwnOaSXoKsKMoukrGxvun+N+6iUVV4eBgsvf1crVXXcuSyItNtpzHK1+z2g/q/Mz+t4zN1+I38pS/UkAvwLgFWOMq0H6bdABf2aM+VUAVwH8wi0cK7K9Y5Ff96dFft0Fu5Wo+nfQ72/39+ypnf2cgWeSSCU5U1hxmtk0Z/isKkAa4kqG8gmdJLfrbBIhhMoba6gYdmyMHGIoZHPiAUZxn/67r3E/y5kvrhmrKQ6lkJcqiqJtvjirmjiwSwuqSJCeYNsQMY0c58w05Wpm1R1vY1W6gS0h2ilxrI2bVyLstt1Ov4a9Lqpri/j6X34JADC7yHxXr0tk/vLLysfT+Pd6jivkeH/1i18HACTEWT/0MPtfdxLM86so6nrxKleXa2vM6+y0uP/84mUAwKXL/PzRh5mN8c9+7X8BADz7vWf4u5trOh4hVFMrkovPE+F++xQVwLMxItK4Kk58cWp5Ic4Dh2YAAJ/6+V+6leHZVbudfjXGIGbi6ArB1aTRsF6hP9cVFOwpy8H2VMnjuEhxjV3rot6KDSiP1ldWg4t2O5WsPmJ03+vVRc0dBelUybz+cZx2gZCn276/v9f/v/iHsiRCp5sLvd48CyaqHIosssgi26Htaq26Z4BEzENDM76vvMpQ0e6GEIqvyoCkU/6Oc7uE1I6KBb5fVB/sxhQR5ug0a8znlsmV3PveJwEAtRVyXxfPMTpfr5GTjPn8vaJmQKN8r4U5bn/1ijjOJH+vMEZkPKKKESNkatbVTW9DNe+j5N4OlHheF9SPe79aPJ7AxNgEjs0Q+VuNY0x5mX5fh5PztFXFUEL+h/LyJifJTX7k4x8HAOQz4hpTjLK/9iqj9OcuSNl/agYA0BJU8bVyefUce+C8do5ZEZmZkwCA+XkeZ6DE11FxXJkcr7P1RUaH1+aoArSyyuurFYirFUe3UKafn3hqf6tehUGAWrXW78FTV3qS6w7rgFuhxPshmd7K+brKnbR0MOPqd+4QZFxI1SHOwHGhQpzQisC99R3UdDGIwCHF3pb9unofwHGd/L2YQ7baLpVSZZNDzE4lK3lz7jpCnJFFFllkO7RdRZyxmMHYiIfuGrmmpqJndTWVs16g7XhaBenrJZSX2ayTW0lrhkCHr88//TQA4MgJRUlVmeI4kYyit76QreuZ4mbQpioeesr3ymnmfOJhVpykxIX2fM1k0u9szqqSpcqZazRDTu7h46x0GS2xtv7UwqVbG6A71Hq9HtZX1vH4+54AADzxYXYfTSY107topau8EeflKzvC5dM2OxzXtWscr/UWucb1VeZnXhTSnF+mf3OjzCeE+tibhPL9elzRfPWb3wEAHDpKndbpQUXblWWREafabjGqfrHCFUlO/g4s/b24wfze4eEZAEBDlSpf/+aztzZAd6j1ej2srq31/dNqqYOCYgnxlMsyIKJ095HX97fC73q10u3suTxZF/VWdoJDqA5iOgTqzHGTZhuF61S0HAKNOQSp+99s4zbfRLRO6JUvKeWdRogzssgii+wdsF1FnImEwcHpBIqGCOHCLGeKJeXvdaRClMtJHUcVQkHIGd/Xc35dtazVGmeuVld5Y1Z5fjlyWEuLRCrXxMmEmvHGRohkjboibpQZPU9m+fulIpGjq81td1wRK2fYepufd2qKnqvm9i6p8kyqR9HsNSLgtZXGLY7QnWmeZ5DNJLFW4Ti/8PIpAMDoKP0wNspsCae3uSE1HIgjjskPU4eJIKcHOP5z5xjlrteIIEfVjTAjFRtfFUgNRXEnJqiOtDjPqP6q8gknJpU/6qLD0gGFNAWcQnlSK5GkkElnbUX/IP08Jk614/RGHXDZpxZaS21UcciuU4MDZEnlUTrg5tIfHYfplP8D3XcOEfpCoL6yFrw4j59wCv12K4dptw203NVfwZRKvB7c9dUWIg7EhW5Hmo4T7fV0HQR6xdbfvZFFiDOyyCKLbIe2q4jTjxkUBuJoCoENjIoDyZKbWl1SDatmjFhCNc3SBgmlytJVhdBmk0gxK06y1SDyaLYYVe9o+6DrZi7+Xk19mgsFVR4UGK1vur7qazyuU2XpcyRSVUlI70/UGhKaOWfumuFxGtzuW9+iWtPL5/Z3dZtngGQ8RLtFJPn008yftcrHLUjhv6uulS1xYTHN24dmWMt+3+P3AACOHiTyLM8SOS5u0J8J+fnoEJHnygpXIq7L4b33U53pT/7zH+n45N66WnF0Ouo542RwUqokE4SaOcxKr+XZ1/WP0a9prUROniTn3VLvqemJt9TJ2DcWi8UwNDQET5U3gcsuUKWQQ3QtdcE06l9v+vmR3K6jWIbvekPJ3kSmwZbjbucwXfTedSftyX9hsDVq7pCki6p3pS3g8ji3I89+fug2pBmGN9cgiBBnZJFFFtkObdd7DsVSMaQKRAKDOXEnqkiIp/mkrygfEoFT0+HMHqhSKGgT2STUTTGuPDHfJ3JtK2rretu4aJ4mSFghD8l1Ii5uBcozK6s2tqma5mLJqS8pWqjfa6iiaWmVUdkNca5V9fn+228wn3Bpf1OcCMMQjWajX9Lx8Z/8JD/vqIugkGYo5GH7lR4cx5RWHItlIpdqmfmX605hX/l2r794EQCw9gy5xyOHiTDfexdrzJ16Tlp+tE5nVZ97UpZ36kpNIYuYoryHDhBxtmrk0O9RvvCzp6jzOX+FSLSpNBDbuKGA+h1vvu+jUCggDFz02XH+HNeKkHdMqmG+Uw9zHKFe4q5rqcY7dMjO9fYRQnXdMvvkKNxb6We66wdbszM6kqt0HGfowuROA8Mdx+WJ6pOMriunneG6r7qsnhtZhDgjiyyyyHZou4o4w9CgVosDPvXvclkigXiaM0BWpGGxqJpx9QKpVdTDRjXf3ZZ0NBOMXqeU59lTRVJM+WEJTQvxpOM4+EFGUXul8/XzyhJp5Y+WiIDW14kkq5rZCoP8vYbyPc9fJjI5+wprncdUUTR2gPtDlTPDitJfWtufwrCeZ5DNJVDU1J4fIRfYlj9Smp8TUtexisYmM+oqqb731SrzdH3paI4eZbT0aIYc5/lLzOOEVHniyv+bW6Bi/5B0PN1rR91N222uAFzFS1tIqasa+JhUssaksnVlgdfb0lX+XkuqTG+cpmbG0BC3s1Jn2s9m4PU7M3S6TkeT17HrRuo4RLcic7qYTo2sLU7SbMurdAivn9+rGMK2LEvXKQhW2/crjKR14MX4edzfqsnvAOybUXoh135hkvY33pb3vW4UVY8sssgiu+22q4iz0wGuXQHaZSLL/AhnpFRaXCKBKAYHeVq1OhFBWb1KNtakQkSg14/Shdvyvlyil5sV3EznamKb4k5VGIK48gh7DeZ9BoquB+I+y1JTcumc60LCly+oN4q6/3Xq3GC8yKjvyUOsVNHmeO7i6g3H5061MGyhUT0HKJ81bujIJfWaOf/aZQBAStkICfVDH1ae5+QwsxocYhkqEtkLuKCl7InRUSLRqUkivYVFVhCdO0dVpJkOa+Ud0q2q+2mjQQRZ2SSidYgz6EgbQVoEp19lvqnL0xwdZeXX1AOM2o+O8P3wCP2bSu7vvuqw5AVdd1KHMF12ghunjuOw7daouItap5S14IlLDLbVljvu0SiLwe3vkGjC3xqNbyn/10XRXQ27+z13XHcdNNS/va8DK27T7deTypNDnqlUVDkUWWSRRXbbbVcRpzUxBPFhdBPs8dIO9aTvEYmlipwRSiOcEQZcn/MGZ6TyOhFLeZUzS7MuVZWeesNYx5Wo94iiqQmp4Lh8r6p0HJvqex63nDnzHrnI0CMy6XZ5/GRWaiqqbS4luP0REDnd/yCRx4kHHgQAzNxFlabHHidSvTZPhIPnLt7CKN2BFlqEnRY8zcOxrrqKKgvi1Pe+CQBYXKKfjcbxsceom/mB9/N62NwkQnz5B98HANSFLM5JEf7i5csAgKZqk122RKpAzrFSESetvM96hUjVcWUx5RkW8+Q0Jw8ToQ4MTQAARidV+fUwa9sHFVVPbNOFdBxrX0Byn5q1Ft1ut480+zqVQnT96HMfKdL8bfqXrmbc5Ve6/dwK0TgVI3GUrsZ9e96lU2p397M7/nYEGo87bYqt57FdTcn1HnI9jdz530p/pv3t+cgiiyyyd8DM9jrQd/THjFkBUAewl8m+Ybxz53fIWjvyDh37XbPIr5Ff30V7V/y6qw9OADDGPG+tfXRXf3QHttfPb6/aXh+3vX5+e9X2+ri9W+cXLdUjiyyyyHZo0YMzssgii2yH9m48OD/zLvzmTmyvn99etb0+bnv9/Paq7fVxe1fOb9c5zsgiiyyyO92ipXpkkUUW2Q4tenBGFllkke3Qdu3BaYz5hDHmdWPMBWPMb+3W797gfKaNMX9njDljjDltjPmf9fmgMearxpjzeh14t891L1vk1/1rkW9vcC67wXEaY3wA5wD8OIBrAJ4D8MvW2tfe8R+//jlNAJiw1v7AGJMHcArAzwD4pwDWrbW/o4tlwFr7m+/Wee5li/y6fy3y7Y1ttxDnYwAuWGsvWms7AP4EwKd26bff0qy1C9baH+jvKoAzAKZ0Xn+ozf4QdExkb22RX/evRb69gb2tB+cOoPwUgNkfen9Nn+0JM8bMAHgYwPcBjFlrFwA6CsD+7sj1Fhb5df9a5NvbYz/yg1NQ/v8B8JMA7gHwy8aYe663+Vt8tifyoIwxOQCfA/Ab1trKu30+77ZFft2/Fvn2Np7Dj8pxGmPeD+D/sNZ+XO//VwCw1v5f19vWGPxELOb1JfNd3883PeT+2npOPclBOaFR97R3AsauKdN2KX7fd/JVElrd1gTKuvdmy0tfVsqXDFlcMliuGVTQby/Kz91phBJQTsS9LcdxrxubzdW9Lgbxo/g1Xyj+xMjoBJzfXIsSr98sS/Jibj9sbZ73pv/dB1ubbPX3s1ubKri3fa9uv9Vvcm1f79s3d9t2vWz/VBteeeO1Pe9XYOe+zSWTTw/n82/eN+4+SahVje6zjO6TjuT+ynUKBwfXu7/c/an7ytcNndJx8znKvLlnUy/YKnTclIBytVrfeny9+u450O/9tu1C6V9m3MA1d5MaZf+63KzXr+vXt6PH+VZQ/n3bNzLGfBrApwHc73kGY8MppNVzxp14zNuqm9cLXfc7fl+WcnfKow5fVs2Cqup94qn3TDqp77PUUSxKaXxjg8runTr1P934ddWtzw280+t0D75ilrqgEyMM0s0tUUm8Lin4QoGf97o8Yl3dLQ9MUak8Hud5Ov3B//rFl65sH589aDv2azKZwu/8q9/vK3mnpfidkNJ26PN9T/qVMUjf0nVB7DeVke6iekZ1zdYeNF7gbgDduBr3wHPXy9Zz7Os42q03sHsQBNj2gN6mSN7vKOC+12uvf1xu96ufuu9O8CtwC779Ib8iGY/jf/+5n0WzzgeVL7+YaeqXljO8jx8o8r67+jK7gf7VM+zNVG7z/vL9rUAirt5igyNU3C+k+f2xg3xGfeTJxwAAPQGV1U3q2cbzvN/OXOBwf+0bz/CkdV5Jd99KjzMRo/86Ok6v65oQ0W9JXZcN6fFutOhXT4+FLz79/ev69e08OG8JyltrPwPgM8aYf+B75ktx30fQUxtPN5NImLTtpPBjTjBWwsYSni3ogdjRTBOqLWgmTgcW5chMmo7JaQZbVfvh0Kp5mARMR+S4DbUDTmm/yQlSJL7+ndFRtmqI6/tLs/MAgERc51fieeXUSWGoWNQA8ft6o/4WQ7Vnbcd+LRQHvhQaIJbkeHc08dU3KSwczwrBy0+ui1aon+rpARm0eF20NjkhOqHZQO26ak3eQJ7h57lsUSe3te2s2Y5Y9aBzSMI9OMNtyLXfPnZbK5btyCTc9oC9g+ymvnV+BYDpUsFuzF1CTPdpPMZN53QfnW/SXw+cZFvlUC0oxoZ5X6X1/ZsrEf58Qy0tNtd539UMx7ndot8ffITP8m6DAsWra9xuLJXW7xBIpZPOjzy/0Txbttx3hELiK8tzAIBmk9dhrSZBcY/XaTLG583kOK+jboL3/QW1ermRvZ3g0DUA0z/0/gCA+ettbK398tv4rch2zyK/7l/bkW8ju769HcT5HIBjxpjDAOYA/BKAf3yjHYwxSMS8Pgc2MMymXHU1R4sHRJpO2t5xWhPjnAnGR7j9pQts2zoc40wxrpYHXm9r29GCEOKQ2vNaX8hUiDCTJZL11aJjZIwzpeNaqhUuvXtWzeRK3G9KbUxFcSIW53sH/UO3lM9zyW67dxQy2bFfgzBApV7rc8CrK2xid21uGQDgp4TItdRKehwn176141YgavrVqBIZpNViw7VZrnaIHDod7njk8DEAwF1HD3F7Rw0ICfYRoVuh6Y/QQU/3sn1Jv80cUvLc/rij/PnDtiPfdkIPl1pJNJq8DxKGCBAB7wNP7Z5Xr5DCOjV/DQBwdpkI0bZ1H29rktZVu2CImkul6edyk+P67CvnAQATQ/yddm9r7COp+y4ed9wLX04cPQoAmDnI68GtVBcXLnOzLs8/N0CqIdAKKJPk9TY5TMQ662euNyR9+5EfnNbanjHm1wF8BYAP4Pettad/1ONFtjcs8uv+tci3t8/eVrM2LdNueanm+x6KhXyfSxwdJZJcXiNCcW1ENzfKAICxYZLFySSRaDpNJDg1TYTpgkDdjpovgTNgMiHSt0nOZHqSv2MVhUgoiNTpkCMdHnLtafl9u01OMl/gzNNUEKq6uaHvOWMODRPJprMKAomriXV4/Jaii72243ruDNupX2v1Op7+3jOo1cVBgn5qtokQWgH9G0/w1Vcb4UCAoaU+zYGQYDbB6yNtOK4p+T/w6K96neP5vIIRy6tcbR5R87Vhx7FlFJ0Nt3KW/Ta2Oo+bRt8d97mtedgdyHHuyLehAZq+wbqCbyYgNzmkYGdOwdGWgqLlKr+viKu22s+Nu6/PY44hdEFVcaM5jeuzL70MADiupod3Hz3I/RL058wMkWU95HW2tLDC362qD7dWOI9+6AEAwIvPsVlgUyvZapfHWavz/AfV1HHK54qmVYuatUUWWWSR3Xbb1fbAsVgMw8ND/Zm6o7aeY+IwM4qaJdXWc2KEiLPbJQe6tkrOLF8g0osp/SDsuKify+PkzNVsKDdWE4iX4nHbnaZeOdMlhXRrai+bVR6ZmynXFP1LxjmTuXyxjvav1hzSUj5bRWkQSnfKCRnvVwuCEOVas9+u17V7jYkrzhiXr8dXtzJoQe1aNX9XlX3QrPM1qTa8OUv/OE45nuR10lJ75zdmGT29srAIACgVuIKYPnAAADAiLr00QITh0t98uzWK3v9/XJQdWxGmSz96M6q+J/LB3zEz6CFp1jGRIVIraSUxOMDxv2R1v6SV3qMVg/N3N0u/dcVhtxRND+RvtyJIKBtjXGlOkwcYv1qVfxcrvF/f9z6mKa0v0c8/9/NPAgC+/MWvAACeefp7AICD9z0CAPjYA2w//cYc23Jf+u5zAIDNDp8fNSVunnwvt292eZ8PD6duOjYR4owsssgi26HtKuI0ADyE6LQ5kwRCbD3HLbaILGNKmK2U17UfkYkVApxbWAAAFHOcOTIxIphKm1yLQwiJlGY+zXhd/Z6rXAgV3QuViZ0UQnLR1obyPxNqWJ9QlDeTIhJJiivdLJf1yt/PpZTHKeScEQLarxZai2Yn7Cf89yt7AnFd4KvROLugdkdRzq52y2cY1axWeB1U3MpAK5SE8n3zCVe5xff1Hv3kuNP2qji3MlcC2RwR0sTEJADg6GHmHebEhSd1XJcV4JIgrBL1w23I1AHUYH8DThjPIJGN4UieK8LDlo4qioPGJqPomRLHsZ6g38I4/fzoQ0RyY4plXLxwAQAwe5UrBM/n/WZ7vA5S4kTf/z7ut8LD4dlvfgMA8Prr5DoDZeEgyxVEWYUttS79f2GBXHo9pP/qyrZZLnO7dorX2bFDvA5KY7wuVhRr+djH7gUA/PvPf/a6YxMhzsgiiyyyHdquIk5m0lkkEq7G29WicqZ3lQMDaXKCcc+VZHJmanVUEqmSrY5qVjsVcmIJIQuHTExc0VghkrQ4VFdqmS+wJNPllxlFxR1n2VU+phHSdNtByKTdEGfT4fyTiHEmKwwOajNyQ5V64xbH58600Fo02y20u1tL69x49St4XNqdIKd7rSsan0oLyTu/qUSupayGnnHRba0oxFW+Of2LW1Xlmduu2uDxN8+fAQCsrq0CAPJaGRyYIhc6IA40IQ7VIedQ0VhXy+w42cBuLcncbxZag1onjqKv7JVVcoCzZSLGDzx4NwCg2eH9N6XxSWU47o+rou4eVeg1xAmvKqbQUJZKwNsYMeXpHrp6CQCQLnPcB0d4n3ZfZRaFQ6rPvEZ/vj7PrIqW7vO5q0TCy2uMtj/28OM8bonc6f/9Xz4PAOg0yZWeeo7Xw9IS88Mfeerum45NhDgjiyyyyHZou4w4DTzP6+fVpbOKjgpJJBR9DsRZQNG58bExAEBvTdilxykqK46qrUqT4jiRXqOxFeENjzE6365JrMBwxoo7JOmitKqFTib43ksQQW7qfLpd5aNJbamlvDSIS3GVKzEh3laXv7eyunKzgbmjzVqLjg1hgq0VO6G3LR8uKe5THHaoii2lBaIrTjMRk9ZAmuPY6JAD64HbKz0UbVVwJRWt98VJOjWmbiikKI7cicgsrjM7Y75NTuvClasA3tQumJwkMsmJQ09phWOFcLsSK9kuArLfLAYPI34KUxrXgrJZXtwgottQTOHQOKPh/2iZebRxrQCHznO75BuMSQQh75cZXRZxJfJ68neg+7L97A8AAEUhyHBYzwUH+ZW1UvB5f7aVhTGoBUjG8jqqLFKjY+rkcQBAXqI9jx2lrOjyJu/PxRqfF40GYyoXz5+/6dhEiDOyyCKLbIe2q4iz2wswt7LZ5zazbc4guSJnlJY4xZzPmWFqQrXNGanrkBLBQIZIpJThdvlxIoW28jfPLZLzKJVYK96uc8dWgwgkruN3K0KOyi8LlTfoi2Or1ci59FSQ0FEYdaTEKPugKifOV5knNiSOTIdBQYg67OZvaXzuVLMAevbNKppASK+l8XOyeq5SKKYKIMd5uprjmLsc+zXmHO9cX/9RX7vCE23XE0nm9FqtkEkgpBn4Lgyu8+3rMTp5Om5fmed1cmXhMgAgqehxRvmGjrN1Ufi45Mv2q6V8D3fnM8iKE/aV/XJc+bHVJa2k5Mgpl8eZ0P0qBGe0whSVibaQP7RijMshMfktLl23bl4rCMUSeu2tcoBjuo4+pphIR7XzwSRXqKnLlwEAjYR+WIj53rtZkTTR4P4TikUcP8ro+l2qWQf+y3XHJkKckUUWWWQ7tF1FnNZatHsh1tc5E2WktzcoLjCu00lJ2LKlyp+akGJfcFhRzrZqY0ekw/f6eUbjcikihJwEk9uKyg5MkAM1gZCGZjKle6LaUj6nOK3FJSluhTxOTsLILeWROaHVtCqS8llObeviXFvKV83n3Ay2P81ai3a386ZuZbhVbain8W+2pYIlBOkLISZjyucT122s8iddjXjoasz5ew1xzB3J4njiHjtOKFcIyAohdZUf2G8Y4Du915b21/+h/ycUpO2I867UBVVd+LfNz93/u18t6LaxPn+xr07U9DmejSKv53RDFUFnGI0OlKfbk3aD53O8kkKSBryvevJP4Pwr5L5dYT82yjzLfJn+aCmppXOIK7uBnvJ0W/ydnqLwtWVyr4357wIAFp5/CQBQuJdc59oikXInw+eBW1E2pPtZiTtsfH2LEGdkkUUW2Q5tl2vVfYwO5tFrcabI56TL2NsqzZ9WNNUhhIaU3jsiuZKCiCdPkKtYXKQeYFscyLBq3F1+aKga24yQbKfBmc5X3qAvRFJf50y12eBrsUCOtNYQt6KoYFIzZFfId+rgtH5HvYUq/P8cYioN7vl2NG/LwjBEo9VCzEG3cCtX2azTPwlV/AyOkSNLO1lGIUjf+V0c1+YGo97NGlcehw6fAABUu/Tjxgb9lFRlV1crF1dp1u81owWLe+8qfhKqaPJ8Rd+7Dgnp/3CcqdSywjK7Tqyp9hl2f+OOXhBgrVbGbF1ZDeKuE4bqZJkBxhbWpLA+Lj3adEtZBxXlO7sWNVKtyh7nfdsSYqyt0r/JUPejYg7tFR4XScUOSkS6MZcHXOF5pe8lMoWyYDLLhJD1Oeabls+yYim8yuswP0iuc73E63NtkeexsMwsgMOJiZuOzf72fGSRRRbZO2C7ijg9Y5BL+jgpfT2njuJJ9mZxlvlePeVvZXOscS1LJcVX1Mz18qmqp83KMqN+3b7sJRGh6zESSsG9IfWdmmaqQoYzT0fIwxohHyGnQl56mxnXdE1cHmAK2wAAIABJREFUZj6l7bZycZeuEpEY1c4nxKVVxeXuV7OwCHq9Pjk1oLzYghT2mxo/GHHZNSKClFYQTpe1JZ1WpwifFlftu15SWgGUskQE48PKhtD4t4QoG3q/uEKE0a1TSyCu6yCm2mg/5Pl0u4r+S/k7FBcXKj8UQlSV+csAgPYGj1urtW9leO5Y69kQG60WFlV51VV+psuLttP0W3KA90lSWSqxeXGIyo+siYsOVNkXPyR9TVXqZUvcrnuO+bSusq+llUf+Q+xg3CjzPsfrZ3WCwn0L/Lwdys/jjI6Pf5gVQ8k078P1c+RiSw2+Lx4iQr6qFWta2RfxuAvDX98ixBlZZJFFtkPbVcTpGyCX8JHNqBZd0dViidEtUY7YkErJ6TPnAAA9cU5JcRiDUkWZF4extsoZp9UjUqgIifY5KqUFlsuMmokK69e6ZzKcgQalBO96IrWlnuQqnZqqpbdQXpmL7judQUV/05mt+puxW5jB7mizFuh1UBSCLwlhzi0QQTRdhZe4TKOKjsNDRCyj06zkOKuaYyuuKyMFfdem+ZVZRkdz40Q+Oek4Xjr3GgAg0HVROkbl79wkubT6FdY0++JKC5YIqlEjQmlUWUmUiPP6qrRUCVYishrShVnD1nbSTmULd24PohtaIpHA9PQBeJd4n6UVfQ46qthSHuxGneP69Cw5wskW77+74XRvOW5N3a+dH9BfTdf9cor+bx0nd9roEfk/cJRIs+7RL00h/sSmONeCOjlcFVJd4nURH6U/G2O8vuKDvK8HnqLqUlkr29Iw/fxIjj2Kvvod6e6Wbh6TiBBnZJFFFtkObVcRZyIex4Hx0T4yGygRIfgqtYkP873rZvm1v2OvkFC14KU8p/rFBSnHDxCJlJRXVlY0bXVZSuAD5MSyyq8s6n0+S4SbV7fLbE55nepRdFEN731xlQ2nwqQeRR31HPJVc22EONKuD7hm4r6+Y3t/c5ywFl7QxbjyVZc2OON35a+YuGJPfu5JafvQI9Q93ND4dQbEaUqjwCvQv2Up81eF+MMGkWK7RQRb1Haz4rTr6rJ5qMS828kTRKDl1+iH+hz9u7HE10qd2wfizDabPO/0AJFHflpZGsorbkmn1eWP7leLx2MYnxxDdY4rusyAg9qq+JEWwcIqx++zL7Hv24khXgf/TL1/Mi5PVipY668Qca6P8P67qKyFjhDo5HFylAcH+H1ngRxkTkjRiJtGVWpaHrnTivKrg4vMerDzfA5s5Hm+2RPM5pg8zJ5FLXGbI1ohPnwfVyjThw/cdGwixBlZZJFFtkPb3cohWFgb9pXWHWLruh4zvio+4lvz6Tzpcfaf8sqnPHRIXQ2Vt3lgQepG4r4KqoH3ddzlZXIsT6h3yfgkZ7aeJRKpSL9vQ7qDa2WeV0zRtpFhzoCuMiaUOk5RSGtD3KrVTNxR97xAtbD71WK+j8FCHsNSEyqvcyYfTNEPSfmzp3EYPcp8zCMTzH89fZUIoSRF/Z5I6NFxIkZPtcN15fl6eW63sUJEcWiUCKGR4H4bAf22vkF/ehOM4h64h1HWuWuMyroKsLi77pTg6ev6apeJnFcgzQKpbnm+U0e61RG6My2wATaDDcQs82Xj0hzo6H4oq+RmvakKMSnEV9SvfC7OFURJXUw7UsOyloh9M+R4XlumvwoeVw4bkkP9wtwXAAAnxIEeHeT3Q0lyofXLvJ+DJve34tA35Hfnz45Wgt1NIufOy1Q/ygjhtnWdHrqHK6Du/JWbjk2EOCOLLLLIdmi7ijg7nS6uzl7rd32sVjlTOKTh8ikD1S5nxI11mkIqI1JL8jjTHT3Cmcj1/vE007muea4PuycEaJWP11ZlT7fI4wxNEEl6mkEPTRPBJFPktCrKA3TK9TFxcK5W3Vd+ZyAu1Be3Y5WPmhOnul8tEfdxaHwQP/eTHwMAXLk4AwCoqkKs7WqJ2/TjzCQRoMtWsMNEEJtCmnXlDR4YZlTUKS/VVMFihSByVhy5OPOxIv1fXybiqM0pP1AqXFlVLE3e+0EAQNglklqeZ35fQ2pO0PEKWfo1puiwdbqhDWVbuPD6PjUDi4QNERMCH9bKr6NKq5j81WhxfKfcyu8wVxJzytdFvwcY9zc9IdeQ98fEECuKYlqYVbSSsOv03/wanxObUkU72FbF1yoRJ/R88MRRN3vcviFtASskmxF3vTCnXknSGqgrO6ak63P4geM3HZsIcUYWWWSR7dB2FXGGYYhGs92v6e4oT3JwZFDfb1VWn57mzPXaq68DeLNv+sQ4Z7aREReVl46f5BETSf5bGel1Oo4TTSKbZoVIcn2FHJb1VPOq7pVuv0KeM2VFuoKua6PrXeQqhFyNdCGtfuw6z4JmyPj+Dr7CNxYFv4X3P0Ik+di9XAlUpT7lFNO7UmzvNaSW1OL3hzvcvqFshZryN13XzA35K3WY49lU3qwtEanMLTLaev4S8/nuGSBSvbpCvzmF/iDFFUzuEPP5Pnh0BgCwPkvE+foPTgEAlhd5vWWNBGAV9W0FPI5RZVJMjm31bq6mcyeaF3pINzOY73FFNqr7ZKDJFVhsWZV+VY7TyXsYczh44hgAYP0ljuOEE6iNqzJH10O6phiCuMaMKsTOvXEZADBc53ZHZvh8uJbg/bd0gb+brkrvU9eVkX9avuNUpXJV5/v1oKrfYXZNVV1v69K4WJ8jNx87OH7zsbnpFpFFFllkkW2x3e2rbgw8P97nvJJCbG09+ZMpRU2lyB2oB011QxUeqvw4fJB5WGn1sMmpYqU4oC6WqnUOxHG46P3wMLdbVr7nghDJqVdfBgDcdRcR0/IKf2d+gVxZT5VCJSlIx5V36HQ7e+I42y3OyCp8QWaQUeGK8gv3q4W9HmrrG7h26VUAwIEpIo+pCSpxx+SfUNxwRZVerpJraJB5u/WmNAWkhlUXIqnWiHhOHKUKTl1ZGC3l3Y6klVco7us973sCALAuvcjLi+QyO+K6AmU7QHmakw/wfEce+HEAQE+16Otnvg8AuPTqcwCA1TdYyeYl+PteTBVD7f2JOIPQYrPexTc2pR5FN+FJ5VGmlS+d6pKLfPg95Lgnp5kP+VfPvgIA2FQecxBTXrMQaFq6nK1rPI6v7rBHpLrUCui3mPKwH/gAs2HWJRGwfoorxrbLconxOmjquNmsTlgK8c2EtCWGuFJtqZfSop4Dm6qF3zgb9RyKLLLIIrvttquIMx6LY3x4HMk4n9cZRcPT6inkesfENYMUUpzpjk4RuZTEgUyOEsnlkuq+p1rmlioIEiGPW9FMmVLvn3iGJOjiChHgrKJ2r18gwlhcVj7npqLuXb7ec5JqPDlFBQNxd447c0rnKeWnBuJujVSfesH+zuP0PR+ldBbVNSKHBXGAw+P0a1HjkM3TbygSgfpG6jfK2ysqD9R6W/M5z7zGvMsRRW0zGa4MGkKkD86QI/3wo+Qum+K8XOOAY9P0x9IaEer8IhHG4iWqWV1Vvl9LyDhdYvS9dN8nAAAPnXg/AGDqElcmLz/9ZQDAyuIljUDlZkN0R5oNuuhU5nFhjfdHs6teXweICB+My38Khx9WTKKQI3Js635uq7dPIk4/tKzey8+JDvdvqjOEp3zRUPmiS7quNs6w4iijjgvVFPN7q4ottHX9uBVJZpjnsa4uqVXdl15XK07pcHrKgqnoestWNm86NhHijCyyyCLbod0UcRpjpgH8EYBxUAbmM9baf22MGQTwpwBmAFwG8IvW2o0bHcsawHoeUpoh4qoEiSfVU6S6tX95Mc/o10MPcYZLx7fq5cXEkbreJVDUL6l8y5xq0BOun7eUyeOKtr12llG/urgwqOKkLa4s4bs8UCnVu546UoyviGNzepsxX/momkF74nY67b2n23g7/Rr3fUwMFmGkgrO+RO7ppZepvP2CsiLGpohIPvjhDwEAplSr3Nog8vdjgp6e8y/9dXCSnFTaVSIlpJea4HUEVRJ1A25XFVfaVFvNM+cvAwA22uSsHzlC5Fob5fEvLRDRnLlCZPvSRZ53NUmEPFzg79wzRmT76IfIhb7wzFcBABVVGO0Fu51+LSQ9/MShLFbWieyeu0Q/ffUyEVn6iGrR1ckhLz3TblWcpvQ267oPUlp5BP5Whf1Q9+O6atmtNAgSytvtlpWP+QazJjLCex1Fx19RvvTlVfohpcdBIlR2hjpGGCn8t8pEtnVLhBrTcyJQhduhgdKNhgXArSHOHoB/bq09CeBxAL9mjLkHwG8B+Jq19hiAr+l9ZHeORX7dnxb5dRfspojTWrsAYEF/V40xZwBMAfgUgI9osz8E8A0Av3nDY4VAp9tDta6a37wUwsvMr3LR8ExaHJiQR3mNM1xbiHNTFQkOYVhl/Ls8z7hUaxqBkJ5qijtStckoz3NR+X9tS4607QtpCsn64lIaqhTpSR3J9dXelFrPorrjWUXpXJ9poxk3ndxVKvmW7Hb6tdmo4+UXnoNdY41vcYiI7tRpIrizQnxPfvQpAMB//uP/BAD4qac+AAAYSIkjlt9jqnFutnidjEi3M0yq19A2BG+c5oFwgInTnxeusELkX/7uvwQArC4Tabzvcf7uJ3/hVwAAo8oLzqpybFJdHU+XCV1C1VgvX+X/d+wgOfcjJ6gXee6V799oeHbVbqdfU3GD45Mx/I/ilKeTrNT5+utEhl+7zPvloUPUfKi9Qc63LD/4WgmWO/KjOORAXUy7qkhaUWXYaobItiXONK8sjKw48VArOaypR5Guh2u6D9fEVY8roTuT5fHyqlS0yqZYVbZOzNdKR7GO+yzv61y130riurYjjtMYMwPgYQDfBzAmJzlnjV5nn08bY543xjzf6tz8hCLbfXu7fm13I7/uRXu7fl1p7O+g5tuxW4ZCxpgcgM8B+A1rbeVWe0pbaz8D4DMAUMqn7epGGZOjzK9yyLMXqr/6EKNg1Yo+7/G1LaTn+mqfvcCZzVPFUEKI4+AMZz5PnEtL/bAD7d/TTJPU9mV1STwnfcbDI4yeD+bJvcUGyaHU63wwbPSUVyYOtaoZbEOvoXX6nOJS1cOo3th7HKez2+HXXCZjV8oNnI2TQ/SXqc94dYGI/kNPfQQA8Nv/278AAPybf/vvAABf+iuq39w9xeshrjy7rLjtQPJDg0VeFyODygsV95kQ8veETGqu37q483//H/4AAPDaWeYTJsWN/8UX/isA4MCJ+wEA9x9jbXJaebkFqflMErCgp+PVxZla5R0fmjp4CyP17tjt8OvD42nb7jQwqIq69x9nrGG1zvvu1BzvhzNLXHEdE/Lr6P6wUjerqkLMtlVJl3Lf64bWqxv/qlMrE7IfuvduAIDauuOVr1Cnd1rHPaB8XJdPm1J+7aai5/U1PkfGhWgnh3m9JdRTKq7utoeqRNLTpdvDccIYEwed8MfW2j/Xx0vGmAl9PwFg7zDkkd2SRX7dnxb59Z23W4mqGwC/B+CMtfZ3f+irLwD4JwB+R69/ebNjdbpdzM7PI64aX4cAp6elrydkVqk5xCl9RMdZqib4zAXqN8b0+byUoYcHyXkWi5wxzp9ndNSqFvan/yHz8ZKWiGagpLw99X9eK7NCKey42ndf50POra6a5YbO21MvnZYqnVzepuu6uFHjTDbsEhX3kN1OvyaSSUzN3IVAupXdLhFDQhzThHoKWfXDnp5knuTf/uXnAADVRfotowqgZNqNlxS+pZaVE2LIKCsjIQSZSnB7p5q0IhWs08r7+7EfI7f64EMPAgD+388SiT7zrb8GAByR7mdCvadWFxllf+k8K4XiygMeK3C7oCnuOrH3svlup18NDIwfg1HUeqJERPjEYa7IKsqPvFzm/dqQJsSo8jl9ZT20dB+3quomqqyZhNTMivq93hJXLAWtHNpaea7r/ioNqKeUovFxceBT4jATjuPO8jowcX7u1fjcGIvxfASg4UkboaHzKorzPHowdbOhuaWl+pMAfgXAK8aYF/XZb4MO+DNjzK8CuArgF27hWJHtHYv8uj8t8usu2K1E1b8DXFd48Kmd/JgF0LMWa5tEYgWpEDmE6buKAUWn61Lods0ErfKy8uqTvKxo2IuvkKPMpjljtVsuWCEOVNHxM+e53ViGXE0+SyQzPs73a1eINIyi88srPN6BA+REAhWht11lSl3K4KFTrNf5FYiMOuJu6p291wXx9vrVoocAgf7fRJIze5bAvu/fJelkrq6TE7u2SC7UKpsipX7sLo9XDBiSUknKSmfV6Z+mU7x+Uuq/HgrxXFU/dZfd8DM/+7MAgCeeYA37rLox/sUX/goA8MJL7HIYSENhY0m17WuMIscCrkwaPXJgFzdYceQq3/aS3e771VoDK33ShPQz7xmkP1YmpMyvLIeeuP5hZVWkcsSSZV0Xrl96T69tn9u7XlQF3ecO73VcBY80IOwi2YUD+vfiUkHKN7ndqM/rZ0MIOJknQg27PHBPvaoqbcVOlG0TaiU5cQ/jZYcPRl0uI4sssshuu+1qgmHMj2FgaBiFArmHlJDEuroYpsVddTucCpxeZ0y17QmnFC9dzOV17teS8vOgaqEPHCGC7KrHTaXKmebyNSKexIgqghQ9zUk304xyhiqkCZVqZeaLXb5yGQBw9DijqB0hmU4glR0BSodADyoan05J/am5P9VznPV6AVbLa+j2VEGlJYKV/154mapJ9z/4Hr1nlNvlXXZUMdTpEnksLFClpqWKk4RWIk7X1MGpuLQBnG5n4JTiFd0dHGZUdnhIWRzS9RyfIKfuehL9zd+w9ryl2ve1NXXLFJcWE/fqy+8DY0Qko2M31228s80gNB4Cl5+slUFRK7KHp7VSky5mZ4mxBtdDLCFuuKVxdLqsnvI3A60sjLIVetquE3ce5v1pdB0FqsyDOjoEUm63QqSpgNeDVc35Yor3fVfPjZBuRFwrzYarodd1MyIdzlTs5iuJCHFGFllkke3QdhVxBmGIaqOBUDPO5Bg5hYSQputfnlUNqok5lSHVqCcUvRbCbCi6mUiTFcmpn3NXlR49VSCkSuLAFJ2tinM7doTcVk8qKT0pj2/WOIMeu4tK1tdmqc/XdapHGraaon6h5p9cJqNXzlh15an6qpjYr2aNRWBCGCGCmrpBNqVDuqg+5//q3/xbAMAV9a2vaWVxYU5dCcWFufzNbiB/qwLMd1FTYU4j/1vly/aJPalVpbPcb22Nv+8qviqbRJ5tVZxdvkzO0yEbBXFhxZ06rtVF8bNJXmeN+v5uc2k8D4l0Fr7GoVOmPx1SnNR9df8mEd+ZslTG5llTXmlynGvKMmlpJeLUz3pWakVq5lRXrmlDyD4mf4fqGRVqBWKEOF3+Z0vPiVAItO4+Typ/WtoSqTghZyjVpqw427vGeH8OJBS7WCvfdGwixBlZZJFFtkPbVcTp+R4y2QwC5WO6Uj3Xu8WpHvm+a9IjTkS9hGLxrdHptpCrUZQ1U+T+1arjTMmxrEjhORbTzJKWHmiJyDaXItIck1rPqkRjMtLvHB3dypEJKDmqBQXljeYL/L3KJmesVSmdWy9344G5wy0Wi6nqi35oiitsK4/TM65Si+MyNMKVRnGQXGHPKXhLp7HXJRJwHJaLsofdrYi03XYVZcKE4qo8XTdl+eu7T38XAPDRj34UAHD6tTM6Dndz2Q++zj90nJwQbyC1LHS43ewVRtX95P5eSQDA/9/elwXHlZ3nfefe3hcAjR1cwSFnOJxd0kijsWVJXiTLihUpTlmx40rZVUnpxamyU6mKHb/EValU/BJX8uAXOU6VkiixXbFVliUnjq1dGkmeRctoFnI4JIcECBDE2nv3XU4evu80B9CQBDQcEEDO99Lo7tv3Htz/3nu+8y/fjyAE8+kBJ17VDXg+smJox2bIPC/O0R59RakTKcWv635fVoVXVfe3kd1cVdOGbu9F3WDuunG+5cGQ9JqVva7pObAhUYqm9nNYN+iIrp9QMZGpDFeo71D++Mmj/MdKHXVlTW4fk/CM08PDw2OH2F3GaQwKxRwCo26FrteQlNSLin4ZRdNyLoyq/Lwh1Sx3ld/Vz3BmyORT7Y8+kFC+NhEX9Duc2Ra6ZICjh1nJEi0wL6yoipZClcebGCYjWl6hr2Z0WAmJor5NVVKcnmFtfGqdipJ65qi2fVRMNDrgWgkWFgnSQcVURnbMK5/T1ZbX1EsG8iWmYnpB6CrJ5DPWjJ+I8bn9OmIZ64Q2pd/Y6zkdV/0uTjZ9/rnPfx4A8IMXWUn0zLPPAQCM7JnIOxrrAC46b2MdX5UszoyBKtYK9qCLmxggDdBTPrVjfs7HaJWPWVHlzvgQz7/rHttQ3uWGtCGeEjOsyY5DYrJlMc4oUFdZV2kEF1MgQvlIc7peSje+AQBkpF1R0n5SXSd9Re2L2t9wRZaM5INd4+/qQ67v+x1WR/Lw8PDwuAtdLnNhgJKiz85XFWomcP3PE+VpxvKNWM1YjYZ8aPJdud8VpLbS1wwTdfja3iDjyMk5U1XXSajGPFJ/7zDnKl7IlKzyAp3PMi8f6oh8craufs6K1nUb9Ol0pNtZ0P83UKRxVOmAwsDAmBBZ5dsa18deM33WNbx3rkidl7zzZeu9RHVgVDvimOVA4d9uZqhj6inj8nWtmOINpioFcmU3LF5j1Hd2ll0tGy3XVbOD1w/wh5inju+OG4j5BGJe7frKLc/PfkaS2kG2g9M9dXq1Vkr7zq6T6kb53PPM2125qi6x8m1eF0Os674uyU5qOTZQLbNO9Urn2d1HGWXFOLvUB88JZVfo84GEgK6LVPsNMmKi4O/Wm/S5h8rnzgf0WZv09o9Fzzg9PDw8dohd93GWc3lkNPO4p3ZBNcdN5f25qHouT2ZYLJc2v9cPO4peT02yosf5REbU9TI7oZlRhCVSf3TXdbJYoW8mq7xL5zKJNMONTzAqnNMMFGrGc/3UrZWivFR7im4/Gn9HTKYzYDQHExYG1oawqtkf5Fm6dLvUqU259AhFVbVB4DbU5+GWfL9I2RduheLs5JhQKF+Zs6sjsq63VFEVZYePqYJEv+soeusYqxunY1aue6n73F2XN6L6tP/8a67b5QGDMQiyWajxAox7lQqYS0tI5GueUUeHMXWzzKqCa0jXhasgctHyWLqZLZ3fjluYiUmG8nW66ylwK1DZxcqnOagkU817VuMr6jgVPS/K6siQHaTfyo4drhj1b6AUlG51VjiW227h4eHh4bEJu+vjBJC1FoGYQU4zw2BGEUNwM3xODCWOHSNQJZC2G666PEHuvyD9v1RMolRR7bvy/bqKDvbkEynJqZaVz7OlipeCFMg7rt+zfp+1UudRVDUIyTwTTT/tjnqsrK9tGrdTKj+osKlFv5sMGKRrYpjdYk+nfuXybp1OagqXz+cYiWrQi6o9Vi8o5wO7AUV3xUDc+Y4GHQPSTZ+3+84HKt+0oqcDX7R8s1bfO9+ms5/LDnBwvvqDjCCTQWgH8mR8HTBO5XfqBqwYnvf3Pshskw3Vgn/nMrNZllWp1RXj78l+qdOxFY9zKluBcdeFxhJsjhWETktAHxel6F5StkRVNfXVgOMe07BL2mHWZe9ov1bPpW739itEzzg9PDw8dohd93EWc9nBjO90/kL1Lx8aItMb+Jo0MzgGZ8U4h1URVBn0NpFPUQJ7xlWiRJzpqqpgccFt5+JoKY80G/H4HXXBjAPOOMsbrDRoqqveyIjUYFocT6HofGEcx5r0QRtirq5yqVjcewrwdxrWGjgG6PIoIZ9SXr7pG75KV3nC8z7I/4SipvI5xi4KbzczUxfNdteHcT7RvHykqkBz37vrzR3HdVMNdD2l+j52WR7KV3S1zwOf2pbsCLdCOrAIAiBXwEClyP3/Yt6xzmOqx4hjbCokws8/ynzpKVX8nb/G++jaoIeXfJ+6f3suT1faA9atQORbdj7mgU9T97lcpSiLueb1u7x8oEMh7VoTAy1r5eLU2bQAGlyfbXN7DYIDbnkPDw+POw+zdRZ9Sw9mzHUALQDLu3bQnWMcb934jltrby8vvc/g7ertehdxV+y6qw9OADDGPGOtfXxXD7oD7PXx7VXs9fO218e3V7HXz9vdGp9fqnt4eHjsEP7B6eHh4bFD3I0H5yfvwjF3gr0+vr2KvX7e9vr49ir2+nm7K+PbdR+nh4eHx36HX6p7eHh47BD+wenh4eGxQ+zag9MY8yFjzFljzHljzG/v1nFvMZ6jxpgvGWNeMsa8YIz5DX0+aoz5G2PMK3qt3e2x7mV4ux5ceNveYiy74eM0xoQAzgH4AIA5AE8D+GVr7Ytv+cFvPqYZADPW2ueMMVUAzwL4GIBfA7Bqrf09XSw1a+1v3a1x7mV4ux5ceNveGrvFON8F4Ly19oK1tg/gjwF8dJeO/Yaw1i5Ya5/T3w0ALwE4rHF9Spt9CjSMxxvD2/Xgwtv2FnhTD84dUPnDAK687v2cPtsTMMbMAngbgG8DmLLWLgA0FIDJuzeyuwNv14MLb9s7gx/5wSkq/wcAfg7AAwB+2RjzwM02f4PP9kQelDGmAuDPAPymtbZ+t8dzt+HtenDhbXsHx/Cj+jiNMU8C+F1r7c/q/b8GAGvtv7/ZtpXh3AcnDpXRbEjWy1AI2AkDD1opSDYsEzqhVMqEOVmpSBL6vVhtSyVblclJVs5sbtY1kB9T0ygMZMKcrN2W5lASnksSCeRGkhmT/FWabp5vYsmkOdkyJ1PmhHpd87DVq63lvS4G8aPY1QTBBzPZLIx1irNqvlZQqwx93O/yPFl9EErey706QWrXYsM1aXMtMZyQcOra9kbJpt9nJTicQoLFkoVz599sET52cnOBa5WBzfJxW++Nra0+3PedVmfP2xXYuW1LQ7WnalNHXnde9J22CQaPVp0PvUus3bSdE0B297VrjeL2l255BN35p/Pm8d8eHOfC+edvatc3o8f5RlT+iR8agjGfAPAJAA/nixn820//NL7xRXYbrBbuBwCUS9ThzOrBVimA+bI3AAAgAElEQVTzxhkfppJ0rXQEADAyPMx/aJn9zi9c/x4AYOgwm4WMHWbvkGyeD9ROiz2JCgU9eA17z7g+2UlCvc3aEPc/6AMOfr5Rpz7nyjWOq9vk8ds96XvKIGurC/y8ze3rzQ19H+t7ju/T/+abr209P3sQO7ZrEASYOjKLwCnklzjBHD09o235m0uvXgUApOrhVB2u6pUTaCXH383MTAMA1pu0w4r0WEfHqIfaX6NeavMau0vWqtzP9HGuJJtxFwCwscLvm+pCGupyj6TbulGnnYo16qVGmgAHuqHpZt3YnPQbi+qR1ZfS/Pef+t5+sCuwDdu+zq7IFYr49T/47OA8uIlM0yFyjmiIePTVW6jRV/dYxy+6vB+HStRlHaqoi6namzcidVRw/dVFXFLXx92+Efm9OQYTHzZ3R00HT84t+9vyQHUT5O9++MRN7fpmHpzbovLW2k8C+KQx5sNDY/nPh3mgPM4HyfeffQoAcHT67QCAapkXcLcvYeKGZvQRHio2NEDtEId971G+dgp8EDdSPijTOg2ZT9iMzebV9Cvh7zMhH3yjQ7wRS2oPHLV4A9ZbvOEbEjC+fI7nL8zLEFneWHPzixx3hcdrNiSIG7tWGa7Z1xucqb2LHds1CMLP28gObrCOHkCLC3zgTY7TDoWMY5a0c9YJ2K7JrhOcuI5MjQEAykXat612zOjxujlzhg/I6R/jxFsp8obMV/jaS7Ui6XFCrK/zAewm5utqW3vxNbXGGOXEHRbUjE0tIIpDvMELahtdLai5n2O+okrff+p7b3DK9iRua1tnVwA4fPpRa8MsUvcz1yTRtcBQ++ecax/sVopqYWEkFO1+6B6ErS4nttBIcFqtLgZtl93xUjfo7T043T/intdh4JpCuqZ8et1yP/7Qc9nc/nhvJjg0B+Do694fAXD1Zhtba//qTRzLY/fg7XpwsSPbetwcb4ZxPg3gXmPMCQDzAH4JwD++1Q+iKMb80goOnWB+ahiS4Y1W7nFbAADmL14AAFyc5xL48CEykpbl9rWMmqENvQwACCpckvXUAqOxzpluNKO2wmKUQ8NkmtXiEW3P4/Vj+ZflO9u4RrfG2gWennPPfBcAUD7K/R4+xaBdQS6FeoO/73U1w6rZ2PIKmU0/6t7qtOw17NiuxhjkcxnYxLWqcD0QyOAma2T2XbUW6TTVdC8k83RNz86cPgUAuPe+WQDAhpbq2YLmdzXVeuBhfn9ilq6cfo9LcRtwv3KZIyNfqWveF7XIJPstugLe3T3D8WfJLAO5GJKcfPBqARFkxaxk160+zv/wr251dvYUdmRbay2iOIVNNvssg8DFGlwsQefLcT63Rk9cszuuBOKQr221RilmxTDV+8IOmGY6OD42Hdn1hd4yUONiEJtb7rg2xDeW7m/s69zqy95O3OdHfnBaa2NjzD8H8NcAQgD/xVr7wo+6P4+9AW/Xgwtv2zuHN9WsTcu0bS/Vut0E5841MHsPGd2J08cAABdeOQ8AaLXpwyqrsX2jQ+f9D84+DwCoHLoXADBWJXOI1fZz7sKKBsTf1XJkIi44U8jxeKPDUwCA5gZ9Ky+/xO9rZTKQ6hBnqGiMM2prnp8vXmNQ6cQRfl5Sh/s45fH6XY47k+Pna6tkSu0WmaYJt3mC9gh2atcwNCiPZJBRtkE1IYMr5vkqlyFKGb7vdsnQ2012PLAl/m7pKr//jnzRXTXTG5skw585QnvMHCKDLY5we+dRlisSBQWZHFOKWtwPitygJzvZnqK9iW6DPJlKcZJBwLioNrb6B6xxPmsxLbu/nNfAzm1rrb0pA3PZK4PvlfXi3jvmF/UYLMqB5zGn6yCLzYjgmKfb/9bB3OyLzRg05dsS3U9dm2NstpvZsr/tBN+9yIeHh4fHDrGr7YH7fYsrlxNYcAaqjzEzoh+QWSYZ+kpGaqMAgHtPnwAAXFvi9y35Cr//AhlmHNB3NTJOJgorn1ie29VGuZ9KiQylUefMsnyNDCTt898vDCma3qfv9fkufa69UUZ3g0lG1UsFHndtnVHehas8XqwoY9TjcZstMqo4dow3v80ztD+RK2Yw++AU8l3lXSobYn6eWQ5nv8/zFqiNcq9ORmliXgeBmN/FZ2jny2r7HIvRjU+Rca6JcZbTRwAAk0P0UU4rfamk7Im8mGG/obSlPu3Qr5PxNC/R91xfWtN2tFtHPvbx+xg/CZSmVJikb9yMKN9Y0dpssM+WEjuEBRDBDtoCb/E03kgfks9y0L5X0fFE0Wzn8izJV6zkGcRqo92TM7mHzefTHccOmP3OzvcN3+bm97fHWxtV9/Dw8Pj/ErvKOK01iHtZrC9x5o/anPHzZc4EtWkyRJvnDDZ5ijN9PaUPsdlRfh243coKmUI1R5/UoSP0RUZYAgBspPy+tUpfWiEc1n44nuqQon05jmOpRWbzV5/hcVLLTI2TOX4eWs54y1fJKPtdjjtUo/uuovRWM3GlyuPtNIF3v2F4pIoPfewn0LrE8/7N//0tAECoaHe77iqxOE8XxQGGS/RylbP8fiwk8xgp8bwhI4bhEqTned6/+7lvAABe+y6Fet7/wR8DADx0/6z2x+1zG7yOzDL3v3KZK4Xuy8zWaC2SeXblg7taJ0N+7RWuhDJjHEfpGFciD3zgYQBAVoncUbL/fJw7hTU3KoRCVyE0KBALNr13UeuMCgWCQYWYy6NW/qeyJZpXaYfx+x7i93CxA+7P5cm6/ZvUZTPoPTa/DsbsXm9SAXZzJ6Z1P7zZBgN4xunh4eGxQ+wq4wxgkDdZRB35IKfpm5q/xsqfenceAGCDcwCARx+6DwDw5M/Kt5WjLzJq8/XcOflK18gciqogSVSzPldnaeZYlUzwUE0VIKN0suQ0b7RizjCvztGXeeHr9LX1G68CAMxRvm8vkfHMHCczKo4ojBvw/wlUelYSk+qLIWddQuABRbGUxUOPHcb5Dn3HG6oEGivRTrGY+HKDjG9G5+3UCL/PyBfmKntqqtjJFVmpk8hOhQLtVi6TY2wscX9nP/clAMDIonyfNVYCxV2tHPrySXbkAxWTaa9zJeKCrMkGx72+TEZUuk7GHKnyqPc2+r7DWY5TBVIHFlGvj/mLlxEqep7VCsDkeH0bOS/zWWk9qHIs21OlkCqsCqE4YazKOqvKvulZAMCaSpVbYrAZ3UeDLAbrtAaU9ykf6qDIfUu+p91SO/9DaZ/BFq5qN2sVpOb2hvWM08PDw2OH2FXGmSQpGmtNDI3zyb5Sp4+jUOGTv9lSdFoz08svXgQALMyTOVarZCJTU4x6Ts5yZmq/RmZw5ToZYrHKGWpsgsyjNiRGGMwBADI5MZpA+Xp9Rt3TyFUu0Od55mEyzftP8LVa4sxYm+D+220yon6f42iskDknfX5fzIlpJtuN5u1PhKHB8HAWy8uMnmcDnpdKyPO8lsqpbGmHnJxWx6rcrpgnk+lrGu9JJKIhBpgrkplaRWVLUtWaHKfdchkxyCvUDlhY4gokTsg4g0BhXPmoM8rXdCuPnsRcSso7XZVIS/saGe1wldtVjFY0qlDqH2yzotWP8NzlBUAqYo7pZR0zFGPLZLL6nCdELmZ0dTtNDvM+nJUmwHRBYj4l2r+j2nUj7YI1ia90+vzcqVyFYrauEskxxFDMttelHV1tu4v69yTG4vbjKsqKWsEEWuk4c8bboJOecXp4eHjsELvKOGEZGQsUhW52GMWcUp5eCDLAq1fpY6hbMoD6GmeMTIFMYqXF1+Eqo52FCmeOoTHWoBfz/LemajN67/K/nGyYU0shQ7Kqma2vscJoiBMj3v8B5nHmFaWfmWaUP6f9nXteOpvy6XXrZEpWjHl4nNsnen9QYUyAYi4Po/+zsUa7BmKcGfmMrKbyOOZ5iaQtUC7JhyafWUMycDkxgqpkyLKqCGq1mGUBVfyMjpC5dHtkHJLZRNSTXVqr2i/fl8pkLrUKx7Gk/M5CgSsEm9Kn2e1z3Fcuk8meuMLrbnKW11mS9rZ3gvYpTBDClEdu6Nfq857+UEEYkkE0moyulLooOs9fuU3maKVeNTJKu81UFXUfoR2WN2j3V5dop/MrfG9Cd/8q/1fMNi+9XpdP21cetSsEcp5MxzidXKBjzoUB43QVT6qt30a6qGecHh4eHjvErjLONE3RbDQQtlTTrHyvSBUEgWaUYp4zuVOIr9aYn5mEnNE6fc787WucSU4cfhAAMFyUWHOkGW+DM1mtLF9jltu3u5zJkOH+0pDjuHCeM1htijPj299BxlkEK5OihEyn25I+aESfZr9DhpKX+kuxzFc3UZrggOf7WQtEMZSOiazm4xEJFZdS2vFKnee9J6bY6CoKm5VOal55tRHtdOQomd3wGPN2lyVMHOn7WFdvJEbhortdRfeTDvfblg+zvsqsCBvLZzlR0/54HTRbZCRtCR1HyrboKsp+8RzzO8efpBZCJnvAK4eshe31YJ3eplMhwuYo9o3acVWOKQpfcL5R6XIubnBFlur9pXVVDsm3ua7zv9Hm923FBuqyT6Dryo0nE7hxRJu+N9bV0Lt/RJ0BJKBtB+pdEqrWODGosb/ladGxPDw8PDx2hF1lnMYAYT5AR71nmq8pP26ZM9HkIT7xy8rH3JAPtJohYxid4sx0/boYXSIfYo+fd5ucwfKGPq8gJFNdXRajKauCpMH9dZrylWW43ZV5+V6OMKpXqJChZJQP2OnIB9bj9kcO8/NhMdpFRffLFW0X8HuzVQbmgCGNY9RX1tBaYTZCTfmbrka/31NPpgzPf9vQ3mvK96sOuagsp/ohFTOPDPM8OoX9jXXZT1HXELT/xGh103i6iq66sHdfWQ7NptMSoN3zklNKlNe33OD1uKbfdyUV3o34/ur88pb/54CH1a1VJHqzwrtTHxowtEGvLtepgZ9XpSXh5FSXdX925dsO1vlFW3Zy+Z6proOyft+PXOUZrye3orGuxYb7nWOaTrXJLfTs5oqjdGtlkNmc8bkd0SvPOD08PDx2iN2NqsPC2BhWvq0J9fwJO/RhxA0pdisq3u+SASwvS+HbqatkySgnJulrmlQTr4kRtVNWbXNWFQhRSIZRVzR+7hrzQxfn6KNc5QviHitPqiPcbnGZtdDDhsynlGMn1clDrGg6dJhMx8T04TXOkCn1Yx4vUY+ktmqhgb+47Rnaj7DWIu1HiBS1Hq3wvGysk7Ffl67q+HH6FGtSzl+cY7R6qMvsh7zyAcdGyegrJUXlQ1KAIVUUXb0sDYLWZgbUdIxGPnO1HsJanduvN5wGgbI0Fskgc8onbcr3tqF8v56YSk810l354mL52JLIxZUPKAw7Uw70KgfdYjcr4JstYWxX4ZOoIicvH38zw/ujLiZfLioPVB0a8op5bHQUjZcPuSK1rEvKrmlr/1kxTXc842jg1mL2LS7ZG5s5hrnzrBfPOD08PDx2iF3O47RA1EVOzKIiH1hWUdZYvWGM9DRLBX6/sqR2rWrdc+YeVg4dHqNeZyajaGpLvjJwZjOakZryoZy9yAqkhXW+BvKdpOv83aglM7yvpnxD5Z/1pVgdRmQozqeTK/L7KemBjg9R0b7eoq+vJ99YOTO2zRO0P2FgkEEwqDXvK6pdb5B5dyzt954PUMXowQfIML/+aQqRL8/zvM+owmS4St9lX5UjPTHAVHmBvZ6YntSJVlbVBVN5lY5BtJr8fn1DFSiq/Al0/S2qi+nMiBJ3S7xuGsrj7EnRPlaeX1iST31AwA64jxMAYF6nh0nctEePY+JioF3ZJ3ZK/4Z52tk8z+PUEO/bovJ3j6sS7MQkV3hlOUe14MDXznOF8uVXuL9VaRCE2Mx843hzb6EBIx6oJW0Om2/t6+6j6h4eHh5vAXaVcYZhgKHhEgqKmlpVEJVVORAnZAxxTJ9mU7XKYVM+EPlI0FGYusMZymSYv5moIiWfVWWKGMoGCSBsnYrhxYh5gUXL/eRD9uleXH8GADCboa/0SEE6gQH301FPpI0+a+zTVfruTErmMlLmaxqQ2TSkQ5kr17Z1fvYrDALkbQnTEycBAM8mdBqvKS/30IM8nz/2fvqI7z9D3/RYiZff//mfXwAA1Nd5ftst+hxXl6V7KuZu1Ze90XMrCdqlJoabV5TV1SSvy+faFwPJSqPA6aauSbE+qxVJJ+R10wGvv77yEtvyWYdV2rVU5n6SbSuK709YaxEl0YBdbe0aOcBWRueaXOrpkgXP3+MjPH+PvuNxAMDkkPrT6wc5VQAdnVDUXSuHWN1SM6fZM6ze4ed//eq6DquVqhhuxlUCBU4v1I3PyWDx+ki0/4HPc1AB5RXgPTw8PO44dt3HGfYsEiMVJPm+2nrQt5uc6bOuD7qi2XnNRLmYvqhyeBwAEPbIcNIOZ6JiltFYSGncqGh5psrtp0feDQDoJPRhtVbpW7u4RB3OWoadUofVLfPYJPf/0iJVlwJD5phV7XVfFSZdzYCdyrd5+Jyih11F29cXtnd+9inSxKJdjxDkaZ+eFgaHjtMX/aF/xPN+6rTUjNQ98sH3kIG6CqCv/+FfAgC+++oFAIDpSffSSYKriHhVDHO0pqi7uld26rRrY4MMR23UEaoyrBfzgw2p8bR1Xb00zyyKy8v8vpG4Lpbqcqlw7NA4fXQV5e2u6no9sLCATdIBY7PBG/s2retnPtDB5PvQxQaqs/xe3Ux7La7UVjNcWVSVPfHKda4wnn6ZTLK1wg4MpWnGMgI5l6M277+KovVdpwwvH/sgRq7nS7Il7zSNlYerzzOD6Lzb7PaPRc84PTw8PHaI3WWcEZAuWaRFPun7Uk7PiTHksuoqqUoPK4aQipJMHnoMAJBNTgMArl8ltclKjy8uynehftwdKbAXipzRAv23wyOM6uaGxGCkr5kTk6h36RS91vkBAKAyLTWVhIyz16UvLExc/3bOVYur3wEA5LPMYxwdZV5oEFW2eYL2J6I4wtzKIp56/ikAwMRJMrOPf+IXAAD3POB80WT4PfUi6iuL4qF30Pf82nNk9n/7J18EAOT6ZCSRmH0q9Z3hAs/30Rn6pl3lR1N2d77L9Z6i6BpnNsvtGllulx2hva/MsQZ+URVl48fok706J11P1/PI8Dqtr0k9KT7g6khgnyHHLB1D29rL54fyOd37lL7KK22+vrxBpvfiCmv+h1Xxlap2fF217NEc86cza5cAAB/7FTLO6/PqATasysACf//Ua7xf1doIw8r7rErFLJ+j3ZzKUq/vYhY83obyyq/3tv849IzTw8PDY4fYVcZZyJXxwJF3IFGXwERKzDMjZCQF5fG5bnbXrzPfclXK8GHhFACg26Uvs6M+64WiegQp76/TYjS11SKzSRJX68r9DFU5UxWl4zl/Xd0P1WVxQRVGlRUpTKu/dlS/BAAoBdJzLM4CADI55Y/1+Hk5T+Z8ZJr5nVkc3t4J2qfI5nOYPnkEcYUM/7HHHwUAnHqUvaISS59jpETcvmvWI99SrsLL8NjDPF/Nz7CHUEYqV/UWmV1OUfXH7mfvn9kTfN1Q7XlriQxiUT6wa2352kL1986QKVamyTx+/MPMK732l38HALgakdF89Fd+BgDw1S9+EwDwra/QBz4vBhr1mK9rzMFWRwKA0NqBGlIu3Nzv3uXX3oiyu6i0q13neXeVVytdp3cpBX6plOm2RKXL/Myupa8z0nHiNcYIFq+c1fH5gyd/8kMAgHGtKCcrfJ4cHdP9rRVGQZoEGa1Mnc8zln7rxUX6VP/z1y8BABa6t68k8ozTw8PDY4fYVcZZKlbwyKPvRyCdxqBCX8WIlLdD6TGG4MzxwlnmVa5cZl7gxUUyyWyGzKVYUbQ9IpOwEWeWlnwlsRVTUVe+tvo5X7hEX1qlIHUc6fQ1VXt8vUGf18loFgCwOk8Gc/nSSzx+n8cdqXBch2bp09uIyVxT+c5Gs2Ku+c3qPQcNYTbEyMwo/tm/+DUAQK7I+TgKeL4DuHw5nuei6yGk/LtYFT+HjpOh3neGzHPueZ4/q/zeMCstAEVrv/sqmeDSOlcci9fJPK9v0I51McIg5PVQKdCOT/zkTwAA3vVzTwAAvvk9ahe0z9P3VlYXzo/8wnsBAOde+AyP9wx93u//CMc3PXvA83ONQS6bgZFK0bBUy9rKi3VZDFtLxB1yoVMxUk26GOSxIe7ngSmpl6ljwIbybiPlVy7Vac8vf+UrAICHHn8SAJCXlkVNKmRHp5jHPSHGOaIVbSCVpJLu80Dj6cvHuS5tg7NXuNJIos29j24Fzzg9PDw8dohdZZz5UhmnHnknbFaVFxk++TMhfR1hws9NkU/89g8488xfIQNc7fK1ql4x8SJ/X1Lt6+Qoo6FjQ2SAzbaL3nImiaSr2ZRqT1dqOIFkdJpdMg6nklNXzbJR/lrWMF/0xfNkrMPj0m/MkEFly9IZFQNeWeOMeWLq8e2eon2J1KZo9Rooj9J+qRS5BxUdmunjnovK3qjVAIC+ZvqRKZ7Hj/zDnwMA/PHiZwEA7XVXK83rYkU6p+OTsnNMxtlT9Duj7IiiOgZMTtBuTzzJvNF3/8w7OK4RjuPQCVaSpYoCnz9PBvqRv/cuAMDp08zCePY5+tjmLtHndvzUoe2cnn2LMAhQLpcQKly9qhI8p5+ZOCV1p8e5pULHVf4kup/efoQM87336nz3pEalp1CiLJp2g/as6D52lUaPv/s9/LzkdF7VxXSQgLlZpSmnFazrNTR3iV1uv/rM9wAAzyzwPn1JOq8byuJwPdFuBc84PTw8PHaI2zJOY8xRAP8VwDTYVOST1tr/ZIwZBfAnAGYBXALwcWvt2q32FYQhSsPDiKU641RmkJX6jaXPoSDfZaTo9rVXmNdl5ROdmGaPofNn6ZvoGKkhKfqaOaw8MjGahcuXAACtNplmWzXnoaLtxqoCpKDaV0X7ryySgdaUN3b0GHvg9FQa0+lzP/0eX6uj/F1XzKovpfI8Xr3VabkruJN2tTZFHPeRDogkz2tGDDAedElUzxdVZkSxuh+qAiRWfuXRR2YBAMVpZllsvDTPMUvV6OgTzOv7+x//IABg4RoZ4NIS7ddQ75pYFWqHZ5i1cUz5mX2tdNY6XMEcOU4GlFE/+AvneLzyL3Jcj7+d2Rzfee4VAEBHJUlJtPd6Sd1JuyZpgnq9Pvg/+64yyKmDbXl6uFpvdxmEyq89NcXz+ivv43270aLd1zZor5p8lvPqZ//IQ1wZPPGen+L3o/QlF2X/vCqCatJnLWggOfW7X1nmc+OFl7lC+No3vwUA+MbXvsHjquPD6I/9PACgHUsH2CiaLoZ8K2yHccYA/qW19gyAdwP4dWPMAwB+G8AXrLX3AviC3nvsH3i7Hkx4u+4Cbss4rbULABb0d8MY8xKAwwA+CuD92uxTAL4M4Ldut78gBKwStwbdCpXfl+bIONIGZxTTJCOIm4xe1ybINHrX+b61REYYK08sapJRruj7UJUDHXWh7HT4faPN/YaulCjk8Y+cUIXSDJmOXCmDPLVWpP7as8zjyyTMz2z3WeMeZOhD6SdkpOUKGWoa3e6s7D7urF0NDAxi+ZIyGZ53VyLcbkvdaFADzC8S1QxnC5zx+5rGiyNS/j5EZrDYov2Glec7eZIMZHiWvu3CIWoRnDJ8jTrOZ63rSddbELgVBo/vupKOTzDvtioGk1OHgVJVPrZ3MYpe+wyju86exfwuN1DYBu6kXa216CfJQI8zI9+f07l1zSFj8a+cq2lX98gp9Yr6B+9ivu0RZSu0FS2fGqFPu6b7dLzMqPmZ06wkGxrmSqCvirC88nEDMc7VJa40XlOWzN898xwA4Onn6MM8L82Dhp4LiXzktSc+BgDouJiKfKtZ+eIH8k63wI58nMaYWQBvA/BtAFMykjPW5E1+8wljzDPGmGfW1265MvC4S3jTdl1p7NZQPXaAN2vXuO3tejNse8o0xlQA/BmA37TW1s12ZJIBWGs/CeCTAHDmgQdtp99FX2pC3T7z6xIpr8fKg4yh6NqG8sTyygMrc7jr0mlcXhDDs2SMcUIfaUW16HFXzKev3j8d+j66yRL/J+V3ZlRhMH6Evzt1H5nt4gqZa04C4SZQH/UWxzlde5hfBKpZV1fMsy9zgphRNLecL23jTN0d3Am7nn7kuO30LcLQ+b5UYSKfV1vRz456SAXB5qh6OXTK6oG+V5R9hswyDqXPmCVDHJXPKxKT7Lu+2qodN3qPQZdEdRt13Q4HlTBkQJUhMs7auCrZDtOeiXyeY8e4/bGT3M7KOZ/Z5rm6G7gTdi1O32MZJ+B5Ntb5Evk6XOL5c+pRsSqJQnVWOFKhPU/Ljh1ltRjl5ZYLPL/HT3ClENzDFVxenSESPR8ay1zpPXv+PADghRe4wvvO98gsX70gZtkQsxx0DFDFmIL/hTHej9UJHse67eTTtHD5m7f3XW+LcRpjsqARPm2t/XN9fM0YM6PvZwAsbWdfHnsH3q4HE96ubz22E1U3AP4IwEvW2t9/3VefBfCrAH5Pr7dt4WgBJKkZ9Pgo5OjjiJxajnQrVyNG20pj9HG974Os9LjaJpO7ssqo58RJzkypmEoSSfFbitPlITKHpSvcb7dPxnnvY/SdQLqQKxv0eY5MSkhSjdA7Tc6koxOcGWMFIcen6PuamHAMiVHbdfVdn1B+YF6170tXXZfLvYM7alcLdCMgkFMz0oohknK7682Tc33M5QNLdSF0xUi7UsWKdFVWh6VCJR3ObIH2yWd5vnuqRY8D+TJ7tH9GlR+ueaGrXIkjMot2h9v1pDmwusrrr6OVSUkdCpal8B+LQZXl82y11B++vfec13fSrsYY5MOsS5/FfYe4uj85w0qd48rbXZcu6YZec8qWqEa8X/qq/e4pb7NaVddYrcRU4IOylPXX1vhM/9KXvgYAeOop6ty+9DJ9mcsr2q9WGAO9TZdXqhWF02ENczxOdkwaA3rv8rdN6LI9XJ7x7aPq21mq/ziAfwLgeWPMd/XZ72UyuIAAAA8ESURBVIAG+FNjzD8FcBnAL25jXx57B96uBxPerruA7UTVv44b4shb8dM7OZhNLfr9FEaHNS7xT81JsupqWVC0rdLia+MCo+ePP8iZ7uSDmgID+iz6He7n6a9yu+VlMsaiVJDaHTLQYeVZPvJO+lQuLjHPC1X+e4eOsVa6VqOvs1ImY+3E9G02FB1O1atobpm1y6MjjgGRkQwX5YOTL7fX3Xu6jXfSrkkKtPoxYvkSM1n1Bmpw5VAVk5gYk48wu1nH0fm+nD5ioraGruIkkPrUuqKjr10k46jN0L5hkfa1Ul1KlT/a6HJ/3b6L6vN4rpIk1jgua0WyIR9ZoPHXm9xvYKUw3+X2r5znimejvvcY5520a7WYx/seuRcjJf7fJyfUgUG+w+GMsmOURdEp876IpUrWa+v+dj5trTxKOfVFV0Vec5n52M2rPP9f+DZ1bf/7//o8AGB5iStFRyxTeRhTp0WgKLtTnjfyhefEaJ1WRWZSKmXSOnBLkhRuZeQqn7w6koeHh8cdx64molkLJP0EiXq+ZDKq8JEyeHWIvqWkQ6Yyf5lqRK/8gNG0auF+AEB3lFG2jhjOWJG+iyDlfidq9wEA8kX6JnuqfBgep880Ut5Wo0H9v8NHyGSN8km/8kX6VLIl/m7ymLpVKu9v8SpnwH6iGvommelogTPacIUzcyz9yDjdexUmdxJpmqDRbCKXJTPLq8Ijl3MqNVphuL7r0g5ot6WGEw2cka9/QWSdDivP4/o6mebn/+pvAQBDYx8GAMzeo6i8oulx4nyZZBINMUcX9c3mXBdFvi5cox378r1mlJ/p3ruOAs6OVy+TIa2sNLdzevYtauU8Pv7OE8jlaZHXFnjdP/UV+h4fVEzAyO59McpXz3Ildupe3oeBovLr8/RRttakZrVAX+Yrr/LzK8vK2y5x5Td6mNktNnRRdu4nFt3ruTxwpU0Vs2SMgRhjV1oVSYErwmKNPlq3MonFOK3UuxzjdLq9t4JnnB4eHh47xK4yTmMsstkIkXTwMoqWdhMyv6vXvg8AePmZ5wEAVeX3lSP6JF76Mn3d+VmnKE3mUjpJJjl7hD6NuWuKtmmGyqjnyNQx17uGTCFt8/OS+qBfPMta5Ke+zfzQIw+o73NVPpmYPrq4zt+NTvD7Sxc5Y768wfzOD0rvcfoIZ+RWvLLdU7QvERiDYj6HQsH1jlKPphp9vvmMfIQd2mtD+pkdRbcrYug2ddHqtnbMl/Iw7fq2d74dAHDpCu30h3/w3wAA73svVYzuf4RdNYenaE9rpZYUqkJEzCLWdXFdtdLnX7206XiJdX23eZ11+mQ2ReUlZhu0e0sVSgcV1hp0bAarqi1/WWpC3/gBtSPmtCIbUyeF4eyWDgvKQphb4P39ymu8D579Lit8Xpkjc29IGR4Z2u2n3sZa9Q+fYcWRFhwoaAUzv0SmOrfE/dabXLGee4FM9+yz7H3l8jhzM6z8Sh1zbfM+hfORijHfYJzex+nh4eFxx7GrjDOxfaxFV9DvcYZQayBcWyfDvLrGWuBl9QCZzlJNZUwzQ12+z+wiGUquwxluLjkHADj9U4yWr6Tcbu0q/72JGc4gj7xTTEhR3uVl+kavq+dQucKZ8swZ1pgPHeEAbaJorxIMF+fpO2mtyhfW44y8LnWX+TP0qZSr9KksLH9/m2dof8IAyCJBkKiraEgG4ip0XN9t180wn1dNuFYCRfmiGw2uBBJVgBXUbzuWj+zkadr3voeZTfH5P+H18pn/QdWbD7bISB//aW6XqsLF5WEa5ftaVRAtLZEBNZq039Hjx/SezGpR0dyMq5QZ42uQpV2brYPdV70ZxfjW1bVBVsjCNZ6XkgrhVuVbvLhIBnioyhXiL3yMK64HHmbvqZwU/8dmuCKYvJ9dan9SzH9ylMx0pKjzXOQB8gXav6zXrKLzTfUKWm3zeltYp/2+OsH7rqP84KsrtK9V6VB7VUrvCp4XSxyvDfh8cYzTbpWyfwN4xunh4eGxQ+wq44zTCGvNBbTqjIonHc7Y6036CFPl3Q0rb6y9wWh6eVS+CPnCsgXOFEMRZ6pgijNUbYIz09AwZ47LZ8k8jUofVq9xnujF9I1MTZNZXpknw1lZ5nhsljPZpNK98qqVdzNST3qbC+eYd1aWov19jzEK2BTzXF6Tcnz+9j6T/QxrU8T9LmIpgyutD6WS+t7Lh+TUqFz03c3sjtGk6rMeJMoHVD91V4G0ukYG8eR7qZ7zxHuoDP6tr7B2+eJr9E1PX6EvK69OAcNOZUdR2Hqddm7I137vAycBACMjjOYO1fgPrG/UNW6+P3Yvsya6yk9s9w8240ySBGura1AyAoyi0Tn1l+8rNjA9SjseOfUYAOCeR98JAKgqH9tpEwxVeP9MjZ3WfrjfQBU7Tj/XKA01ccxPte39WMryWoGWlB0xNczr6onHeT3kK4x5fO6LXwAAXL7K3lRJKk0M3a+BNBAyUE+iLczzVvCM08PDw2OH2FXGmSYROo1FmJC+o2yVvonhkpjcBTLH6gRntmicvkeTJWM4NPoQAGBunox14xUyuwcOMwpXqXCGOnqEzGLlKn9/4UXXlY8zSlgi08gVOQNNHeL+F+fIRHupmIR1MyBnuqERzrAnpAd5XV0RY0X966ucGRcXyFR6CRnvmPJHDyqS1KLVjhBJXzOKXTdB2rVUdD1qXL4mP3e1xImYZtTh79tNUpxr82SYU/Jd1YZ5HttioMcfZv7tWpevru+6CowQSRE8V1SUXN0ZM6oomTrMFcfsPephI5+bXKHoqwJpQ0r+ZUWPiwXtp5Td1vnZr8iGAWaGy4hkt8jw/OfLfL2sgrjcMO3zE+9lL6dR+TojMcRUeZVNBc+dnaq5zcfL6Lpw3ShD10zIGcTlXaZbfJF6GRkiwz19kiu/F8+yAnB+nozT5W26FYTzdQ/yh+WLv72H0zNODw8Pjx1jdyuH4i46qy8jzHOq6kkWJVclY5t5kBU4rpIkzqsmdYO+zfoSmWJzna+dBTLG559mVH1syEU9OeO9+/1kFrMnGIUdneBxhybJMIpj8nUE9G0tz3OmWlqlbzXNX+bAIzGLVHmK0iE0UoivVjSzqitms+kqHPhakKrPQUWSpFjf6LzuvfRUVatvVHPekw/bMU0XNXUVRs22upGKGVZHySCefB+ZzLFZMohA+YLVUUbjH3snVxwlqd4MDfF66UHHk2/ViOnkxTgctei6LqiqYS8Uaa+q8hFdt8Qw57Ioeps+P6jIZ0LcMz6ERCpC6xle520x/3trXHmdfAej54cPMyuhr/MYOqV4t0P94VSxbuTZimGKx5kbbSu13ZaSMrj9uPvODsYLAEPKxjh1jONxep1zq1yKWNWqB1JBcz7NQMe1qY+qe3h4eNxx7CrjzAYG08UM2k7RHXzyWzGBXI0Mob8mVSNJra69RF9Xrqloek8VPKpQ6VnOiGlCxrF2jQyioSjqPSekXiQ9xlX1aQ+aPEBBFSEnTnDmnDpMxrHWJaO4fp1MMu1zvKHCgY8+Mcv3CWuoU4gJx/w/jP4/15f94CJAihyyqlFHwNdmS/mvqvVuSa8xlL1r6i0UZgbpCwCAgnyH02J45XHmdxZVwZVIVSuTcvtMjduX82SgWSnQR6pVDxKnx0kGXFff7p7G5ZhoRsdTkBd5qXVl1PW0JXWsQNHkZqO7rbOzX5EJAoxXi4j6PC/NNu+f0kNcARwdJ7M/fY98zOJhQVZqZyKOWRF8l23houYZ1bYPXJkuyh44ndvNDNDVlDu5TKdlYLVdqOyZcpH2eeRhZl/0RFX/79efAQAsbXR1XP3O+VCxOXvmVvCM08PDw2OH2FXGmbEhxuMaeuoiuTS3rlfqXcYl9UXvKz9znjNMYVVTjGZ6xNIFPEWGOXZSis/6HdRfe/EC95uskQlOntB+pRBe7NFntrpBJpRN6NMcm6JPdHqUvrOkS/3FK/PcX7Hi8kY5nrhLxpRxU+yy8hM3FI3s3l5tZT/DWot+ZAddLjuKjrdUGpZ3eZyZsl71O+Xj9aRC1EukAK/8SMcw8vJdx4ZMwSmKJ8qn7bWU5xeqW6GY7/IqVxSjNfrkUvnKlqXy01UN+vgMfdyJmMZq3TUVFCPSgBeuamUhBpSkBzs/FzaFjXvoyqdb1ArvwVP0HR6q8bovqrdTEDoGt9knGbjsFPde59nocyd/mQabfZpxohWGi+qr8qylTgGui2lH10Ginkgdp2qlPM2ZI6wkG6tdAgCs1K9sGqfreup6Ut1czvQGPOP08PDw2CHMduoy79jBjLkOoAVgedcOunOM460b33Fr7cRbtO+7Bm9Xb9e7iLti1119cAKAMeYZa+3ju3rQHWCvj2+vYq+ft70+vr2KvX7e7tb4/FLdw8PDY4fwD04PDw+PHeJuPDg/eReOuRPs9fHtVez187bXx7dXsdfP210Z3677OD08PDz2O/xS3cPDw2OH8A9ODw8Pjx1i1x6cxpgPGWPOGmPOG2N+e7eOe4vxHDXGfMkY85Ix5gVjzG/o81FjzN8YY17Ra+1uj3Uvw9v14MLb9hZj2Q0fpzEmBHAOwAcAzAF4GsAvW2tffMsPfvMxzQCYsdY+Z4ypAngWwMcA/BqAVWvt7+liqVlrf+tujXMvw9v14MLb9tbYLcb5LgDnrbUXrLV9AH8M4KO7dOw3hLV2wVr7nP5uAHgJwGGN61Pa7FOgYTzeGN6uBxfetrfAbj04DwO48rr3c/psT8AYMwvgbQC+DWDKWrsA0FAAJu/eyPY8vF0PLrxtb4HdenC+kdzInsiDMsZUAPwZgN+01tbv9nj2GbxdDy68bW+B3XpwzgE4+rr3RwBc3aVj3xTGmCxogE9ba/9cH1+TL8X5VJbu1vj2AbxdDy68bW+B3XpwPg3gXmPMCWNMDsAvAfjsLh37DWEo8/xHAF6y1v7+6776LIBf1d+/CuAvdnts+wjergcX3ra3GstuVQ4ZYz4M4D8CCAH8F2vtv9uVA998PO8B8DUAzwPq/wv8Dugz+VMAxwBcBvCL1trVuzLIfQBv14MLb9tbjMWXXHp4eHjsDL5yyMPDw2OH8A9ODw8Pjx3CPzg9PDw8dgj/4PTw8PDYIfyD08PDw2OH8A9ODw8Pjx3CPzg9PDw8doj/B7hN1y9HP0FuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(330+1+i)\n",
    "    plt.imshow(x_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageDataGenerator is a class in Tensorflow Keras which helps in \n",
    "1. Generating mini-batches for training <br>\n",
    "2. Augmenting  and preprocessing images before they are loaded to a network for training <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Training data into training and validation according to the section 4.2 K. He et.al 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape  X=(45000, 32, 32, 3), y=(45000, 10)\n",
      "Validation Data shape  X=(5000, 32, 32, 3), y=(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "validation_size = 5000\n",
    "validation_indices = np.random.choice(x_train.shape[0],validation_size,replace = False)\n",
    "training_indices = np.array([i for i in range(x_train.shape[0]) if i not in validation_indices])\n",
    "x_validation = x_train[validation_indices]\n",
    "y_validation = y_train[validation_indices]\n",
    "y_validation = to_categorical(y_validation, 10)\n",
    "x_train_1 = x_train[training_indices]\n",
    "y_train_1 = y_train[training_indices]\n",
    "y_train_1 = to_categorical(y_train_1, 10)\n",
    "print('Training Data shape  X=%s, y=%s'% (x_train_1.shape, y_train_1.shape))\n",
    "print('Validation Data shape  X=%s, y=%s'% (x_validation.shape, y_validation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is performed on the training data, according to the section 4.2 K. He et.al 2015, but not on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_size = 45e3\n",
    "validation_size = 5e3\n",
    "learning_rate = 0.1\n",
    "iterations = 64e3\n",
    "epochs = int(np.floor(iterations/(train_size/128)))\n",
    "steps_per_epoch = int(train_size/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = ImageDataGenerator( \n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True).flow(x_train_1,y_train_1,batch_size=batch_size,shuffle=True)\n",
    "validation_data_generator = ImageDataGenerator().flow(x_validation, y_validation, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to the network are of the size 32 x 32 <br>\n",
    "\n",
    "According Ke. He et. al in Section 4.2 there are total of 6n + 2 weighted layers <br>\n",
    "- First layer is a 3 x 3 convolution with stride of 1\n",
    "- Stack of 6n layers with 3 x 3 convolution layers of feature map sizes 32,16,8 with 2n layers of each feature map size.<br>\n",
    "The number of filters in each 2n layer stack is 16,32,64\n",
    "- Last layer is a fully connected layer for classification\n",
    "\n",
    "***\n",
    "For ResNet-20 the value of n = 3 <br>\n",
    "- There are 3 stacks of 6 (2*3) layers with each stack containing different feature map(32,16,8) size and number of filters\n",
    "- There are two type of shortcut connections <br>\n",
    "\n",
    "    1.Skip connections within each stack where there is no change in dimensions <br>\n",
    "     In this case Identity mapping is used. <br><br>\n",
    "    2.Skip connections between stacks with change in dimensions <br>\n",
    "    Projections shortcut is used to match dimensions using 1 x 1 convolutions\n",
    "***\n",
    "A residual building block function _residual_block_ is defined which contain convolution and batch normalization. This function can be called according to the ResNet size that we intend to analyze. <br>\n",
    "Below is the function definition of the residual building block function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, number_of_filters=16, filter_size=3,strides=1,\n",
    "                   activation='relu',batch_normalization=True):\n",
    "    x = inputs    \n",
    "    x = layers.Conv2D(number_of_filters, filter_size, strides, padding='same',kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    if batch_normalization:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if activation is not None:        \n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet20():  \n",
    "    n = 3 # number of stacks\n",
    "    layers_per_stack = 6\n",
    "    num_res_blocks  = 3\n",
    "    number_of_filters = 16\n",
    "    \n",
    "    filter_size = 3\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = residual_block(inputs)\n",
    "    \n",
    "    for stack in range(n):\n",
    "    \n",
    "        for layer in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and layer==0:\n",
    "                strides = 2\n",
    "            y = residual_block(x,number_of_filters,filter_size,strides)\n",
    "            y = residual_block(y,number_of_filters,filter_size,activation=None)\n",
    "         \n",
    "            if  stack > 0 and layer==0:\n",
    "            # for reducing the dimension of the output of the skip-connection a 1x1 convolution is used \n",
    "            # with strides=2\n",
    "                x = layers.Conv2D(filters=number_of_filters,kernel_size=1,strides=strides,activation=None,kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "            x = layers.add([x,y])\n",
    "        \n",
    "            # It must be noted that activation is applied to the second layer after the skip \n",
    "            # -connection is added\n",
    "            x = layers.Activation('relu')(x)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        number_of_filters*=2\n",
    "\n",
    "    x = layers.AveragePooling2D(pool_size=8)(x)\n",
    "    y = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(10,activation='softmax',kernel_initializer='he_normal')(y)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = resnet20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 32)   4640        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   544         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32)   0           conv2d_9[0][0]                   \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     18496       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     2112        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 64)     0           conv2d_16[0][0]                  \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a callback functions which are used scheduling the learning rate, storing metrics and model weights during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Callback functions for storing metrics and model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_callback():\n",
    "    logdir = \"logs\\\\scalars\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "def model_check_point(experiment_name):\n",
    "    checkpoint_path = experiment_name + \"_model_checkpoints\\\\weights.hdf5\"\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                              save_best_only=True,\n",
    "                                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learning_rate_schedule(epoch):\n",
    "    learning_rate = 0.1\n",
    "    learning_rate_scale_factor = 0.1\n",
    "    if epoch > np.floor(3.2e4/(train_size/batch_size)):\n",
    "        learning_rate*=learning_rate_scale_factor\n",
    "    if epoch > np.floor(4.8e4/(train_size/batch_size)):\n",
    "        learning_rate*=learning_rate_scale_factor\n",
    "        \n",
    "    return learning_rate\n",
    "\n",
    "def lr_scheduler_callback():\n",
    "    return LearningRateScheduler(learning_rate_schedule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Training using different optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_schedule(0),decay=decay,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [lr_scheduler,tensorboard_callback(),model_check_point('sgd')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = resnet20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd.compile(loss='categorical_crossentropy', optimizer=sgd_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/182\n",
      "WARNING:tensorflow:From C:\\Users\\tayyabm\\AppData\\Local\\Continuum\\anaconda3\\envs\\test1\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9891 - accuracy: 0.3412\n",
      "Epoch 00001: val_loss improved from inf to 2.14309, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 81s 231ms/step - loss: 1.9883 - accuracy: 0.3414 - val_loss: 2.1431 - val_accuracy: 0.3488\n",
      "Epoch 2/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5666 - accuracy: 0.4815\n",
      "Epoch 00002: val_loss did not improve from 2.14309\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5665 - accuracy: 0.4816 - val_loss: 2.3845 - val_accuracy: 0.3352\n",
      "Epoch 3/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.3526 - accuracy: 0.5647\n",
      "Epoch 00003: val_loss improved from 2.14309 to 1.88254, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.3522 - accuracy: 0.5648 - val_loss: 1.8825 - val_accuracy: 0.4606\n",
      "Epoch 4/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.1841 - accuracy: 0.6324\n",
      "Epoch 00004: val_loss improved from 1.88254 to 1.40615, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.1841 - accuracy: 0.6324 - val_loss: 1.4062 - val_accuracy: 0.5776\n",
      "Epoch 5/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.0608 - accuracy: 0.6788\n",
      "Epoch 00005: val_loss did not improve from 1.40615\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 1.0606 - accuracy: 0.6787 - val_loss: 1.4520 - val_accuracy: 0.5888\n",
      "Epoch 6/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.9746 - accuracy: 0.7111\n",
      "Epoch 00006: val_loss improved from 1.40615 to 1.19738, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.9745 - accuracy: 0.7112 - val_loss: 1.1974 - val_accuracy: 0.6684\n",
      "Epoch 7/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.7393\n",
      "Epoch 00007: val_loss improved from 1.19738 to 1.14349, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.9054 - accuracy: 0.7393 - val_loss: 1.1435 - val_accuracy: 0.6616\n",
      "Epoch 8/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.8547 - accuracy: 0.7582\n",
      "Epoch 00008: val_loss improved from 1.14349 to 0.97987, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.8547 - accuracy: 0.7582 - val_loss: 0.9799 - val_accuracy: 0.7220\n",
      "Epoch 9/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.8062 - accuracy: 0.7713\n",
      "Epoch 00009: val_loss did not improve from 0.97987\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.8061 - accuracy: 0.7713 - val_loss: 0.9821 - val_accuracy: 0.7130\n",
      "Epoch 10/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.7749 - accuracy: 0.7878\n",
      "Epoch 00010: val_loss did not improve from 0.97987\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.7750 - accuracy: 0.7879 - val_loss: 1.2757 - val_accuracy: 0.6514\n",
      "Epoch 11/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.7333 - accuracy: 0.8013\n",
      "Epoch 00011: val_loss did not improve from 0.97987\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.7336 - accuracy: 0.8012 - val_loss: 1.0113 - val_accuracy: 0.7184\n",
      "Epoch 12/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.7105 - accuracy: 0.8090\n",
      "Epoch 00012: val_loss improved from 0.97987 to 0.84319, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 0.7109 - accuracy: 0.8089 - val_loss: 0.8432 - val_accuracy: 0.7632\n",
      "Epoch 13/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.8171\n",
      "Epoch 00013: val_loss did not improve from 0.84319\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.6898 - accuracy: 0.8172 - val_loss: 1.1127 - val_accuracy: 0.7080\n",
      "Epoch 14/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.6594 - accuracy: 0.8271\n",
      "Epoch 00014: val_loss did not improve from 0.84319\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.6595 - accuracy: 0.8271 - val_loss: 0.8604 - val_accuracy: 0.7708\n",
      "Epoch 15/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.6375 - accuracy: 0.8335\n",
      "Epoch 00015: val_loss improved from 0.84319 to 0.74430, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 0.6375 - accuracy: 0.8335 - val_loss: 0.7443 - val_accuracy: 0.8036\n",
      "Epoch 16/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.6249 - accuracy: 0.8399\n",
      "Epoch 00016: val_loss improved from 0.74430 to 0.73370, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.6248 - accuracy: 0.8400 - val_loss: 0.7337 - val_accuracy: 0.8060\n",
      "Epoch 17/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.6122 - accuracy: 0.8454\n",
      "Epoch 00017: val_loss did not improve from 0.73370\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.6118 - accuracy: 0.8455 - val_loss: 0.8076 - val_accuracy: 0.7870\n",
      "Epoch 18/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5894 - accuracy: 0.8531\n",
      "Epoch 00018: val_loss did not improve from 0.73370\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.5895 - accuracy: 0.8531 - val_loss: 0.8706 - val_accuracy: 0.7808\n",
      "Epoch 19/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5780 - accuracy: 0.8534\n",
      "Epoch 00019: val_loss did not improve from 0.73370\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.5780 - accuracy: 0.8534 - val_loss: 0.7363 - val_accuracy: 0.8012\n",
      "Epoch 20/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5720 - accuracy: 0.8580\n",
      "Epoch 00020: val_loss did not improve from 0.73370\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.5721 - accuracy: 0.8579 - val_loss: 0.7583 - val_accuracy: 0.8010\n",
      "Epoch 21/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5633 - accuracy: 0.8628\n",
      "Epoch 00021: val_loss did not improve from 0.73370\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.5632 - accuracy: 0.8628 - val_loss: 0.8326 - val_accuracy: 0.8038\n",
      "Epoch 22/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5436 - accuracy: 0.8674\n",
      "Epoch 00022: val_loss improved from 0.73370 to 0.68474, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 0.5436 - accuracy: 0.8674 - val_loss: 0.6847 - val_accuracy: 0.8170\n",
      "Epoch 23/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5340 - accuracy: 0.8712\n",
      "Epoch 00023: val_loss did not improve from 0.68474\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.5338 - accuracy: 0.8712 - val_loss: 1.0266 - val_accuracy: 0.7366\n",
      "Epoch 24/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5302 - accuracy: 0.8719\n",
      "Epoch 00024: val_loss did not improve from 0.68474\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.5305 - accuracy: 0.8718 - val_loss: 0.9921 - val_accuracy: 0.7564\n",
      "Epoch 25/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.8743\n",
      "Epoch 00025: val_loss did not improve from 0.68474\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.5243 - accuracy: 0.8743 - val_loss: 0.7078 - val_accuracy: 0.8238\n",
      "Epoch 26/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5107 - accuracy: 0.8785\n",
      "Epoch 00026: val_loss improved from 0.68474 to 0.67751, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.5106 - accuracy: 0.8785 - val_loss: 0.6775 - val_accuracy: 0.8286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.5010 - accuracy: 0.8832\n",
      "Epoch 00027: val_loss did not improve from 0.67751\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.5013 - accuracy: 0.8831 - val_loss: 0.7010 - val_accuracy: 0.8188\n",
      "Epoch 28/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.8842\n",
      "Epoch 00028: val_loss did not improve from 0.67751\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.4918 - accuracy: 0.8841 - val_loss: 1.0066 - val_accuracy: 0.7390\n",
      "Epoch 29/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4902 - accuracy: 0.8856\n",
      "Epoch 00029: val_loss did not improve from 0.67751\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.4901 - accuracy: 0.8857 - val_loss: 0.7726 - val_accuracy: 0.8062\n",
      "Epoch 30/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.8887\n",
      "Epoch 00030: val_loss did not improve from 0.67751\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.4779 - accuracy: 0.8888 - val_loss: 0.8690 - val_accuracy: 0.7908\n",
      "Epoch 31/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4743 - accuracy: 0.8903\n",
      "Epoch 00031: val_loss did not improve from 0.67751\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.4742 - accuracy: 0.8903 - val_loss: 0.8354 - val_accuracy: 0.7948\n",
      "Epoch 32/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.8919\n",
      "Epoch 00032: val_loss did not improve from 0.67751\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.4674 - accuracy: 0.8918 - val_loss: 1.0287 - val_accuracy: 0.7490\n",
      "Epoch 33/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4617 - accuracy: 0.8951\n",
      "Epoch 00033: val_loss improved from 0.67751 to 0.62383, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.4615 - accuracy: 0.8951 - val_loss: 0.6238 - val_accuracy: 0.8442\n",
      "Epoch 34/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4621 - accuracy: 0.8949\n",
      "Epoch 00034: val_loss did not improve from 0.62383\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.4620 - accuracy: 0.8949 - val_loss: 0.7816 - val_accuracy: 0.8176\n",
      "Epoch 35/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4451 - accuracy: 0.9000\n",
      "Epoch 00035: val_loss did not improve from 0.62383\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.4450 - accuracy: 0.9001 - val_loss: 0.9191 - val_accuracy: 0.7816\n",
      "Epoch 36/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.8986\n",
      "Epoch 00036: val_loss did not improve from 0.62383\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.4515 - accuracy: 0.8987 - val_loss: 0.7827 - val_accuracy: 0.8024\n",
      "Epoch 37/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.9013\n",
      "Epoch 00037: val_loss did not improve from 0.62383\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.4448 - accuracy: 0.9013 - val_loss: 0.7010 - val_accuracy: 0.8354\n",
      "Epoch 38/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.9037\n",
      "Epoch 00038: val_loss improved from 0.62383 to 0.61231, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 0.4349 - accuracy: 0.9037 - val_loss: 0.6123 - val_accuracy: 0.8554\n",
      "Epoch 39/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.9058\n",
      "Epoch 00039: val_loss did not improve from 0.61231\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.4293 - accuracy: 0.9059 - val_loss: 0.7323 - val_accuracy: 0.8062\n",
      "Epoch 40/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4311 - accuracy: 0.9051\n",
      "Epoch 00040: val_loss did not improve from 0.61231\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.4312 - accuracy: 0.9051 - val_loss: 0.7191 - val_accuracy: 0.8310\n",
      "Epoch 41/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.9082\n",
      "Epoch 00041: val_loss did not improve from 0.61231\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.4216 - accuracy: 0.9081 - val_loss: 0.7632 - val_accuracy: 0.8186\n",
      "Epoch 42/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.9102\n",
      "Epoch 00042: val_loss improved from 0.61231 to 0.56025, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.4185 - accuracy: 0.9103 - val_loss: 0.5602 - val_accuracy: 0.8662\n",
      "Epoch 43/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4122 - accuracy: 0.9127\n",
      "Epoch 00043: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.4122 - accuracy: 0.9127 - val_loss: 0.6997 - val_accuracy: 0.8340\n",
      "Epoch 44/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4130 - accuracy: 0.9131\n",
      "Epoch 00044: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.4130 - accuracy: 0.9130 - val_loss: 0.7436 - val_accuracy: 0.8216\n",
      "Epoch 45/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4045 - accuracy: 0.9137\n",
      "Epoch 00045: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.4046 - accuracy: 0.9137 - val_loss: 0.7785 - val_accuracy: 0.8210\n",
      "Epoch 46/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.4030 - accuracy: 0.9135\n",
      "Epoch 00046: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.4031 - accuracy: 0.9135 - val_loss: 0.6586 - val_accuracy: 0.8408\n",
      "Epoch 47/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.9166\n",
      "Epoch 00047: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3998 - accuracy: 0.9166 - val_loss: 0.7457 - val_accuracy: 0.8318\n",
      "Epoch 48/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.9188\n",
      "Epoch 00048: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3909 - accuracy: 0.9187 - val_loss: 0.6488 - val_accuracy: 0.8512\n",
      "Epoch 49/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.9211\n",
      "Epoch 00049: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3886 - accuracy: 0.9210 - val_loss: 0.6848 - val_accuracy: 0.8452\n",
      "Epoch 50/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3840 - accuracy: 0.9186\n",
      "Epoch 00050: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3840 - accuracy: 0.9186 - val_loss: 0.6463 - val_accuracy: 0.8454\n",
      "Epoch 51/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.9231\n",
      "Epoch 00051: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3793 - accuracy: 0.9231 - val_loss: 0.6334 - val_accuracy: 0.8460\n",
      "Epoch 52/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3819 - accuracy: 0.9206\n",
      "Epoch 00052: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3819 - accuracy: 0.9207 - val_loss: 0.7316 - val_accuracy: 0.8296\n",
      "Epoch 53/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.9228\n",
      "Epoch 00053: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3738 - accuracy: 0.9228 - val_loss: 0.5629 - val_accuracy: 0.8724\n",
      "Epoch 54/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3715 - accuracy: 0.9252\n",
      "Epoch 00054: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3713 - accuracy: 0.9253 - val_loss: 0.6144 - val_accuracy: 0.8518\n",
      "Epoch 55/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.9256\n",
      "Epoch 00055: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3657 - accuracy: 0.9256 - val_loss: 0.8151 - val_accuracy: 0.8078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.9274\n",
      "Epoch 00056: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3654 - accuracy: 0.9275 - val_loss: 0.7242 - val_accuracy: 0.8296\n",
      "Epoch 57/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3662 - accuracy: 0.9263\n",
      "Epoch 00057: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.3662 - accuracy: 0.9262 - val_loss: 0.7312 - val_accuracy: 0.8322\n",
      "Epoch 58/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.9300\n",
      "Epoch 00058: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.3553 - accuracy: 0.9300 - val_loss: 0.7579 - val_accuracy: 0.8332\n",
      "Epoch 59/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.9295\n",
      "Epoch 00059: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3549 - accuracy: 0.9294 - val_loss: 0.6423 - val_accuracy: 0.8530\n",
      "Epoch 60/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy: 0.9314\n",
      "Epoch 00060: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3528 - accuracy: 0.9314 - val_loss: 0.6248 - val_accuracy: 0.8502\n",
      "Epoch 61/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3562 - accuracy: 0.9281\n",
      "Epoch 00061: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3562 - accuracy: 0.9282 - val_loss: 0.6624 - val_accuracy: 0.8496\n",
      "Epoch 62/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.9312\n",
      "Epoch 00062: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3480 - accuracy: 0.9313 - val_loss: 0.6992 - val_accuracy: 0.8288\n",
      "Epoch 63/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.9318\n",
      "Epoch 00063: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.3480 - accuracy: 0.9318 - val_loss: 0.7720 - val_accuracy: 0.8242\n",
      "Epoch 64/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.9340\n",
      "Epoch 00064: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3408 - accuracy: 0.9340 - val_loss: 0.6493 - val_accuracy: 0.8606\n",
      "Epoch 65/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.9346\n",
      "Epoch 00065: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3419 - accuracy: 0.9347 - val_loss: 0.6742 - val_accuracy: 0.8450\n",
      "Epoch 66/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.9326\n",
      "Epoch 00066: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3454 - accuracy: 0.9325 - val_loss: 0.6938 - val_accuracy: 0.8440\n",
      "Epoch 67/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.9359\n",
      "Epoch 00067: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3372 - accuracy: 0.9360 - val_loss: 0.7535 - val_accuracy: 0.8448\n",
      "Epoch 68/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.9376\n",
      "Epoch 00068: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3327 - accuracy: 0.9376 - val_loss: 0.7846 - val_accuracy: 0.8352\n",
      "Epoch 69/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.9354\n",
      "Epoch 00069: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3349 - accuracy: 0.9354 - val_loss: 0.7517 - val_accuracy: 0.8368\n",
      "Epoch 70/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.9379\n",
      "Epoch 00070: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3300 - accuracy: 0.9379 - val_loss: 0.5937 - val_accuracy: 0.8710\n",
      "Epoch 71/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.9410\n",
      "Epoch 00071: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3220 - accuracy: 0.9411 - val_loss: 0.6962 - val_accuracy: 0.8414\n",
      "Epoch 72/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.9384\n",
      "Epoch 00072: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3214 - accuracy: 0.9384 - val_loss: 0.6314 - val_accuracy: 0.8604\n",
      "Epoch 73/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.9396\n",
      "Epoch 00073: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3248 - accuracy: 0.9396 - val_loss: 0.5733 - val_accuracy: 0.8730\n",
      "Epoch 74/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.9414\n",
      "Epoch 00074: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.3160 - accuracy: 0.9413 - val_loss: 0.6066 - val_accuracy: 0.8662\n",
      "Epoch 75/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.9414\n",
      "Epoch 00075: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3155 - accuracy: 0.9414 - val_loss: 0.6219 - val_accuracy: 0.8624\n",
      "Epoch 76/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.9417\n",
      "Epoch 00076: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3149 - accuracy: 0.9418 - val_loss: 0.7668 - val_accuracy: 0.8338\n",
      "Epoch 77/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.9412\n",
      "Epoch 00077: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3155 - accuracy: 0.9412 - val_loss: 0.6485 - val_accuracy: 0.8566\n",
      "Epoch 78/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.9426\n",
      "Epoch 00078: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3135 - accuracy: 0.9426 - val_loss: 0.7254 - val_accuracy: 0.8438\n",
      "Epoch 79/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.9442\n",
      "Epoch 00079: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3072 - accuracy: 0.9442 - val_loss: 0.6587 - val_accuracy: 0.8542\n",
      "Epoch 80/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9420\n",
      "Epoch 00080: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.3122 - accuracy: 0.9419 - val_loss: 0.5932 - val_accuracy: 0.8730\n",
      "Epoch 81/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.9472\n",
      "Epoch 00081: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.2967 - accuracy: 0.9472 - val_loss: 0.6237 - val_accuracy: 0.8576\n",
      "Epoch 82/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.9480\n",
      "Epoch 00082: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.2954 - accuracy: 0.9480 - val_loss: 0.7761 - val_accuracy: 0.8300\n",
      "Epoch 83/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.9457\n",
      "Epoch 00083: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.3015 - accuracy: 0.9457 - val_loss: 0.7288 - val_accuracy: 0.8400\n",
      "Epoch 84/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.9450\n",
      "Epoch 00084: val_loss did not improve from 0.56025\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.3011 - accuracy: 0.9450 - val_loss: 0.6345 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9467\n",
      "Epoch 00085: val_loss improved from 0.56025 to 0.54233, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2999 - accuracy: 0.9467 - val_loss: 0.5423 - val_accuracy: 0.8804\n",
      "Epoch 86/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.9458\n",
      "Epoch 00086: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.2966 - accuracy: 0.9458 - val_loss: 0.6623 - val_accuracy: 0.8694\n",
      "Epoch 87/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.9475\n",
      "Epoch 00087: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.2958 - accuracy: 0.9476 - val_loss: 0.6568 - val_accuracy: 0.8640\n",
      "Epoch 88/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9475\n",
      "Epoch 00088: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.2921 - accuracy: 0.9476 - val_loss: 0.6286 - val_accuracy: 0.8630\n",
      "Epoch 89/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9482\n",
      "Epoch 00089: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.2895 - accuracy: 0.9483 - val_loss: 0.6677 - val_accuracy: 0.8612\n",
      "Epoch 90/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9494\n",
      "Epoch 00090: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2896 - accuracy: 0.9494 - val_loss: 0.7026 - val_accuracy: 0.8536\n",
      "Epoch 91/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9496\n",
      "Epoch 00091: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2870 - accuracy: 0.9496 - val_loss: 0.7555 - val_accuracy: 0.8440\n",
      "Epoch 92/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9497\n",
      "Epoch 00092: val_loss did not improve from 0.54233\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2879 - accuracy: 0.9497 - val_loss: 0.6317 - val_accuracy: 0.8614\n",
      "Epoch 93/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2477 - accuracy: 0.9652\n",
      "Epoch 00093: val_loss improved from 0.54233 to 0.48383, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 0.2479 - accuracy: 0.9651 - val_loss: 0.4838 - val_accuracy: 0.9014\n",
      "Epoch 94/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2222 - accuracy: 0.9742\n",
      "Epoch 00094: val_loss improved from 0.48383 to 0.47317, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 0.2221 - accuracy: 0.9742 - val_loss: 0.4732 - val_accuracy: 0.9034\n",
      "Epoch 95/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9771\n",
      "Epoch 00095: val_loss improved from 0.47317 to 0.45380, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2154 - accuracy: 0.9770 - val_loss: 0.4538 - val_accuracy: 0.9060\n",
      "Epoch 96/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9780\n",
      "Epoch 00096: val_loss improved from 0.45380 to 0.45084, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2101 - accuracy: 0.9780 - val_loss: 0.4508 - val_accuracy: 0.9048\n",
      "Epoch 97/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2054 - accuracy: 0.9803\n",
      "Epoch 00097: val_loss improved from 0.45084 to 0.45031, saving model to sgd_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.2055 - accuracy: 0.9802 - val_loss: 0.4503 - val_accuracy: 0.9084\n",
      "Epoch 98/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.2026 - accuracy: 0.9804\n",
      "Epoch 00098: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.2026 - accuracy: 0.9804 - val_loss: 0.4687 - val_accuracy: 0.9068\n",
      "Epoch 99/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9818\n",
      "Epoch 00099: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1997 - accuracy: 0.9818 - val_loss: 0.4564 - val_accuracy: 0.9112\n",
      "Epoch 100/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1968 - accuracy: 0.9827\n",
      "Epoch 00100: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1968 - accuracy: 0.9826 - val_loss: 0.4629 - val_accuracy: 0.9088\n",
      "Epoch 101/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9821\n",
      "Epoch 00101: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1961 - accuracy: 0.9822 - val_loss: 0.4561 - val_accuracy: 0.9090\n",
      "Epoch 102/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1917 - accuracy: 0.9840\n",
      "Epoch 00102: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1917 - accuracy: 0.9840 - val_loss: 0.4668 - val_accuracy: 0.9084\n",
      "Epoch 103/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9834\n",
      "Epoch 00103: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1925 - accuracy: 0.9834 - val_loss: 0.4697 - val_accuracy: 0.9084\n",
      "Epoch 104/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9845\n",
      "Epoch 00104: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1897 - accuracy: 0.9845 - val_loss: 0.4584 - val_accuracy: 0.9078\n",
      "Epoch 105/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9838\n",
      "Epoch 00105: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1896 - accuracy: 0.9838 - val_loss: 0.4670 - val_accuracy: 0.9048\n",
      "Epoch 106/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1864 - accuracy: 0.9855\n",
      "Epoch 00106: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1863 - accuracy: 0.9855 - val_loss: 0.4783 - val_accuracy: 0.9088\n",
      "Epoch 107/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9862\n",
      "Epoch 00107: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1848 - accuracy: 0.9862 - val_loss: 0.4659 - val_accuracy: 0.9072\n",
      "Epoch 108/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9863\n",
      "Epoch 00108: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1839 - accuracy: 0.9863 - val_loss: 0.4803 - val_accuracy: 0.9090\n",
      "Epoch 109/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9861\n",
      "Epoch 00109: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1833 - accuracy: 0.9861 - val_loss: 0.4623 - val_accuracy: 0.9084\n",
      "Epoch 110/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9871\n",
      "Epoch 00110: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1806 - accuracy: 0.9871 - val_loss: 0.4756 - val_accuracy: 0.9104\n",
      "Epoch 111/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9865\n",
      "Epoch 00111: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1809 - accuracy: 0.9865 - val_loss: 0.4696 - val_accuracy: 0.9124\n",
      "Epoch 112/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.9876\n",
      "Epoch 00112: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1790 - accuracy: 0.9875 - val_loss: 0.4663 - val_accuracy: 0.9108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9869\n",
      "Epoch 00113: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1782 - accuracy: 0.9870 - val_loss: 0.4685 - val_accuracy: 0.9086\n",
      "Epoch 114/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9884\n",
      "Epoch 00114: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1758 - accuracy: 0.9883 - val_loss: 0.4664 - val_accuracy: 0.9094\n",
      "Epoch 115/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9886\n",
      "Epoch 00115: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1754 - accuracy: 0.9886 - val_loss: 0.4676 - val_accuracy: 0.9102\n",
      "Epoch 116/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9875\n",
      "Epoch 00116: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1763 - accuracy: 0.9875 - val_loss: 0.4587 - val_accuracy: 0.9110\n",
      "Epoch 117/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9881\n",
      "Epoch 00117: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1735 - accuracy: 0.9881 - val_loss: 0.4708 - val_accuracy: 0.9074\n",
      "Epoch 118/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9888\n",
      "Epoch 00118: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1723 - accuracy: 0.9888 - val_loss: 0.4689 - val_accuracy: 0.9100\n",
      "Epoch 119/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9884\n",
      "Epoch 00119: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1722 - accuracy: 0.9884 - val_loss: 0.4773 - val_accuracy: 0.9100\n",
      "Epoch 120/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9893\n",
      "Epoch 00120: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1705 - accuracy: 0.9892 - val_loss: 0.4742 - val_accuracy: 0.9078\n",
      "Epoch 121/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9893\n",
      "Epoch 00121: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1701 - accuracy: 0.9893 - val_loss: 0.4651 - val_accuracy: 0.9122\n",
      "Epoch 122/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9887\n",
      "Epoch 00122: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1705 - accuracy: 0.9886 - val_loss: 0.4722 - val_accuracy: 0.9090\n",
      "Epoch 123/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9892\n",
      "Epoch 00123: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1692 - accuracy: 0.9892 - val_loss: 0.4779 - val_accuracy: 0.9112\n",
      "Epoch 124/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9904\n",
      "Epoch 00124: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1658 - accuracy: 0.9904 - val_loss: 0.4868 - val_accuracy: 0.9086\n",
      "Epoch 125/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.9888\n",
      "Epoch 00125: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1682 - accuracy: 0.9888 - val_loss: 0.4753 - val_accuracy: 0.9100\n",
      "Epoch 126/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9899\n",
      "Epoch 00126: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1665 - accuracy: 0.9899 - val_loss: 0.4859 - val_accuracy: 0.9112\n",
      "Epoch 127/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.9903\n",
      "Epoch 00127: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1653 - accuracy: 0.9904 - val_loss: 0.4842 - val_accuracy: 0.9082\n",
      "Epoch 128/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9900\n",
      "Epoch 00128: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1661 - accuracy: 0.9899 - val_loss: 0.5008 - val_accuracy: 0.9094\n",
      "Epoch 129/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1641 - accuracy: 0.9902\n",
      "Epoch 00129: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1641 - accuracy: 0.9902 - val_loss: 0.4696 - val_accuracy: 0.9112\n",
      "Epoch 130/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9902\n",
      "Epoch 00130: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1625 - accuracy: 0.9903 - val_loss: 0.4962 - val_accuracy: 0.9100\n",
      "Epoch 131/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1632 - accuracy: 0.9901\n",
      "Epoch 00131: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1633 - accuracy: 0.9901 - val_loss: 0.4754 - val_accuracy: 0.9116\n",
      "Epoch 132/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1611 - accuracy: 0.9907\n",
      "Epoch 00132: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1611 - accuracy: 0.9907 - val_loss: 0.4684 - val_accuracy: 0.9124\n",
      "Epoch 133/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9898\n",
      "Epoch 00133: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1624 - accuracy: 0.9899 - val_loss: 0.4795 - val_accuracy: 0.9124\n",
      "Epoch 134/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9916\n",
      "Epoch 00134: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1607 - accuracy: 0.9916 - val_loss: 0.4840 - val_accuracy: 0.9100\n",
      "Epoch 135/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9903\n",
      "Epoch 00135: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1611 - accuracy: 0.9904 - val_loss: 0.4721 - val_accuracy: 0.9100\n",
      "Epoch 136/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9907\n",
      "Epoch 00136: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1604 - accuracy: 0.9906 - val_loss: 0.4842 - val_accuracy: 0.9096\n",
      "Epoch 137/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9907\n",
      "Epoch 00137: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1599 - accuracy: 0.9907 - val_loss: 0.4932 - val_accuracy: 0.9072\n",
      "Epoch 138/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1582 - accuracy: 0.9909\n",
      "Epoch 00138: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1582 - accuracy: 0.9910 - val_loss: 0.4879 - val_accuracy: 0.9092\n",
      "Epoch 139/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1581 - accuracy: 0.9915\n",
      "Epoch 00139: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1582 - accuracy: 0.9914 - val_loss: 0.4890 - val_accuracy: 0.9106\n",
      "Epoch 140/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9919\n",
      "Epoch 00140: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1563 - accuracy: 0.9919 - val_loss: 0.4863 - val_accuracy: 0.9100\n",
      "Epoch 141/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9918\n",
      "Epoch 00141: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1573 - accuracy: 0.9917 - val_loss: 0.4999 - val_accuracy: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1581 - accuracy: 0.9910\n",
      "Epoch 00142: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1580 - accuracy: 0.9910 - val_loss: 0.4894 - val_accuracy: 0.9102\n",
      "Epoch 143/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.9919\n",
      "Epoch 00143: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1566 - accuracy: 0.9920 - val_loss: 0.4788 - val_accuracy: 0.9106\n",
      "Epoch 144/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9915\n",
      "Epoch 00144: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1569 - accuracy: 0.9915 - val_loss: 0.4739 - val_accuracy: 0.9122\n",
      "Epoch 145/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1570 - accuracy: 0.9916\n",
      "Epoch 00145: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1570 - accuracy: 0.9916 - val_loss: 0.4759 - val_accuracy: 0.9112\n",
      "Epoch 146/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9922\n",
      "Epoch 00146: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1557 - accuracy: 0.9922 - val_loss: 0.4751 - val_accuracy: 0.9110\n",
      "Epoch 147/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9918\n",
      "Epoch 00147: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1565 - accuracy: 0.9919 - val_loss: 0.4730 - val_accuracy: 0.9132\n",
      "Epoch 148/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9918\n",
      "Epoch 00148: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1569 - accuracy: 0.9918 - val_loss: 0.5148 - val_accuracy: 0.9118\n",
      "Epoch 149/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9916\n",
      "Epoch 00149: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 0.1563 - accuracy: 0.9916 - val_loss: 0.4792 - val_accuracy: 0.9118\n",
      "Epoch 150/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9909\n",
      "Epoch 00150: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1572 - accuracy: 0.9910 - val_loss: 0.4948 - val_accuracy: 0.9110\n",
      "Epoch 151/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9927\n",
      "Epoch 00151: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1545 - accuracy: 0.9927 - val_loss: 0.4769 - val_accuracy: 0.9104\n",
      "Epoch 152/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9924\n",
      "Epoch 00152: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1555 - accuracy: 0.9924 - val_loss: 0.4718 - val_accuracy: 0.9116\n",
      "Epoch 153/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9919\n",
      "Epoch 00153: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1567 - accuracy: 0.9919 - val_loss: 0.5118 - val_accuracy: 0.9112\n",
      "Epoch 154/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9916\n",
      "Epoch 00154: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1568 - accuracy: 0.9916 - val_loss: 0.4725 - val_accuracy: 0.9114\n",
      "Epoch 155/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9920\n",
      "Epoch 00155: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1559 - accuracy: 0.9921 - val_loss: 0.4959 - val_accuracy: 0.9102\n",
      "Epoch 156/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9921\n",
      "Epoch 00156: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1561 - accuracy: 0.9921 - val_loss: 0.4726 - val_accuracy: 0.9110\n",
      "Epoch 157/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9930\n",
      "Epoch 00157: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 224ms/step - loss: 0.1544 - accuracy: 0.9930 - val_loss: 0.4892 - val_accuracy: 0.9114\n",
      "Epoch 158/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9921\n",
      "Epoch 00158: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1561 - accuracy: 0.9921 - val_loss: 0.4847 - val_accuracy: 0.9118\n",
      "Epoch 159/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9925\n",
      "Epoch 00159: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1545 - accuracy: 0.9925 - val_loss: 0.4735 - val_accuracy: 0.9128\n",
      "Epoch 160/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9931\n",
      "Epoch 00160: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1544 - accuracy: 0.9931 - val_loss: 0.4774 - val_accuracy: 0.9108\n",
      "Epoch 161/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9926\n",
      "Epoch 00161: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1546 - accuracy: 0.9926 - val_loss: 0.4722 - val_accuracy: 0.9114\n",
      "Epoch 162/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9925\n",
      "Epoch 00162: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1548 - accuracy: 0.9925 - val_loss: 0.4741 - val_accuracy: 0.9114\n",
      "Epoch 163/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9915\n",
      "Epoch 00163: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1557 - accuracy: 0.9915 - val_loss: 0.4783 - val_accuracy: 0.9124\n",
      "Epoch 164/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9925\n",
      "Epoch 00164: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1541 - accuracy: 0.9925 - val_loss: 0.4800 - val_accuracy: 0.9114\n",
      "Epoch 165/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9922\n",
      "Epoch 00165: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1557 - accuracy: 0.9922 - val_loss: 0.4851 - val_accuracy: 0.9106\n",
      "Epoch 166/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1552 - accuracy: 0.9919\n",
      "Epoch 00166: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1552 - accuracy: 0.9919 - val_loss: 0.4723 - val_accuracy: 0.9116\n",
      "Epoch 167/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9922\n",
      "Epoch 00167: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1539 - accuracy: 0.9922 - val_loss: 0.4784 - val_accuracy: 0.9108\n",
      "Epoch 168/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9926\n",
      "Epoch 00168: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1551 - accuracy: 0.9926 - val_loss: 0.4713 - val_accuracy: 0.9110\n",
      "Epoch 169/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9916\n",
      "Epoch 00169: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1557 - accuracy: 0.9916 - val_loss: 0.4934 - val_accuracy: 0.9106\n",
      "Epoch 170/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9925\n",
      "Epoch 00170: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1546 - accuracy: 0.9925 - val_loss: 0.4900 - val_accuracy: 0.9106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9923\n",
      "Epoch 00171: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1544 - accuracy: 0.9923 - val_loss: 0.4718 - val_accuracy: 0.9106\n",
      "Epoch 172/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9918\n",
      "Epoch 00172: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1545 - accuracy: 0.9919 - val_loss: 0.4796 - val_accuracy: 0.9120\n",
      "Epoch 173/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9917\n",
      "Epoch 00173: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1551 - accuracy: 0.9917 - val_loss: 0.4710 - val_accuracy: 0.9116\n",
      "Epoch 174/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9921\n",
      "Epoch 00174: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1542 - accuracy: 0.9921 - val_loss: 0.4818 - val_accuracy: 0.9106\n",
      "Epoch 175/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9923\n",
      "Epoch 00175: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1540 - accuracy: 0.9923 - val_loss: 0.4796 - val_accuracy: 0.9112\n",
      "Epoch 176/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9926\n",
      "Epoch 00176: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1532 - accuracy: 0.9927 - val_loss: 0.4785 - val_accuracy: 0.9106\n",
      "Epoch 177/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9922\n",
      "Epoch 00177: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1547 - accuracy: 0.9921 - val_loss: 0.4713 - val_accuracy: 0.9112\n",
      "Epoch 178/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9923\n",
      "Epoch 00178: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1541 - accuracy: 0.9923 - val_loss: 0.4711 - val_accuracy: 0.9114\n",
      "Epoch 179/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9935\n",
      "Epoch 00179: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 0.1522 - accuracy: 0.9935 - val_loss: 0.4919 - val_accuracy: 0.9100\n",
      "Epoch 180/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1550 - accuracy: 0.9916\n",
      "Epoch 00180: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1550 - accuracy: 0.9916 - val_loss: 0.4713 - val_accuracy: 0.9118\n",
      "Epoch 181/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9923\n",
      "Epoch 00181: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 78s 223ms/step - loss: 0.1538 - accuracy: 0.9923 - val_loss: 0.4822 - val_accuracy: 0.9104\n",
      "Epoch 182/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9924\n",
      "Epoch 00182: val_loss did not improve from 0.45031\n",
      "351/351 [==============================] - 79s 224ms/step - loss: 0.1540 - accuracy: 0.9924 - val_loss: 0.4716 - val_accuracy: 0.9104\n"
     ]
    }
   ],
   "source": [
    "history_sgd = model_sgd.fit_generator(training_data_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_data_generator,\n",
    "                  callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model trained using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_sgd = model_sgd.predict(x_t,batch_size=100)\n",
    "y_pred_sgd = np.array([np.argmax(i) for i in y_test_pred_sgd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy using SGD optimizer is :  0.9057\n"
     ]
    }
   ],
   "source": [
    "m_sgd = Accuracy()\n",
    "m_sgd.update_state(y_true,y_pred_sgd)\n",
    "print('Test Accuracy using SGD optimizer is : ', m_sgd.result().numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.  RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.9\n",
    "rms_prop = tf.keras.optimizers.RMSprop(learning_rate=learning_rate_schedule(0),rho=rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [lr_scheduler,tensorboard_callback(),model_check_point('rms')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms = resnet20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rms.compile(loss='categorical_crossentropy', optimizer=rms_prop, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 2.7547 - accuracy: 0.1499\n",
      "Epoch 00001: val_loss improved from inf to 2.47538, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 2.7531 - accuracy: 0.1499 - val_loss: 2.4754 - val_accuracy: 0.1012\n",
      "Epoch 2/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 2.1475 - accuracy: 0.1882\n",
      "Epoch 00002: val_loss did not improve from 2.47538\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 2.1474 - accuracy: 0.1883 - val_loss: 2.9585 - val_accuracy: 0.0932\n",
      "Epoch 3/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 2.0542 - accuracy: 0.2249\n",
      "Epoch 00003: val_loss improved from 2.47538 to 2.04162, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 2.0538 - accuracy: 0.2249 - val_loss: 2.0416 - val_accuracy: 0.2232\n",
      "Epoch 4/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 2.0064 - accuracy: 0.2468\n",
      "Epoch 00004: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 2.0064 - accuracy: 0.2469 - val_loss: 2.6748 - val_accuracy: 0.1560\n",
      "Epoch 5/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9911 - accuracy: 0.2584\n",
      "Epoch 00005: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.9915 - accuracy: 0.2583 - val_loss: 7.7648 - val_accuracy: 0.1166\n",
      "Epoch 6/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9737 - accuracy: 0.2716\n",
      "Epoch 00006: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9734 - accuracy: 0.2717 - val_loss: 2.9118 - val_accuracy: 0.1064\n",
      "Epoch 7/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9755 - accuracy: 0.2693\n",
      "Epoch 00007: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9754 - accuracy: 0.2693 - val_loss: 2.3895 - val_accuracy: 0.2010\n",
      "Epoch 8/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9577 - accuracy: 0.2845\n",
      "Epoch 00008: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9577 - accuracy: 0.2844 - val_loss: 6.3907 - val_accuracy: 0.0986\n",
      "Epoch 9/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9493 - accuracy: 0.2895\n",
      "Epoch 00009: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9494 - accuracy: 0.2894 - val_loss: 2.4970 - val_accuracy: 0.1652\n",
      "Epoch 10/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9391 - accuracy: 0.2916\n",
      "Epoch 00010: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9389 - accuracy: 0.2916 - val_loss: 2.0868 - val_accuracy: 0.2608\n",
      "Epoch 11/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9391 - accuracy: 0.2951\n",
      "Epoch 00011: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9389 - accuracy: 0.2953 - val_loss: 3.3500 - val_accuracy: 0.1336\n",
      "Epoch 12/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9354 - accuracy: 0.2992\n",
      "Epoch 00012: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9354 - accuracy: 0.2992 - val_loss: 3.1992 - val_accuracy: 0.1252\n",
      "Epoch 13/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9284 - accuracy: 0.2986\n",
      "Epoch 00013: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.9285 - accuracy: 0.2986 - val_loss: 3.2129 - val_accuracy: 0.2066\n",
      "Epoch 14/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9302 - accuracy: 0.2994\n",
      "Epoch 00014: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9301 - accuracy: 0.2993 - val_loss: 2.6386 - val_accuracy: 0.1850\n",
      "Epoch 15/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9253 - accuracy: 0.3055\n",
      "Epoch 00015: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9254 - accuracy: 0.3056 - val_loss: 3.4233 - val_accuracy: 0.1198\n",
      "Epoch 16/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9261 - accuracy: 0.3045\n",
      "Epoch 00016: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9260 - accuracy: 0.3047 - val_loss: 3.3624 - val_accuracy: 0.2218\n",
      "Epoch 17/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9230 - accuracy: 0.3062\n",
      "Epoch 00017: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9224 - accuracy: 0.3063 - val_loss: 2.8124 - val_accuracy: 0.1418\n",
      "Epoch 18/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9216 - accuracy: 0.3087\n",
      "Epoch 00018: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9219 - accuracy: 0.3084 - val_loss: 4.1407 - val_accuracy: 0.1166\n",
      "Epoch 19/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9261 - accuracy: 0.3049\n",
      "Epoch 00019: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9263 - accuracy: 0.3047 - val_loss: 4.8730 - val_accuracy: 0.0984\n",
      "Epoch 20/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9202 - accuracy: 0.3061\n",
      "Epoch 00020: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9203 - accuracy: 0.3061 - val_loss: 3.1716 - val_accuracy: 0.1148\n",
      "Epoch 21/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9271 - accuracy: 0.3037\n",
      "Epoch 00021: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9267 - accuracy: 0.3038 - val_loss: 2.2781 - val_accuracy: 0.2556\n",
      "Epoch 22/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9212 - accuracy: 0.3069\n",
      "Epoch 00022: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9215 - accuracy: 0.3068 - val_loss: 3.6327 - val_accuracy: 0.1400\n",
      "Epoch 23/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9234 - accuracy: 0.3058\n",
      "Epoch 00023: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9230 - accuracy: 0.3059 - val_loss: 2.5631 - val_accuracy: 0.1978\n",
      "Epoch 24/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9314 - accuracy: 0.3062\n",
      "Epoch 00024: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9314 - accuracy: 0.3062 - val_loss: 3.3253 - val_accuracy: 0.1864\n",
      "Epoch 25/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9141 - accuracy: 0.3104\n",
      "Epoch 00025: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9143 - accuracy: 0.3103 - val_loss: 2.6746 - val_accuracy: 0.1160\n",
      "Epoch 26/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9141 - accuracy: 0.3097\n",
      "Epoch 00026: val_loss did not improve from 2.04162\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9139 - accuracy: 0.3098 - val_loss: 10.9266 - val_accuracy: 0.0984\n",
      "Epoch 27/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9260 - accuracy: 0.3097\n",
      "Epoch 00027: val_loss improved from 2.04162 to 1.87338, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9255 - accuracy: 0.3099 - val_loss: 1.8734 - val_accuracy: 0.3314\n",
      "Epoch 28/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9138 - accuracy: 0.3082\n",
      "Epoch 00028: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9142 - accuracy: 0.3082 - val_loss: 3.1640 - val_accuracy: 0.1008\n",
      "Epoch 29/182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/351 [============================>.] - ETA: 0s - loss: 1.9136 - accuracy: 0.3094\n",
      "Epoch 00029: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9139 - accuracy: 0.3093 - val_loss: 2.4783 - val_accuracy: 0.1186\n",
      "Epoch 30/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9148 - accuracy: 0.3090\n",
      "Epoch 00030: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9149 - accuracy: 0.3089 - val_loss: 3.6212 - val_accuracy: 0.1146\n",
      "Epoch 31/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9268 - accuracy: 0.3067\n",
      "Epoch 00031: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9269 - accuracy: 0.3068 - val_loss: 5.5755 - val_accuracy: 0.0982\n",
      "Epoch 32/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9087 - accuracy: 0.3089\n",
      "Epoch 00032: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9087 - accuracy: 0.3088 - val_loss: 3.8197 - val_accuracy: 0.1044\n",
      "Epoch 33/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9163 - accuracy: 0.3108\n",
      "Epoch 00033: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9166 - accuracy: 0.3105 - val_loss: 2.0953 - val_accuracy: 0.2350\n",
      "Epoch 34/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9148 - accuracy: 0.3107\n",
      "Epoch 00034: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9149 - accuracy: 0.3107 - val_loss: 2.3275 - val_accuracy: 0.1596\n",
      "Epoch 35/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9077 - accuracy: 0.3135\n",
      "Epoch 00035: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9077 - accuracy: 0.3135 - val_loss: 8.4477 - val_accuracy: 0.0984\n",
      "Epoch 36/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9165 - accuracy: 0.3093\n",
      "Epoch 00036: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9164 - accuracy: 0.3091 - val_loss: 2.2723 - val_accuracy: 0.2360\n",
      "Epoch 37/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9178 - accuracy: 0.3127\n",
      "Epoch 00037: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9178 - accuracy: 0.3128 - val_loss: 3.9585 - val_accuracy: 0.1652\n",
      "Epoch 38/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9219 - accuracy: 0.3098\n",
      "Epoch 00038: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9220 - accuracy: 0.3095 - val_loss: 3.9921 - val_accuracy: 0.0984\n",
      "Epoch 39/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9126 - accuracy: 0.3120\n",
      "Epoch 00039: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9126 - accuracy: 0.3120 - val_loss: 3.3559 - val_accuracy: 0.1180\n",
      "Epoch 40/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9141 - accuracy: 0.3094\n",
      "Epoch 00040: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9140 - accuracy: 0.3094 - val_loss: 2.9923 - val_accuracy: 0.1338\n",
      "Epoch 41/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9194 - accuracy: 0.3069\n",
      "Epoch 00041: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.9197 - accuracy: 0.3069 - val_loss: 6.6755 - val_accuracy: 0.1038\n",
      "Epoch 42/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9153 - accuracy: 0.3092\n",
      "Epoch 00042: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9156 - accuracy: 0.3092 - val_loss: 5.2037 - val_accuracy: 0.1152\n",
      "Epoch 43/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9263 - accuracy: 0.3066\n",
      "Epoch 00043: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9259 - accuracy: 0.3068 - val_loss: 5.4056 - val_accuracy: 0.1186\n",
      "Epoch 44/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9204 - accuracy: 0.3093\n",
      "Epoch 00044: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 304ms/step - loss: 1.9203 - accuracy: 0.3093 - val_loss: 3.8430 - val_accuracy: 0.1544\n",
      "Epoch 45/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9126 - accuracy: 0.3113\n",
      "Epoch 00045: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.9124 - accuracy: 0.3114 - val_loss: 2.5998 - val_accuracy: 0.1372\n",
      "Epoch 46/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9160 - accuracy: 0.3104\n",
      "Epoch 00046: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9162 - accuracy: 0.3105 - val_loss: 6.0533 - val_accuracy: 0.1006\n",
      "Epoch 47/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9145 - accuracy: 0.3091\n",
      "Epoch 00047: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9144 - accuracy: 0.3092 - val_loss: 2.1236 - val_accuracy: 0.2706\n",
      "Epoch 48/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9142 - accuracy: 0.3122\n",
      "Epoch 00048: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9141 - accuracy: 0.3122 - val_loss: 2.7148 - val_accuracy: 0.1612\n",
      "Epoch 49/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9181 - accuracy: 0.3145\n",
      "Epoch 00049: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9182 - accuracy: 0.3145 - val_loss: 6.6308 - val_accuracy: 0.1100\n",
      "Epoch 50/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9111 - accuracy: 0.3115\n",
      "Epoch 00050: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 309ms/step - loss: 1.9110 - accuracy: 0.3115 - val_loss: 2.5501 - val_accuracy: 0.2170\n",
      "Epoch 51/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9100 - accuracy: 0.3131\n",
      "Epoch 00051: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9102 - accuracy: 0.3131 - val_loss: 2.8063 - val_accuracy: 0.1092\n",
      "Epoch 52/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9048 - accuracy: 0.3166\n",
      "Epoch 00052: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9045 - accuracy: 0.3164 - val_loss: 5.1763 - val_accuracy: 0.1004\n",
      "Epoch 53/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9119 - accuracy: 0.3134\n",
      "Epoch 00053: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9120 - accuracy: 0.3133 - val_loss: 4.3199 - val_accuracy: 0.1552\n",
      "Epoch 54/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9034 - accuracy: 0.3183\n",
      "Epoch 00054: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9034 - accuracy: 0.3184 - val_loss: 3.2642 - val_accuracy: 0.1332\n",
      "Epoch 55/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9103 - accuracy: 0.3109\n",
      "Epoch 00055: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9104 - accuracy: 0.3109 - val_loss: 2.5364 - val_accuracy: 0.1992\n",
      "Epoch 56/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9047 - accuracy: 0.3150\n",
      "Epoch 00056: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9048 - accuracy: 0.3150 - val_loss: 9.4918 - val_accuracy: 0.0984\n",
      "Epoch 57/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9096 - accuracy: 0.3153\n",
      "Epoch 00057: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9096 - accuracy: 0.3151 - val_loss: 2.2562 - val_accuracy: 0.1926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9099 - accuracy: 0.3169\n",
      "Epoch 00058: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9102 - accuracy: 0.3168 - val_loss: 12.2480 - val_accuracy: 0.0908\n",
      "Epoch 59/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9055 - accuracy: 0.3152\n",
      "Epoch 00059: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9054 - accuracy: 0.3153 - val_loss: 1.9692 - val_accuracy: 0.2956\n",
      "Epoch 60/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9110 - accuracy: 0.3159\n",
      "Epoch 00060: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9109 - accuracy: 0.3161 - val_loss: 5.0206 - val_accuracy: 0.0986\n",
      "Epoch 61/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9101 - accuracy: 0.3153\n",
      "Epoch 00061: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9101 - accuracy: 0.3151 - val_loss: 10.0926 - val_accuracy: 0.1002\n",
      "Epoch 62/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9179 - accuracy: 0.3130\n",
      "Epoch 00062: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9181 - accuracy: 0.3130 - val_loss: 4.5388 - val_accuracy: 0.1092\n",
      "Epoch 63/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9127 - accuracy: 0.3142\n",
      "Epoch 00063: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9132 - accuracy: 0.3140 - val_loss: 6.2498 - val_accuracy: 0.1170\n",
      "Epoch 64/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9119 - accuracy: 0.3133\n",
      "Epoch 00064: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9116 - accuracy: 0.3134 - val_loss: 3.0407 - val_accuracy: 0.1438\n",
      "Epoch 65/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9087 - accuracy: 0.3130\n",
      "Epoch 00065: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9088 - accuracy: 0.3130 - val_loss: 7.5568 - val_accuracy: 0.0984\n",
      "Epoch 66/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9124 - accuracy: 0.3094\n",
      "Epoch 00066: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9123 - accuracy: 0.3093 - val_loss: 2.4814 - val_accuracy: 0.2032\n",
      "Epoch 67/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9086 - accuracy: 0.3165\n",
      "Epoch 00067: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9086 - accuracy: 0.3164 - val_loss: 7.5498 - val_accuracy: 0.1218\n",
      "Epoch 68/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9102 - accuracy: 0.3122\n",
      "Epoch 00068: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9105 - accuracy: 0.3121 - val_loss: 3.7083 - val_accuracy: 0.2126\n",
      "Epoch 69/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9167 - accuracy: 0.3099\n",
      "Epoch 00069: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.9166 - accuracy: 0.3100 - val_loss: 5.5810 - val_accuracy: 0.1170\n",
      "Epoch 70/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9107 - accuracy: 0.3167\n",
      "Epoch 00070: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9110 - accuracy: 0.3166 - val_loss: 6.5312 - val_accuracy: 0.1002\n",
      "Epoch 71/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9091 - accuracy: 0.3163\n",
      "Epoch 00071: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9093 - accuracy: 0.3163 - val_loss: 2.5291 - val_accuracy: 0.1422\n",
      "Epoch 72/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9124 - accuracy: 0.3087\n",
      "Epoch 00072: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9126 - accuracy: 0.3084 - val_loss: 1.9063 - val_accuracy: 0.3370\n",
      "Epoch 73/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9109 - accuracy: 0.3168\n",
      "Epoch 00073: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9110 - accuracy: 0.3167 - val_loss: 6.0856 - val_accuracy: 0.1202\n",
      "Epoch 74/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9123 - accuracy: 0.3153\n",
      "Epoch 00074: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9121 - accuracy: 0.3154 - val_loss: 7.7066 - val_accuracy: 0.1054\n",
      "Epoch 75/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9120 - accuracy: 0.3130\n",
      "Epoch 00075: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9120 - accuracy: 0.3130 - val_loss: 3.9777 - val_accuracy: 0.1236\n",
      "Epoch 76/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9167 - accuracy: 0.3177\n",
      "Epoch 00076: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 309ms/step - loss: 1.9164 - accuracy: 0.3179 - val_loss: 8.2002 - val_accuracy: 0.1028\n",
      "Epoch 77/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9115 - accuracy: 0.3147\n",
      "Epoch 00077: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9113 - accuracy: 0.3149 - val_loss: 2.3229 - val_accuracy: 0.2350\n",
      "Epoch 78/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9109 - accuracy: 0.3155\n",
      "Epoch 00078: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9110 - accuracy: 0.3155 - val_loss: 2.1427 - val_accuracy: 0.2308\n",
      "Epoch 79/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9042 - accuracy: 0.3192\n",
      "Epoch 00079: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9041 - accuracy: 0.3194 - val_loss: 2.2374 - val_accuracy: 0.2218\n",
      "Epoch 80/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9175 - accuracy: 0.3153\n",
      "Epoch 00080: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9174 - accuracy: 0.3153 - val_loss: 5.4465 - val_accuracy: 0.1044\n",
      "Epoch 81/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9163 - accuracy: 0.3095\n",
      "Epoch 00081: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9163 - accuracy: 0.3094 - val_loss: 2.5109 - val_accuracy: 0.2320\n",
      "Epoch 82/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9120 - accuracy: 0.3166\n",
      "Epoch 00082: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9121 - accuracy: 0.3165 - val_loss: 8.0311 - val_accuracy: 0.1006\n",
      "Epoch 83/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9100 - accuracy: 0.3142\n",
      "Epoch 00083: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9101 - accuracy: 0.3141 - val_loss: 2.6057 - val_accuracy: 0.2074\n",
      "Epoch 84/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9090 - accuracy: 0.3160\n",
      "Epoch 00084: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.9087 - accuracy: 0.3160 - val_loss: 4.2771 - val_accuracy: 0.1018\n",
      "Epoch 85/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9096 - accuracy: 0.3149\n",
      "Epoch 00085: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9097 - accuracy: 0.3148 - val_loss: 2.2247 - val_accuracy: 0.2652\n",
      "Epoch 86/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9099 - accuracy: 0.3150\n",
      "Epoch 00086: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.9101 - accuracy: 0.3149 - val_loss: 10.5055 - val_accuracy: 0.1002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9125 - accuracy: 0.3151\n",
      "Epoch 00087: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 309ms/step - loss: 1.9127 - accuracy: 0.3151 - val_loss: 9.8328 - val_accuracy: 0.0984\n",
      "Epoch 88/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9155 - accuracy: 0.3150\n",
      "Epoch 00088: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 109s 309ms/step - loss: 1.9155 - accuracy: 0.3151 - val_loss: 2.8995 - val_accuracy: 0.1092\n",
      "Epoch 89/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9153 - accuracy: 0.3132\n",
      "Epoch 00089: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 109s 310ms/step - loss: 1.9153 - accuracy: 0.3130 - val_loss: 4.3365 - val_accuracy: 0.1200\n",
      "Epoch 90/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9084 - accuracy: 0.3138\n",
      "Epoch 00090: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 309ms/step - loss: 1.9085 - accuracy: 0.3137 - val_loss: 2.2918 - val_accuracy: 0.2438\n",
      "Epoch 91/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9147 - accuracy: 0.3127\n",
      "Epoch 00091: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.9144 - accuracy: 0.3129 - val_loss: 4.0280 - val_accuracy: 0.1538\n",
      "Epoch 92/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9112 - accuracy: 0.3133\n",
      "Epoch 00092: val_loss did not improve from 1.87338\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.9115 - accuracy: 0.3132 - val_loss: 2.0061 - val_accuracy: 0.3048\n",
      "Epoch 93/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7386 - accuracy: 0.3544\n",
      "Epoch 00093: val_loss improved from 1.87338 to 1.86466, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.7385 - accuracy: 0.3543 - val_loss: 1.8647 - val_accuracy: 0.2992\n",
      "Epoch 94/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7097 - accuracy: 0.3628\n",
      "Epoch 00094: val_loss improved from 1.86466 to 1.83658, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.7097 - accuracy: 0.3629 - val_loss: 1.8366 - val_accuracy: 0.3270\n",
      "Epoch 95/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6966 - accuracy: 0.3682\n",
      "Epoch 00095: val_loss improved from 1.83658 to 1.74374, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6969 - accuracy: 0.3681 - val_loss: 1.7437 - val_accuracy: 0.3526\n",
      "Epoch 96/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6829 - accuracy: 0.3757\n",
      "Epoch 00096: val_loss improved from 1.74374 to 1.67817, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6826 - accuracy: 0.3758 - val_loss: 1.6782 - val_accuracy: 0.3680\n",
      "Epoch 97/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6798 - accuracy: 0.3785\n",
      "Epoch 00097: val_loss did not improve from 1.67817\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6795 - accuracy: 0.3786 - val_loss: 1.9019 - val_accuracy: 0.2618\n",
      "Epoch 98/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6653 - accuracy: 0.3820\n",
      "Epoch 00098: val_loss did not improve from 1.67817\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6655 - accuracy: 0.3819 - val_loss: 1.7082 - val_accuracy: 0.3630\n",
      "Epoch 99/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6625 - accuracy: 0.3789\n",
      "Epoch 00099: val_loss did not improve from 1.67817\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6626 - accuracy: 0.3789 - val_loss: 2.2999 - val_accuracy: 0.2236\n",
      "Epoch 100/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6551 - accuracy: 0.3854\n",
      "Epoch 00100: val_loss did not improve from 1.67817\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6552 - accuracy: 0.3854 - val_loss: 1.8338 - val_accuracy: 0.3338\n",
      "Epoch 101/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6514 - accuracy: 0.3861\n",
      "Epoch 00101: val_loss did not improve from 1.67817\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6514 - accuracy: 0.3862 - val_loss: 1.7281 - val_accuracy: 0.3608\n",
      "Epoch 102/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6487 - accuracy: 0.3846\n",
      "Epoch 00102: val_loss did not improve from 1.67817\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6487 - accuracy: 0.3845 - val_loss: 2.7791 - val_accuracy: 0.1630\n",
      "Epoch 103/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6499 - accuracy: 0.3831\n",
      "Epoch 00103: val_loss improved from 1.67817 to 1.62647, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 109s 309ms/step - loss: 1.6496 - accuracy: 0.3833 - val_loss: 1.6265 - val_accuracy: 0.3886\n",
      "Epoch 104/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6464 - accuracy: 0.3883\n",
      "Epoch 00104: val_loss did not improve from 1.62647\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6463 - accuracy: 0.3884 - val_loss: 5.4133 - val_accuracy: 0.1232\n",
      "Epoch 105/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6403 - accuracy: 0.3931\n",
      "Epoch 00105: val_loss did not improve from 1.62647\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6400 - accuracy: 0.3933 - val_loss: 3.2475 - val_accuracy: 0.2116\n",
      "Epoch 106/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6385 - accuracy: 0.3920\n",
      "Epoch 00106: val_loss improved from 1.62647 to 1.61508, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 309ms/step - loss: 1.6391 - accuracy: 0.3918 - val_loss: 1.6151 - val_accuracy: 0.3968\n",
      "Epoch 107/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6382 - accuracy: 0.3911\n",
      "Epoch 00107: val_loss did not improve from 1.61508\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6382 - accuracy: 0.3910 - val_loss: 2.6295 - val_accuracy: 0.2204\n",
      "Epoch 108/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6330 - accuracy: 0.3941\n",
      "Epoch 00108: val_loss did not improve from 1.61508\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6332 - accuracy: 0.3941 - val_loss: 2.9266 - val_accuracy: 0.1640\n",
      "Epoch 109/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6329 - accuracy: 0.3967\n",
      "Epoch 00109: val_loss improved from 1.61508 to 1.61465, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6330 - accuracy: 0.3967 - val_loss: 1.6146 - val_accuracy: 0.4022\n",
      "Epoch 110/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6325 - accuracy: 0.3948\n",
      "Epoch 00110: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6330 - accuracy: 0.3945 - val_loss: 1.7014 - val_accuracy: 0.3548\n",
      "Epoch 111/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6326 - accuracy: 0.3921\n",
      "Epoch 00111: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6325 - accuracy: 0.3922 - val_loss: 2.0462 - val_accuracy: 0.2824\n",
      "Epoch 112/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6306 - accuracy: 0.3963\n",
      "Epoch 00112: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6309 - accuracy: 0.3962 - val_loss: 1.7795 - val_accuracy: 0.3582\n",
      "Epoch 113/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6228 - accuracy: 0.4005\n",
      "Epoch 00113: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.6224 - accuracy: 0.4005 - val_loss: 1.7679 - val_accuracy: 0.3844\n",
      "Epoch 114/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6267 - accuracy: 0.3985\n",
      "Epoch 00114: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6270 - accuracy: 0.3985 - val_loss: 1.9723 - val_accuracy: 0.2992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6247 - accuracy: 0.3943\n",
      "Epoch 00115: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6247 - accuracy: 0.3944 - val_loss: 1.9363 - val_accuracy: 0.3132\n",
      "Epoch 116/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6240 - accuracy: 0.4016\n",
      "Epoch 00116: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6238 - accuracy: 0.4018 - val_loss: 1.7603 - val_accuracy: 0.3356\n",
      "Epoch 117/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6239 - accuracy: 0.3992\n",
      "Epoch 00117: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6238 - accuracy: 0.3990 - val_loss: 2.2940 - val_accuracy: 0.2438\n",
      "Epoch 118/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6210 - accuracy: 0.3975\n",
      "Epoch 00118: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6208 - accuracy: 0.3976 - val_loss: 1.7845 - val_accuracy: 0.3502\n",
      "Epoch 119/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6179 - accuracy: 0.3996\n",
      "Epoch 00119: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6179 - accuracy: 0.3998 - val_loss: 4.9137 - val_accuracy: 0.1386\n",
      "Epoch 120/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6208 - accuracy: 0.3971\n",
      "Epoch 00120: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.6211 - accuracy: 0.3971 - val_loss: 1.6812 - val_accuracy: 0.3684\n",
      "Epoch 121/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6152 - accuracy: 0.4014\n",
      "Epoch 00121: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 304ms/step - loss: 1.6153 - accuracy: 0.4014 - val_loss: 1.7090 - val_accuracy: 0.3806\n",
      "Epoch 122/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6182 - accuracy: 0.4028\n",
      "Epoch 00122: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6183 - accuracy: 0.4027 - val_loss: 1.9050 - val_accuracy: 0.3422\n",
      "Epoch 123/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6207 - accuracy: 0.4000\n",
      "Epoch 00123: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6209 - accuracy: 0.3999 - val_loss: 3.2222 - val_accuracy: 0.1916\n",
      "Epoch 124/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6135 - accuracy: 0.4000\n",
      "Epoch 00124: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6138 - accuracy: 0.3999 - val_loss: 3.8005 - val_accuracy: 0.1894\n",
      "Epoch 125/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6189 - accuracy: 0.4002\n",
      "Epoch 00125: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6189 - accuracy: 0.4003 - val_loss: 2.5451 - val_accuracy: 0.2310\n",
      "Epoch 126/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6107 - accuracy: 0.4049\n",
      "Epoch 00126: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6105 - accuracy: 0.4050 - val_loss: 2.1702 - val_accuracy: 0.2830\n",
      "Epoch 127/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6082 - accuracy: 0.4049\n",
      "Epoch 00127: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6079 - accuracy: 0.4050 - val_loss: 5.4521 - val_accuracy: 0.1572\n",
      "Epoch 128/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6122 - accuracy: 0.4036\n",
      "Epoch 00128: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6124 - accuracy: 0.4036 - val_loss: 3.2159 - val_accuracy: 0.2108\n",
      "Epoch 129/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6184 - accuracy: 0.4003\n",
      "Epoch 00129: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6188 - accuracy: 0.4002 - val_loss: 2.1029 - val_accuracy: 0.2864\n",
      "Epoch 130/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6089 - accuracy: 0.4015\n",
      "Epoch 00130: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6087 - accuracy: 0.4016 - val_loss: 2.2103 - val_accuracy: 0.2688\n",
      "Epoch 131/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6140 - accuracy: 0.4022\n",
      "Epoch 00131: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6142 - accuracy: 0.4021 - val_loss: 1.8000 - val_accuracy: 0.3380\n",
      "Epoch 132/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6104 - accuracy: 0.4049\n",
      "Epoch 00132: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6104 - accuracy: 0.4049 - val_loss: 1.8582 - val_accuracy: 0.3380\n",
      "Epoch 133/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6105 - accuracy: 0.4064\n",
      "Epoch 00133: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.6105 - accuracy: 0.4065 - val_loss: 2.1527 - val_accuracy: 0.2660\n",
      "Epoch 134/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6094 - accuracy: 0.4023\n",
      "Epoch 00134: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.6089 - accuracy: 0.4024 - val_loss: 1.6350 - val_accuracy: 0.3924\n",
      "Epoch 135/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6077 - accuracy: 0.4044\n",
      "Epoch 00135: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.6077 - accuracy: 0.4044 - val_loss: 2.4094 - val_accuracy: 0.2556\n",
      "Epoch 136/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6084 - accuracy: 0.4045\n",
      "Epoch 00136: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.6088 - accuracy: 0.4044 - val_loss: 2.1281 - val_accuracy: 0.2604\n",
      "Epoch 137/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6068 - accuracy: 0.4045\n",
      "Epoch 00137: val_loss did not improve from 1.61465\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.6064 - accuracy: 0.4046 - val_loss: 1.6440 - val_accuracy: 0.3984\n",
      "Epoch 138/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5739 - accuracy: 0.4205\n",
      "Epoch 00138: val_loss improved from 1.61465 to 1.60607, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5740 - accuracy: 0.4205 - val_loss: 1.6061 - val_accuracy: 0.4078\n",
      "Epoch 139/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5799 - accuracy: 0.4174\n",
      "Epoch 00139: val_loss improved from 1.60607 to 1.59656, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5799 - accuracy: 0.4174 - val_loss: 1.5966 - val_accuracy: 0.4180\n",
      "Epoch 140/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5697 - accuracy: 0.4200\n",
      "Epoch 00140: val_loss did not improve from 1.59656\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.5697 - accuracy: 0.4198 - val_loss: 1.6613 - val_accuracy: 0.3940\n",
      "Epoch 141/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5700 - accuracy: 0.4202\n",
      "Epoch 00141: val_loss did not improve from 1.59656\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.5699 - accuracy: 0.4202 - val_loss: 1.6689 - val_accuracy: 0.3906\n",
      "Epoch 142/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5819 - accuracy: 0.4167\n",
      "Epoch 00142: val_loss improved from 1.59656 to 1.55764, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5817 - accuracy: 0.4168 - val_loss: 1.5576 - val_accuracy: 0.4234\n",
      "Epoch 143/182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/351 [============================>.] - ETA: 0s - loss: 1.5715 - accuracy: 0.4197\n",
      "Epoch 00143: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5711 - accuracy: 0.4200 - val_loss: 1.7434 - val_accuracy: 0.3660\n",
      "Epoch 144/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5775 - accuracy: 0.4172\n",
      "Epoch 00144: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5776 - accuracy: 0.4171 - val_loss: 1.5914 - val_accuracy: 0.4152\n",
      "Epoch 145/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5705 - accuracy: 0.4185\n",
      "Epoch 00145: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5708 - accuracy: 0.4184 - val_loss: 1.6352 - val_accuracy: 0.4026\n",
      "Epoch 146/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5698 - accuracy: 0.4205\n",
      "Epoch 00146: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5703 - accuracy: 0.4204 - val_loss: 1.6428 - val_accuracy: 0.4012\n",
      "Epoch 147/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5652 - accuracy: 0.4233\n",
      "Epoch 00147: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5651 - accuracy: 0.4234 - val_loss: 1.6226 - val_accuracy: 0.4082\n",
      "Epoch 148/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5749 - accuracy: 0.4195\n",
      "Epoch 00148: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.5748 - accuracy: 0.4195 - val_loss: 1.5886 - val_accuracy: 0.4148\n",
      "Epoch 149/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5695 - accuracy: 0.4224\n",
      "Epoch 00149: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.5692 - accuracy: 0.4225 - val_loss: 1.5655 - val_accuracy: 0.4238\n",
      "Epoch 150/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5659 - accuracy: 0.4212\n",
      "Epoch 00150: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.5663 - accuracy: 0.4209 - val_loss: 1.5823 - val_accuracy: 0.4108\n",
      "Epoch 151/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5725 - accuracy: 0.4209\n",
      "Epoch 00151: val_loss did not improve from 1.55764\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.5724 - accuracy: 0.4207 - val_loss: 1.6781 - val_accuracy: 0.3914\n",
      "Epoch 152/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5663 - accuracy: 0.4222\n",
      "Epoch 00152: val_loss improved from 1.55764 to 1.54896, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.5664 - accuracy: 0.4222 - val_loss: 1.5490 - val_accuracy: 0.4222\n",
      "Epoch 153/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5688 - accuracy: 0.4190\n",
      "Epoch 00153: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.5690 - accuracy: 0.4189 - val_loss: 1.5641 - val_accuracy: 0.4252\n",
      "Epoch 154/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5668 - accuracy: 0.4243\n",
      "Epoch 00154: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5670 - accuracy: 0.4243 - val_loss: 1.5889 - val_accuracy: 0.4182\n",
      "Epoch 155/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5686 - accuracy: 0.4234\n",
      "Epoch 00155: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5685 - accuracy: 0.4232 - val_loss: 1.5592 - val_accuracy: 0.4232\n",
      "Epoch 156/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5707 - accuracy: 0.4226\n",
      "Epoch 00156: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5708 - accuracy: 0.4225 - val_loss: 1.6005 - val_accuracy: 0.4098\n",
      "Epoch 157/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5681 - accuracy: 0.4215\n",
      "Epoch 00157: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5680 - accuracy: 0.4214 - val_loss: 1.6160 - val_accuracy: 0.4040\n",
      "Epoch 158/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5630 - accuracy: 0.4218\n",
      "Epoch 00158: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5631 - accuracy: 0.4219 - val_loss: 1.5579 - val_accuracy: 0.4214\n",
      "Epoch 159/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5677 - accuracy: 0.4201\n",
      "Epoch 00159: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5676 - accuracy: 0.4200 - val_loss: 1.6501 - val_accuracy: 0.3982\n",
      "Epoch 160/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5614 - accuracy: 0.4240\n",
      "Epoch 00160: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5615 - accuracy: 0.4240 - val_loss: 1.5732 - val_accuracy: 0.4170\n",
      "Epoch 161/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5636 - accuracy: 0.4190\n",
      "Epoch 00161: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5634 - accuracy: 0.4191 - val_loss: 1.6105 - val_accuracy: 0.4024\n",
      "Epoch 162/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5663 - accuracy: 0.4221\n",
      "Epoch 00162: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.5665 - accuracy: 0.4222 - val_loss: 1.5526 - val_accuracy: 0.4266\n",
      "Epoch 163/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5682 - accuracy: 0.4202\n",
      "Epoch 00163: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5682 - accuracy: 0.4201 - val_loss: 1.5992 - val_accuracy: 0.4076\n",
      "Epoch 164/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5625 - accuracy: 0.4237\n",
      "Epoch 00164: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5626 - accuracy: 0.4235 - val_loss: 1.5779 - val_accuracy: 0.4108\n",
      "Epoch 165/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5661 - accuracy: 0.4200\n",
      "Epoch 00165: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5665 - accuracy: 0.4199 - val_loss: 1.5763 - val_accuracy: 0.4248\n",
      "Epoch 166/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5632 - accuracy: 0.4206\n",
      "Epoch 00166: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.5631 - accuracy: 0.4207 - val_loss: 1.5989 - val_accuracy: 0.4124\n",
      "Epoch 167/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5668 - accuracy: 0.4213\n",
      "Epoch 00167: val_loss did not improve from 1.54896\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5668 - accuracy: 0.4212 - val_loss: 1.5768 - val_accuracy: 0.4206\n",
      "Epoch 168/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5638 - accuracy: 0.4237\n",
      "Epoch 00168: val_loss improved from 1.54896 to 1.54826, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5639 - accuracy: 0.4236 - val_loss: 1.5483 - val_accuracy: 0.4268\n",
      "Epoch 169/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5529 - accuracy: 0.4260\n",
      "Epoch 00169: val_loss did not improve from 1.54826\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5531 - accuracy: 0.4261 - val_loss: 1.5880 - val_accuracy: 0.4132\n",
      "Epoch 170/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5682 - accuracy: 0.4221\n",
      "Epoch 00170: val_loss improved from 1.54826 to 1.53750, saving model to rms_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 108s 309ms/step - loss: 1.5679 - accuracy: 0.4223 - val_loss: 1.5375 - val_accuracy: 0.4266\n",
      "Epoch 171/182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/351 [============================>.] - ETA: 0s - loss: 1.5640 - accuracy: 0.4225\n",
      "Epoch 00171: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5639 - accuracy: 0.4225 - val_loss: 1.6131 - val_accuracy: 0.4090\n",
      "Epoch 172/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5700 - accuracy: 0.4171\n",
      "Epoch 00172: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5702 - accuracy: 0.4171 - val_loss: 1.7183 - val_accuracy: 0.3830\n",
      "Epoch 173/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5582 - accuracy: 0.4240\n",
      "Epoch 00173: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5577 - accuracy: 0.4242 - val_loss: 1.5753 - val_accuracy: 0.4204\n",
      "Epoch 174/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5682 - accuracy: 0.4230\n",
      "Epoch 00174: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 308ms/step - loss: 1.5679 - accuracy: 0.4231 - val_loss: 1.5909 - val_accuracy: 0.4136\n",
      "Epoch 175/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5565 - accuracy: 0.4233\n",
      "Epoch 00175: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5566 - accuracy: 0.4233 - val_loss: 1.6979 - val_accuracy: 0.3850\n",
      "Epoch 176/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5630 - accuracy: 0.4221\n",
      "Epoch 00176: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 306ms/step - loss: 1.5630 - accuracy: 0.4221 - val_loss: 1.6255 - val_accuracy: 0.4040\n",
      "Epoch 177/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5629 - accuracy: 0.4237\n",
      "Epoch 00177: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 107s 305ms/step - loss: 1.5628 - accuracy: 0.4237 - val_loss: 1.5486 - val_accuracy: 0.4222\n",
      "Epoch 178/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5585 - accuracy: 0.4237\n",
      "Epoch 00178: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5588 - accuracy: 0.4236 - val_loss: 1.5540 - val_accuracy: 0.4194\n",
      "Epoch 179/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5597 - accuracy: 0.4243\n",
      "Epoch 00179: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5599 - accuracy: 0.4242 - val_loss: 1.5583 - val_accuracy: 0.4248\n",
      "Epoch 180/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5577 - accuracy: 0.4235\n",
      "Epoch 00180: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5578 - accuracy: 0.4234 - val_loss: 1.6055 - val_accuracy: 0.4082\n",
      "Epoch 181/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5597 - accuracy: 0.4249\n",
      "Epoch 00181: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 107s 306ms/step - loss: 1.5596 - accuracy: 0.4248 - val_loss: 1.7603 - val_accuracy: 0.3770\n",
      "Epoch 182/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5650 - accuracy: 0.4221\n",
      "Epoch 00182: val_loss did not improve from 1.53750\n",
      "351/351 [==============================] - 108s 307ms/step - loss: 1.5649 - accuracy: 0.4222 - val_loss: 1.5544 - val_accuracy: 0.4272\n"
     ]
    }
   ],
   "source": [
    "history_rms = model_rms.fit_generator(training_data_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_data_generator,\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model trained using RMSprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rms = model_rms.predict(x_t,batch_size=100)\n",
    "y_pred_rms = np.array([np.argmax(i) for i in y_test_pred_rms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rms = Accuracy()\n",
    "m_rms.update_state(y_true,y_pred_rms)\n",
    "print('Test Accuracy using RMSprop optimizer is : ', m_rms.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule(0), beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [lr_scheduler,tensorboard_callback(),model_check_point('adam')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = resnet20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 3.1296 - accuracy: 0.1665\n",
      "Epoch 00001: val_loss improved from inf to 3.11320, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 3.1264 - accuracy: 0.1667 - val_loss: 3.1132 - val_accuracy: 0.1060\n",
      "Epoch 2/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 2.0132 - accuracy: 0.2550\n",
      "Epoch 00002: val_loss improved from 3.11320 to 2.16883, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 2.0132 - accuracy: 0.2550 - val_loss: 2.1688 - val_accuracy: 0.2074\n",
      "Epoch 3/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.9152 - accuracy: 0.2865\n",
      "Epoch 00003: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.9151 - accuracy: 0.2866 - val_loss: 5.5452 - val_accuracy: 0.1404\n",
      "Epoch 4/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8663 - accuracy: 0.3016\n",
      "Epoch 00004: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8666 - accuracy: 0.3015 - val_loss: 2.2480 - val_accuracy: 0.2122\n",
      "Epoch 5/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8547 - accuracy: 0.3042\n",
      "Epoch 00005: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8547 - accuracy: 0.3044 - val_loss: 2.5030 - val_accuracy: 0.1724\n",
      "Epoch 6/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8362 - accuracy: 0.3132\n",
      "Epoch 00006: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8363 - accuracy: 0.3133 - val_loss: 2.9767 - val_accuracy: 0.1542\n",
      "Epoch 7/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8310 - accuracy: 0.3166\n",
      "Epoch 00007: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8312 - accuracy: 0.3164 - val_loss: 2.4849 - val_accuracy: 0.1624\n",
      "Epoch 8/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8369 - accuracy: 0.3156\n",
      "Epoch 00008: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8368 - accuracy: 0.3155 - val_loss: 5.5182 - val_accuracy: 0.1032\n",
      "Epoch 9/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8184 - accuracy: 0.3203\n",
      "Epoch 00009: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8184 - accuracy: 0.3204 - val_loss: 2.8726 - val_accuracy: 0.1546\n",
      "Epoch 10/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8231 - accuracy: 0.3170\n",
      "Epoch 00010: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8227 - accuracy: 0.3171 - val_loss: 2.4340 - val_accuracy: 0.2264\n",
      "Epoch 11/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8335 - accuracy: 0.3167\n",
      "Epoch 00011: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8333 - accuracy: 0.3169 - val_loss: 4.3026 - val_accuracy: 0.1506\n",
      "Epoch 12/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8187 - accuracy: 0.3196\n",
      "Epoch 00012: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8191 - accuracy: 0.3194 - val_loss: 3.4565 - val_accuracy: 0.1184\n",
      "Epoch 13/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8219 - accuracy: 0.3171\n",
      "Epoch 00013: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8220 - accuracy: 0.3173 - val_loss: 7.6178 - val_accuracy: 0.0982\n",
      "Epoch 14/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8222 - accuracy: 0.3204\n",
      "Epoch 00014: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8222 - accuracy: 0.3205 - val_loss: 3.8568 - val_accuracy: 0.1008\n",
      "Epoch 15/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8313 - accuracy: 0.3174\n",
      "Epoch 00015: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8314 - accuracy: 0.3174 - val_loss: 6.7078 - val_accuracy: 0.1184\n",
      "Epoch 16/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8355 - accuracy: 0.3149\n",
      "Epoch 00016: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8359 - accuracy: 0.3148 - val_loss: 3.3764 - val_accuracy: 0.1618\n",
      "Epoch 17/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8204 - accuracy: 0.3200\n",
      "Epoch 00017: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8205 - accuracy: 0.3200 - val_loss: 5.8937 - val_accuracy: 0.1222\n",
      "Epoch 18/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8232 - accuracy: 0.3168\n",
      "Epoch 00018: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8235 - accuracy: 0.3167 - val_loss: 6.8167 - val_accuracy: 0.0970\n",
      "Epoch 19/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8256 - accuracy: 0.3196\n",
      "Epoch 00019: val_loss did not improve from 2.16883\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8255 - accuracy: 0.3196 - val_loss: 3.4553 - val_accuracy: 0.1376\n",
      "Epoch 20/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8272 - accuracy: 0.3181\n",
      "Epoch 00020: val_loss improved from 2.16883 to 2.07807, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 81s 230ms/step - loss: 1.8268 - accuracy: 0.3184 - val_loss: 2.0781 - val_accuracy: 0.2352\n",
      "Epoch 21/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8284 - accuracy: 0.3175\n",
      "Epoch 00021: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8281 - accuracy: 0.3175 - val_loss: 2.6127 - val_accuracy: 0.1620\n",
      "Epoch 22/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8139 - accuracy: 0.3216\n",
      "Epoch 00022: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8140 - accuracy: 0.3216 - val_loss: 9.7480 - val_accuracy: 0.1198\n",
      "Epoch 23/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8095 - accuracy: 0.3246\n",
      "Epoch 00023: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8094 - accuracy: 0.3248 - val_loss: 3.0848 - val_accuracy: 0.1222\n",
      "Epoch 24/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8307 - accuracy: 0.3211\n",
      "Epoch 00024: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8306 - accuracy: 0.3212 - val_loss: 3.0823 - val_accuracy: 0.1508\n",
      "Epoch 25/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8089 - accuracy: 0.3251\n",
      "Epoch 00025: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8086 - accuracy: 0.3253 - val_loss: 2.1493 - val_accuracy: 0.2214\n",
      "Epoch 26/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8153 - accuracy: 0.3229\n",
      "Epoch 00026: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8154 - accuracy: 0.3229 - val_loss: 3.1013 - val_accuracy: 0.1506\n",
      "Epoch 27/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8153 - accuracy: 0.3209\n",
      "Epoch 00027: val_loss did not improve from 2.07807\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8155 - accuracy: 0.3209 - val_loss: 2.5552 - val_accuracy: 0.1890\n",
      "Epoch 28/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8058 - accuracy: 0.3221\n",
      "Epoch 00028: val_loss improved from 2.07807 to 2.05498, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8060 - accuracy: 0.3222 - val_loss: 2.0550 - val_accuracy: 0.2322\n",
      "Epoch 29/182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/351 [============================>.] - ETA: 0s - loss: 1.8112 - accuracy: 0.3224\n",
      "Epoch 00029: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8111 - accuracy: 0.3225 - val_loss: 2.4219 - val_accuracy: 0.1768\n",
      "Epoch 30/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8128 - accuracy: 0.3187\n",
      "Epoch 00030: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8129 - accuracy: 0.3188 - val_loss: 3.3992 - val_accuracy: 0.1696\n",
      "Epoch 31/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8055 - accuracy: 0.3229\n",
      "Epoch 00031: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8054 - accuracy: 0.3231 - val_loss: 2.5410 - val_accuracy: 0.1706\n",
      "Epoch 32/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8074 - accuracy: 0.3252\n",
      "Epoch 00032: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8076 - accuracy: 0.3251 - val_loss: 2.0975 - val_accuracy: 0.2330\n",
      "Epoch 33/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8049 - accuracy: 0.3232\n",
      "Epoch 00033: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8050 - accuracy: 0.3231 - val_loss: 3.2305 - val_accuracy: 0.1542\n",
      "Epoch 34/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8010 - accuracy: 0.3292\n",
      "Epoch 00034: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 81s 229ms/step - loss: 1.8006 - accuracy: 0.3293 - val_loss: 2.5472 - val_accuracy: 0.2124\n",
      "Epoch 35/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8113 - accuracy: 0.3174\n",
      "Epoch 00035: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8113 - accuracy: 0.3174 - val_loss: 2.4447 - val_accuracy: 0.1884\n",
      "Epoch 36/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8135 - accuracy: 0.3241\n",
      "Epoch 00036: val_loss did not improve from 2.05498\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8136 - accuracy: 0.3241 - val_loss: 4.2618 - val_accuracy: 0.1140\n",
      "Epoch 37/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8179 - accuracy: 0.3194\n",
      "Epoch 00037: val_loss improved from 2.05498 to 2.00175, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8182 - accuracy: 0.3193 - val_loss: 2.0017 - val_accuracy: 0.2624\n",
      "Epoch 38/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8268 - accuracy: 0.3180\n",
      "Epoch 00038: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8268 - accuracy: 0.3180 - val_loss: 2.1942 - val_accuracy: 0.2134\n",
      "Epoch 39/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8227 - accuracy: 0.3190\n",
      "Epoch 00039: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8225 - accuracy: 0.3190 - val_loss: 2.1336 - val_accuracy: 0.2244\n",
      "Epoch 40/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8025 - accuracy: 0.3253\n",
      "Epoch 00040: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8024 - accuracy: 0.3253 - val_loss: 3.4177 - val_accuracy: 0.1408\n",
      "Epoch 41/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8090 - accuracy: 0.3227\n",
      "Epoch 00041: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8088 - accuracy: 0.3229 - val_loss: 2.3458 - val_accuracy: 0.1750\n",
      "Epoch 42/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8062 - accuracy: 0.3230\n",
      "Epoch 00042: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8064 - accuracy: 0.3229 - val_loss: 2.4439 - val_accuracy: 0.1648\n",
      "Epoch 43/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8174 - accuracy: 0.3196\n",
      "Epoch 00043: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8173 - accuracy: 0.3196 - val_loss: 2.2541 - val_accuracy: 0.1986\n",
      "Epoch 44/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8069 - accuracy: 0.3239\n",
      "Epoch 00044: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8073 - accuracy: 0.3237 - val_loss: 2.2857 - val_accuracy: 0.2200\n",
      "Epoch 45/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8115 - accuracy: 0.3219\n",
      "Epoch 00045: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8114 - accuracy: 0.3218 - val_loss: 5.4834 - val_accuracy: 0.1582\n",
      "Epoch 46/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8080 - accuracy: 0.3258\n",
      "Epoch 00046: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8079 - accuracy: 0.3256 - val_loss: 2.4615 - val_accuracy: 0.1848\n",
      "Epoch 47/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8201 - accuracy: 0.3205\n",
      "Epoch 00047: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8199 - accuracy: 0.3204 - val_loss: 6.2904 - val_accuracy: 0.0858\n",
      "Epoch 48/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8030 - accuracy: 0.3267\n",
      "Epoch 00048: val_loss did not improve from 2.00175\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8030 - accuracy: 0.3268 - val_loss: 2.4900 - val_accuracy: 0.1948\n",
      "Epoch 49/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8049 - accuracy: 0.3262\n",
      "Epoch 00049: val_loss improved from 2.00175 to 1.84216, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 81s 230ms/step - loss: 1.8049 - accuracy: 0.3263 - val_loss: 1.8422 - val_accuracy: 0.3124\n",
      "Epoch 50/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7996 - accuracy: 0.3276\n",
      "Epoch 00050: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.7993 - accuracy: 0.3277 - val_loss: 11.0216 - val_accuracy: 0.1014\n",
      "Epoch 51/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8094 - accuracy: 0.3208\n",
      "Epoch 00051: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8093 - accuracy: 0.3209 - val_loss: 1.8629 - val_accuracy: 0.2850\n",
      "Epoch 52/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8010 - accuracy: 0.3239\n",
      "Epoch 00052: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8009 - accuracy: 0.3239 - val_loss: 1.9233 - val_accuracy: 0.3122\n",
      "Epoch 53/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7998 - accuracy: 0.3245\n",
      "Epoch 00053: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.7998 - accuracy: 0.3246 - val_loss: 1.8567 - val_accuracy: 0.3156\n",
      "Epoch 54/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8013 - accuracy: 0.3263\n",
      "Epoch 00054: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8012 - accuracy: 0.3263 - val_loss: 4.9780 - val_accuracy: 0.1712\n",
      "Epoch 55/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8010 - accuracy: 0.3256\n",
      "Epoch 00055: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8009 - accuracy: 0.3257 - val_loss: 3.3166 - val_accuracy: 0.1560\n",
      "Epoch 56/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8127 - accuracy: 0.3183\n",
      "Epoch 00056: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8126 - accuracy: 0.3183 - val_loss: 2.0459 - val_accuracy: 0.2552\n",
      "Epoch 57/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7962 - accuracy: 0.3242\n",
      "Epoch 00057: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.7964 - accuracy: 0.3241 - val_loss: 2.6507 - val_accuracy: 0.1452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8100 - accuracy: 0.3189\n",
      "Epoch 00058: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 229ms/step - loss: 1.8102 - accuracy: 0.3189 - val_loss: 3.3147 - val_accuracy: 0.1394\n",
      "Epoch 59/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7898 - accuracy: 0.3294\n",
      "Epoch 00059: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 229ms/step - loss: 1.7898 - accuracy: 0.3293 - val_loss: 2.0869 - val_accuracy: 0.2418\n",
      "Epoch 60/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8123 - accuracy: 0.3191\n",
      "Epoch 00060: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8120 - accuracy: 0.3190 - val_loss: 6.0248 - val_accuracy: 0.1070\n",
      "Epoch 61/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7992 - accuracy: 0.3235\n",
      "Epoch 00061: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.7991 - accuracy: 0.3235 - val_loss: 2.1410 - val_accuracy: 0.2270\n",
      "Epoch 62/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7967 - accuracy: 0.3300\n",
      "Epoch 00062: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.7966 - accuracy: 0.3299 - val_loss: 2.1207 - val_accuracy: 0.2640\n",
      "Epoch 63/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8036 - accuracy: 0.3235\n",
      "Epoch 00063: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8036 - accuracy: 0.3234 - val_loss: 2.7225 - val_accuracy: 0.1672\n",
      "Epoch 64/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8047 - accuracy: 0.3230\n",
      "Epoch 00064: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8044 - accuracy: 0.3229 - val_loss: 3.5537 - val_accuracy: 0.1110\n",
      "Epoch 65/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8061 - accuracy: 0.3221\n",
      "Epoch 00065: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8061 - accuracy: 0.3221 - val_loss: 2.0784 - val_accuracy: 0.2578\n",
      "Epoch 66/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8058 - accuracy: 0.3229\n",
      "Epoch 00066: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8055 - accuracy: 0.3230 - val_loss: 3.1397 - val_accuracy: 0.1132\n",
      "Epoch 67/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8150 - accuracy: 0.3255\n",
      "Epoch 00067: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8150 - accuracy: 0.3255 - val_loss: 2.0998 - val_accuracy: 0.2262\n",
      "Epoch 68/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8025 - accuracy: 0.3282\n",
      "Epoch 00068: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8026 - accuracy: 0.3280 - val_loss: 2.1579 - val_accuracy: 0.2470\n",
      "Epoch 69/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8077 - accuracy: 0.3234\n",
      "Epoch 00069: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8072 - accuracy: 0.3236 - val_loss: 2.9472 - val_accuracy: 0.1724\n",
      "Epoch 70/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7993 - accuracy: 0.3233\n",
      "Epoch 00070: val_loss did not improve from 1.84216\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.7991 - accuracy: 0.3234 - val_loss: 3.7988 - val_accuracy: 0.1100\n",
      "Epoch 71/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8151 - accuracy: 0.3219\n",
      "Epoch 00071: val_loss improved from 1.84216 to 1.76957, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 81s 230ms/step - loss: 1.8153 - accuracy: 0.3218 - val_loss: 1.7696 - val_accuracy: 0.3350\n",
      "Epoch 72/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7997 - accuracy: 0.3251\n",
      "Epoch 00072: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8001 - accuracy: 0.3249 - val_loss: 2.7430 - val_accuracy: 0.1688\n",
      "Epoch 73/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7966 - accuracy: 0.3255\n",
      "Epoch 00073: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 81s 230ms/step - loss: 1.7969 - accuracy: 0.3255 - val_loss: 1.9983 - val_accuracy: 0.2694\n",
      "Epoch 74/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7961 - accuracy: 0.3273\n",
      "Epoch 00074: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.7960 - accuracy: 0.3275 - val_loss: 1.9028 - val_accuracy: 0.2884\n",
      "Epoch 75/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8106 - accuracy: 0.3240\n",
      "Epoch 00075: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8102 - accuracy: 0.3242 - val_loss: 2.1326 - val_accuracy: 0.2360\n",
      "Epoch 76/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8040 - accuracy: 0.3210\n",
      "Epoch 00076: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8039 - accuracy: 0.3211 - val_loss: 2.5298 - val_accuracy: 0.2146\n",
      "Epoch 77/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8189 - accuracy: 0.3183\n",
      "Epoch 00077: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8191 - accuracy: 0.3182 - val_loss: 2.1007 - val_accuracy: 0.2292\n",
      "Epoch 78/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8044 - accuracy: 0.3228\n",
      "Epoch 00078: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8042 - accuracy: 0.3228 - val_loss: 2.8546 - val_accuracy: 0.1924\n",
      "Epoch 79/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8043 - accuracy: 0.3237\n",
      "Epoch 00079: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8045 - accuracy: 0.3236 - val_loss: 2.7211 - val_accuracy: 0.1954\n",
      "Epoch 80/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7948 - accuracy: 0.3274\n",
      "Epoch 00080: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.7948 - accuracy: 0.3274 - val_loss: 2.4726 - val_accuracy: 0.1780\n",
      "Epoch 81/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8011 - accuracy: 0.3273\n",
      "Epoch 00081: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8008 - accuracy: 0.3275 - val_loss: 4.2525 - val_accuracy: 0.1260\n",
      "Epoch 82/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8024 - accuracy: 0.3281\n",
      "Epoch 00082: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8021 - accuracy: 0.3282 - val_loss: 6.8677 - val_accuracy: 0.0946\n",
      "Epoch 83/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7970 - accuracy: 0.3265\n",
      "Epoch 00083: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.7965 - accuracy: 0.3266 - val_loss: 2.3834 - val_accuracy: 0.2038\n",
      "Epoch 84/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8063 - accuracy: 0.3235\n",
      "Epoch 00084: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8063 - accuracy: 0.3236 - val_loss: 4.1166 - val_accuracy: 0.0884\n",
      "Epoch 85/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8095 - accuracy: 0.3204\n",
      "Epoch 00085: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8094 - accuracy: 0.3203 - val_loss: 2.5972 - val_accuracy: 0.2030\n",
      "Epoch 86/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8048 - accuracy: 0.3256\n",
      "Epoch 00086: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.8049 - accuracy: 0.3256 - val_loss: 3.3482 - val_accuracy: 0.1134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8013 - accuracy: 0.3248\n",
      "Epoch 00087: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8012 - accuracy: 0.3247 - val_loss: 2.2845 - val_accuracy: 0.1962\n",
      "Epoch 88/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8072 - accuracy: 0.3277\n",
      "Epoch 00088: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.8069 - accuracy: 0.3279 - val_loss: 3.3892 - val_accuracy: 0.1698\n",
      "Epoch 89/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8163 - accuracy: 0.3261\n",
      "Epoch 00089: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8166 - accuracy: 0.3260 - val_loss: 2.0289 - val_accuracy: 0.2686\n",
      "Epoch 90/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8095 - accuracy: 0.3210\n",
      "Epoch 00090: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8092 - accuracy: 0.3212 - val_loss: 10.7355 - val_accuracy: 0.1002\n",
      "Epoch 91/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8080 - accuracy: 0.3264\n",
      "Epoch 00091: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.8085 - accuracy: 0.3263 - val_loss: 4.7232 - val_accuracy: 0.1742\n",
      "Epoch 92/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.8057 - accuracy: 0.3263\n",
      "Epoch 00092: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.8062 - accuracy: 0.3262 - val_loss: 5.4595 - val_accuracy: 0.1318\n",
      "Epoch 93/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7353 - accuracy: 0.3508\n",
      "Epoch 00093: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.7353 - accuracy: 0.3508 - val_loss: 2.1599 - val_accuracy: 0.2316\n",
      "Epoch 94/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6998 - accuracy: 0.3594\n",
      "Epoch 00094: val_loss did not improve from 1.76957\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6998 - accuracy: 0.3594 - val_loss: 1.9297 - val_accuracy: 0.2846\n",
      "Epoch 95/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.7003 - accuracy: 0.3563\n",
      "Epoch 00095: val_loss improved from 1.76957 to 1.74973, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 229ms/step - loss: 1.7001 - accuracy: 0.3564 - val_loss: 1.7497 - val_accuracy: 0.3346\n",
      "Epoch 96/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6818 - accuracy: 0.3636\n",
      "Epoch 00096: val_loss improved from 1.74973 to 1.74635, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.6817 - accuracy: 0.3636 - val_loss: 1.7464 - val_accuracy: 0.3330\n",
      "Epoch 97/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6841 - accuracy: 0.3637\n",
      "Epoch 00097: val_loss did not improve from 1.74635\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6842 - accuracy: 0.3637 - val_loss: 2.2837 - val_accuracy: 0.2044\n",
      "Epoch 98/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6765 - accuracy: 0.3660\n",
      "Epoch 00098: val_loss did not improve from 1.74635\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6764 - accuracy: 0.3661 - val_loss: 1.7718 - val_accuracy: 0.3250\n",
      "Epoch 99/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6704 - accuracy: 0.3656\n",
      "Epoch 00099: val_loss improved from 1.74635 to 1.72030, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 81s 229ms/step - loss: 1.6706 - accuracy: 0.3656 - val_loss: 1.7203 - val_accuracy: 0.3542\n",
      "Epoch 100/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6674 - accuracy: 0.3648\n",
      "Epoch 00100: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6675 - accuracy: 0.3649 - val_loss: 2.0945 - val_accuracy: 0.2398\n",
      "Epoch 101/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6711 - accuracy: 0.3669\n",
      "Epoch 00101: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6713 - accuracy: 0.3669 - val_loss: 1.7890 - val_accuracy: 0.3412\n",
      "Epoch 102/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6629 - accuracy: 0.3667\n",
      "Epoch 00102: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6632 - accuracy: 0.3665 - val_loss: 2.0305 - val_accuracy: 0.2900\n",
      "Epoch 103/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6575 - accuracy: 0.3718\n",
      "Epoch 00103: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6574 - accuracy: 0.3720 - val_loss: 2.3553 - val_accuracy: 0.2478\n",
      "Epoch 104/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6574 - accuracy: 0.3696\n",
      "Epoch 00104: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6575 - accuracy: 0.3695 - val_loss: 1.7960 - val_accuracy: 0.3060\n",
      "Epoch 105/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6614 - accuracy: 0.3715\n",
      "Epoch 00105: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6617 - accuracy: 0.3713 - val_loss: 1.8604 - val_accuracy: 0.2998\n",
      "Epoch 106/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6625 - accuracy: 0.3671\n",
      "Epoch 00106: val_loss did not improve from 1.72030\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6626 - accuracy: 0.3670 - val_loss: 2.6706 - val_accuracy: 0.2438\n",
      "Epoch 107/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6578 - accuracy: 0.3719\n",
      "Epoch 00107: val_loss improved from 1.72030 to 1.67917, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.6579 - accuracy: 0.3720 - val_loss: 1.6792 - val_accuracy: 0.3574\n",
      "Epoch 108/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6486 - accuracy: 0.3753\n",
      "Epoch 00108: val_loss did not improve from 1.67917\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6488 - accuracy: 0.3753 - val_loss: 1.7307 - val_accuracy: 0.3364\n",
      "Epoch 109/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6536 - accuracy: 0.3715\n",
      "Epoch 00109: val_loss did not improve from 1.67917\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6541 - accuracy: 0.3714 - val_loss: 1.6824 - val_accuracy: 0.3634\n",
      "Epoch 110/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6550 - accuracy: 0.3730\n",
      "Epoch 00110: val_loss did not improve from 1.67917\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6549 - accuracy: 0.3729 - val_loss: 2.3589 - val_accuracy: 0.2500\n",
      "Epoch 111/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6505 - accuracy: 0.3727\n",
      "Epoch 00111: val_loss did not improve from 1.67917\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6504 - accuracy: 0.3729 - val_loss: 1.9400 - val_accuracy: 0.3204\n",
      "Epoch 112/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6488 - accuracy: 0.3757\n",
      "Epoch 00112: val_loss did not improve from 1.67917\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6492 - accuracy: 0.3754 - val_loss: 1.8373 - val_accuracy: 0.3040\n",
      "Epoch 113/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6463 - accuracy: 0.3747\n",
      "Epoch 00113: val_loss improved from 1.67917 to 1.61912, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 229ms/step - loss: 1.6465 - accuracy: 0.3746 - val_loss: 1.6191 - val_accuracy: 0.3764\n",
      "Epoch 114/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6527 - accuracy: 0.3722\n",
      "Epoch 00114: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6528 - accuracy: 0.3722 - val_loss: 1.7072 - val_accuracy: 0.3520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6435 - accuracy: 0.3741\n",
      "Epoch 00115: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.6438 - accuracy: 0.3740 - val_loss: 2.1591 - val_accuracy: 0.2786\n",
      "Epoch 116/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6465 - accuracy: 0.3755\n",
      "Epoch 00116: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6463 - accuracy: 0.3754 - val_loss: 1.8061 - val_accuracy: 0.3310\n",
      "Epoch 117/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6423 - accuracy: 0.3763\n",
      "Epoch 00117: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6423 - accuracy: 0.3762 - val_loss: 3.7495 - val_accuracy: 0.1708\n",
      "Epoch 118/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6475 - accuracy: 0.3739\n",
      "Epoch 00118: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6472 - accuracy: 0.3739 - val_loss: 2.4699 - val_accuracy: 0.2060\n",
      "Epoch 119/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6379 - accuracy: 0.3754\n",
      "Epoch 00119: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6376 - accuracy: 0.3757 - val_loss: 1.6570 - val_accuracy: 0.3684\n",
      "Epoch 120/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6474 - accuracy: 0.3738\n",
      "Epoch 00120: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6473 - accuracy: 0.3739 - val_loss: 2.3472 - val_accuracy: 0.2610\n",
      "Epoch 121/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6434 - accuracy: 0.3760\n",
      "Epoch 00121: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6436 - accuracy: 0.3760 - val_loss: 1.7333 - val_accuracy: 0.3470\n",
      "Epoch 122/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6447 - accuracy: 0.3780\n",
      "Epoch 00122: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6446 - accuracy: 0.3782 - val_loss: 1.8289 - val_accuracy: 0.3204\n",
      "Epoch 123/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6464 - accuracy: 0.3752\n",
      "Epoch 00123: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6462 - accuracy: 0.3755 - val_loss: 1.8401 - val_accuracy: 0.3146\n",
      "Epoch 124/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6364 - accuracy: 0.3777\n",
      "Epoch 00124: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6363 - accuracy: 0.3775 - val_loss: 1.7941 - val_accuracy: 0.3374\n",
      "Epoch 125/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6345 - accuracy: 0.3798\n",
      "Epoch 00125: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6342 - accuracy: 0.3801 - val_loss: 1.7000 - val_accuracy: 0.3664\n",
      "Epoch 126/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6405 - accuracy: 0.3768\n",
      "Epoch 00126: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6406 - accuracy: 0.3766 - val_loss: 2.8710 - val_accuracy: 0.2066\n",
      "Epoch 127/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6349 - accuracy: 0.3847\n",
      "Epoch 00127: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6350 - accuracy: 0.3847 - val_loss: 1.6990 - val_accuracy: 0.3632\n",
      "Epoch 128/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6379 - accuracy: 0.3805\n",
      "Epoch 00128: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6376 - accuracy: 0.3807 - val_loss: 2.0508 - val_accuracy: 0.2662\n",
      "Epoch 129/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6353 - accuracy: 0.3802\n",
      "Epoch 00129: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6353 - accuracy: 0.3804 - val_loss: 1.7137 - val_accuracy: 0.3398\n",
      "Epoch 130/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6366 - accuracy: 0.3779\n",
      "Epoch 00130: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6366 - accuracy: 0.3778 - val_loss: 1.6755 - val_accuracy: 0.3844\n",
      "Epoch 131/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6364 - accuracy: 0.3785\n",
      "Epoch 00131: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6361 - accuracy: 0.3786 - val_loss: 1.9945 - val_accuracy: 0.2888\n",
      "Epoch 132/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6336 - accuracy: 0.3800\n",
      "Epoch 00132: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6339 - accuracy: 0.3800 - val_loss: 1.9186 - val_accuracy: 0.2880\n",
      "Epoch 133/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6393 - accuracy: 0.3758\n",
      "Epoch 00133: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6391 - accuracy: 0.3757 - val_loss: 2.1386 - val_accuracy: 0.2974\n",
      "Epoch 134/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6360 - accuracy: 0.3795\n",
      "Epoch 00134: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6362 - accuracy: 0.3794 - val_loss: 1.8153 - val_accuracy: 0.3628\n",
      "Epoch 135/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6359 - accuracy: 0.3803\n",
      "Epoch 00135: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6356 - accuracy: 0.3802 - val_loss: 3.0034 - val_accuracy: 0.1956\n",
      "Epoch 136/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6302 - accuracy: 0.3822\n",
      "Epoch 00136: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6304 - accuracy: 0.3821 - val_loss: 1.8853 - val_accuracy: 0.3050\n",
      "Epoch 137/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6326 - accuracy: 0.3841\n",
      "Epoch 00137: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6329 - accuracy: 0.3840 - val_loss: 2.4936 - val_accuracy: 0.2100\n",
      "Epoch 138/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6152 - accuracy: 0.3903\n",
      "Epoch 00138: val_loss did not improve from 1.61912\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6153 - accuracy: 0.3901 - val_loss: 1.6242 - val_accuracy: 0.4012\n",
      "Epoch 139/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6120 - accuracy: 0.3899\n",
      "Epoch 00139: val_loss improved from 1.61912 to 1.61015, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 229ms/step - loss: 1.6120 - accuracy: 0.3901 - val_loss: 1.6101 - val_accuracy: 0.3946\n",
      "Epoch 140/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6098 - accuracy: 0.3944\n",
      "Epoch 00140: val_loss did not improve from 1.61015\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6096 - accuracy: 0.3945 - val_loss: 1.6289 - val_accuracy: 0.3836\n",
      "Epoch 141/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6041 - accuracy: 0.3938\n",
      "Epoch 00141: val_loss did not improve from 1.61015\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6039 - accuracy: 0.3939 - val_loss: 1.6114 - val_accuracy: 0.4026\n",
      "Epoch 142/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6074 - accuracy: 0.3949\n",
      "Epoch 00142: val_loss did not improve from 1.61015\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6072 - accuracy: 0.3951 - val_loss: 1.6458 - val_accuracy: 0.3706\n",
      "Epoch 143/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6058 - accuracy: 0.3948\n",
      "Epoch 00143: val_loss improved from 1.61015 to 1.60774, saving model to adam_model_checkpoints\\weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 81s 229ms/step - loss: 1.6056 - accuracy: 0.3947 - val_loss: 1.6077 - val_accuracy: 0.3988\n",
      "Epoch 144/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6041 - accuracy: 0.3966\n",
      "Epoch 00144: val_loss did not improve from 1.60774\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6038 - accuracy: 0.3966 - val_loss: 1.6726 - val_accuracy: 0.3846\n",
      "Epoch 145/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6046 - accuracy: 0.3943\n",
      "Epoch 00145: val_loss improved from 1.60774 to 1.60012, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.6050 - accuracy: 0.3943 - val_loss: 1.6001 - val_accuracy: 0.4038\n",
      "Epoch 146/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6035 - accuracy: 0.3958\n",
      "Epoch 00146: val_loss did not improve from 1.60012\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6033 - accuracy: 0.3959 - val_loss: 1.6504 - val_accuracy: 0.3960\n",
      "Epoch 147/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6054 - accuracy: 0.3938\n",
      "Epoch 00147: val_loss improved from 1.60012 to 1.58680, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6056 - accuracy: 0.3938 - val_loss: 1.5868 - val_accuracy: 0.4024\n",
      "Epoch 148/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6047 - accuracy: 0.3949\n",
      "Epoch 00148: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6050 - accuracy: 0.3947 - val_loss: 1.6497 - val_accuracy: 0.3914\n",
      "Epoch 149/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5964 - accuracy: 0.3969\n",
      "Epoch 00149: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.5962 - accuracy: 0.3970 - val_loss: 1.6383 - val_accuracy: 0.3834\n",
      "Epoch 150/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6115 - accuracy: 0.3921\n",
      "Epoch 00150: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6115 - accuracy: 0.3921 - val_loss: 1.5897 - val_accuracy: 0.4026\n",
      "Epoch 151/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6025 - accuracy: 0.3923\n",
      "Epoch 00151: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6027 - accuracy: 0.3922 - val_loss: 1.6322 - val_accuracy: 0.3750\n",
      "Epoch 152/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6017 - accuracy: 0.3980\n",
      "Epoch 00152: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6015 - accuracy: 0.3980 - val_loss: 1.5955 - val_accuracy: 0.4026\n",
      "Epoch 153/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6022 - accuracy: 0.3952\n",
      "Epoch 00153: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6020 - accuracy: 0.3954 - val_loss: 1.6139 - val_accuracy: 0.3960\n",
      "Epoch 154/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6066 - accuracy: 0.3947\n",
      "Epoch 00154: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6066 - accuracy: 0.3948 - val_loss: 1.6061 - val_accuracy: 0.4000\n",
      "Epoch 155/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5959 - accuracy: 0.3984\n",
      "Epoch 00155: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.5959 - accuracy: 0.3984 - val_loss: 1.6190 - val_accuracy: 0.3904\n",
      "Epoch 156/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6000 - accuracy: 0.3944\n",
      "Epoch 00156: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6001 - accuracy: 0.3942 - val_loss: 1.6203 - val_accuracy: 0.3798\n",
      "Epoch 157/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5998 - accuracy: 0.3926\n",
      "Epoch 00157: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.5998 - accuracy: 0.3926 - val_loss: 1.6322 - val_accuracy: 0.4020\n",
      "Epoch 158/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6020 - accuracy: 0.3968\n",
      "Epoch 00158: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6015 - accuracy: 0.3970 - val_loss: 1.6050 - val_accuracy: 0.3868\n",
      "Epoch 159/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5958 - accuracy: 0.3997\n",
      "Epoch 00159: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5955 - accuracy: 0.3998 - val_loss: 1.6446 - val_accuracy: 0.3964\n",
      "Epoch 160/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5993 - accuracy: 0.3940\n",
      "Epoch 00160: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.5992 - accuracy: 0.3941 - val_loss: 1.5930 - val_accuracy: 0.4028\n",
      "Epoch 161/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6009 - accuracy: 0.3978\n",
      "Epoch 00161: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.6009 - accuracy: 0.3977 - val_loss: 1.6352 - val_accuracy: 0.3808\n",
      "Epoch 162/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6085 - accuracy: 0.3924\n",
      "Epoch 00162: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6084 - accuracy: 0.3923 - val_loss: 1.6731 - val_accuracy: 0.3676\n",
      "Epoch 163/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5915 - accuracy: 0.3990\n",
      "Epoch 00163: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.5916 - accuracy: 0.3987 - val_loss: 1.6363 - val_accuracy: 0.3952\n",
      "Epoch 164/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6027 - accuracy: 0.3959\n",
      "Epoch 00164: val_loss did not improve from 1.58680\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6027 - accuracy: 0.3958 - val_loss: 1.6530 - val_accuracy: 0.3638\n",
      "Epoch 165/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5954 - accuracy: 0.3975\n",
      "Epoch 00165: val_loss improved from 1.58680 to 1.58613, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.5952 - accuracy: 0.3976 - val_loss: 1.5861 - val_accuracy: 0.4030\n",
      "Epoch 166/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5945 - accuracy: 0.3956\n",
      "Epoch 00166: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5945 - accuracy: 0.3955 - val_loss: 1.6316 - val_accuracy: 0.3772\n",
      "Epoch 167/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6013 - accuracy: 0.3979\n",
      "Epoch 00167: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6014 - accuracy: 0.3976 - val_loss: 1.6205 - val_accuracy: 0.3990\n",
      "Epoch 168/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5963 - accuracy: 0.3961\n",
      "Epoch 00168: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.5960 - accuracy: 0.3963 - val_loss: 1.6243 - val_accuracy: 0.3858\n",
      "Epoch 169/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5954 - accuracy: 0.3985\n",
      "Epoch 00169: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.5954 - accuracy: 0.3984 - val_loss: 1.6808 - val_accuracy: 0.3798\n",
      "Epoch 170/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5966 - accuracy: 0.3984\n",
      "Epoch 00170: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.5966 - accuracy: 0.3984 - val_loss: 1.6108 - val_accuracy: 0.3880\n",
      "Epoch 171/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5987 - accuracy: 0.3967\n",
      "Epoch 00171: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.5985 - accuracy: 0.3968 - val_loss: 1.6015 - val_accuracy: 0.4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5950 - accuracy: 0.3953\n",
      "Epoch 00172: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 80s 228ms/step - loss: 1.5949 - accuracy: 0.3953 - val_loss: 1.6153 - val_accuracy: 0.4006\n",
      "Epoch 173/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5977 - accuracy: 0.3971\n",
      "Epoch 00173: val_loss did not improve from 1.58613\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5979 - accuracy: 0.3972 - val_loss: 1.6030 - val_accuracy: 0.4062\n",
      "Epoch 174/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5918 - accuracy: 0.3979\n",
      "Epoch 00174: val_loss improved from 1.58613 to 1.58561, saving model to adam_model_checkpoints\\weights.hdf5\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.5918 - accuracy: 0.3979 - val_loss: 1.5856 - val_accuracy: 0.4090\n",
      "Epoch 175/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5996 - accuracy: 0.3991\n",
      "Epoch 00175: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.5995 - accuracy: 0.3992 - val_loss: 1.6666 - val_accuracy: 0.3822\n",
      "Epoch 176/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5874 - accuracy: 0.4006\n",
      "Epoch 00176: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5876 - accuracy: 0.4005 - val_loss: 1.6026 - val_accuracy: 0.3976\n",
      "Epoch 177/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6003 - accuracy: 0.3937\n",
      "Epoch 00177: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.6006 - accuracy: 0.3936 - val_loss: 1.6088 - val_accuracy: 0.3996\n",
      "Epoch 178/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5977 - accuracy: 0.3947\n",
      "Epoch 00178: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 225ms/step - loss: 1.5974 - accuracy: 0.3949 - val_loss: 1.6322 - val_accuracy: 0.3898\n",
      "Epoch 179/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5935 - accuracy: 0.3980\n",
      "Epoch 00179: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5934 - accuracy: 0.3981 - val_loss: 1.5999 - val_accuracy: 0.3924\n",
      "Epoch 180/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5968 - accuracy: 0.3984\n",
      "Epoch 00180: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5968 - accuracy: 0.3984 - val_loss: 1.6281 - val_accuracy: 0.3946\n",
      "Epoch 181/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.5956 - accuracy: 0.3970\n",
      "Epoch 00181: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 79s 226ms/step - loss: 1.5954 - accuracy: 0.3970 - val_loss: 1.6149 - val_accuracy: 0.3980\n",
      "Epoch 182/182\n",
      "350/351 [============================>.] - ETA: 0s - loss: 1.6027 - accuracy: 0.3924\n",
      "Epoch 00182: val_loss did not improve from 1.58561\n",
      "351/351 [==============================] - 80s 227ms/step - loss: 1.6029 - accuracy: 0.3922 - val_loss: 1.6143 - val_accuracy: 0.3992\n"
     ]
    }
   ],
   "source": [
    "history_adam = model_adam.fit_generator(training_data_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_data_generator,\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model trained using Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_adam = model_rms.predict(x_t,batch_size=100)\n",
    "y_pred_adam = np.array([np.argmax(i) for i in y_test_pred_adam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_adam = Accuracy()\n",
    "m_adam.update_state(y_true,y_pred_rms)\n",
    "print('Test Accuracy using RMSprop optimizer is : ', m_adam.result().numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d__ . ResNet-18 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/qubvel/classification_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
